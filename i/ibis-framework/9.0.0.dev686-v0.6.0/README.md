# Comparing `tmp/ibis_framework-9.0.0.dev686.tar.gz` & `tmp/ibis-framework-v0.6.0.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "ibis_framework-9.0.0.dev686.tar", max compression
+gzip compressed data, was "dist/ibis-framework-v0.6.0.tar", last modified: Mon Nov 30 09:48:46 2015, max compression
```

## Comparing `ibis_framework-9.0.0.dev686.tar` & `ibis-framework-v0.6.0.tar`

### file list

```diff
@@ -1,1814 +1,204 @@
--rw-r--r--   0        0        0    11346 2024-04-28 00:02:48.365097 ibis_framework-9.0.0.dev686/LICENSE.txt
--rw-r--r--   0        0        0    11109 2024-04-28 00:02:48.365097 ibis_framework-9.0.0.dev686/README.md
--rw-r--r--   0        0        0     4186 2024-04-28 00:03:03.653022 ibis_framework-9.0.0.dev686/ibis/__init__.py
--rw-r--r--   0        0        0    44321 2024-04-28 00:02:48.509096 ibis_framework-9.0.0.dev686/ibis/backends/__init__.py
--rw-r--r--   0        0        0    44723 2024-04-28 00:02:48.509096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/__init__.py
--rw-r--r--   0        0        0     6319 2024-04-28 00:02:48.509096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/client.py
--rw-r--r--   0        0        0    26700 2024-04-28 00:02:48.509096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/compiler.py
--rw-r--r--   0        0        0      466 2024-04-28 00:02:48.509096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/converter.py
--rw-r--r--   0        0        0     2473 2024-04-28 00:02:48.509096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/datatypes.py
--rw-r--r--   0        0        0        0 2024-04-28 00:02:48.509096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/__init__.py
--rw-r--r--   0        0        0     2927 2024-04-28 00:02:48.509096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/conftest.py
--rw-r--r--   0        0        0        0 2024-04-28 00:02:48.509096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/system/__init__.py
--rw-r--r--   0        0        0     3130 2024-04-28 00:02:48.509096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/system/conftest.py
--rw-r--r--   0        0        0      146 2024-04-28 00:02:48.509096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/system/snapshots/test_client/test_cross_project_query/out.sql
--rw-r--r--   0        0        0      173 2024-04-28 00:02:48.509096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/system/snapshots/test_client/test_multiple_project_queries/out.sql
--rw-r--r--   0        0        0      155 2024-04-28 00:02:48.509096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/system/snapshots/test_client/test_multiple_project_queries_database_api/out.sql
--rw-r--r--   0        0        0    13933 2024-04-28 00:02:48.509096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/system/test_client.py
--rw-r--r--   0        0        0     7291 2024-04-28 00:02:48.509096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/system/test_connect.py
--rw-r--r--   0        0        0        0 2024-04-28 00:02:48.509096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/system/udf/__init__.py
--rw-r--r--   0        0        0      389 2024-04-28 00:02:48.513096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/system/udf/snapshots/test_udf_execute/test_udf_with_struct/out.sql
--rw-r--r--   0        0        0     4775 2024-04-28 00:02:48.513096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/system/udf/test_udf_execute.py
--rw-r--r--   0        0        0        0 2024-04-28 00:02:48.513096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/__init__.py
--rw-r--r--   0        0        0      196 2024-04-28 00:02:48.513096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_approx/filter-approx_median/out.sql
--rw-r--r--   0        0        0      166 2024-04-28 00:02:48.513096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_approx/filter-approx_nunique/out.sql
--rw-r--r--   0        0        0      122 2024-04-28 00:02:48.513096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_approx/no_filter-approx_median/out.sql
--rw-r--r--   0        0        0      120 2024-04-28 00:02:48.513096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_approx/no_filter-approx_nunique/out.sql
--rw-r--r--   0        0        0       77 2024-04-28 00:02:48.513096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_binary/out.sql
--rw-r--r--   0        0        0      143 2024-04-28 00:02:48.513096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_bit/filter-bit_and/out.sql
--rw-r--r--   0        0        0      141 2024-04-28 00:02:48.513096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_bit/filter-bit_or/out.sql
--rw-r--r--   0        0        0      143 2024-04-28 00:02:48.513096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_bit/filter-bit_xor/out.sql
--rw-r--r--   0        0        0       87 2024-04-28 00:02:48.513096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_bit/no_filter-bit_and/out.sql
--rw-r--r--   0        0        0       85 2024-04-28 00:02:48.513096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_bit/no_filter-bit_or/out.sql
--rw-r--r--   0        0        0       87 2024-04-28 00:02:48.513096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_bit/no_filter-bit_xor/out.sql
--rw-r--r--   0        0        0       98 2024-04-28 00:02:48.513096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_bool_reducers/mean/out.sql
--rw-r--r--   0        0        0       97 2024-04-28 00:02:48.513096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_bool_reducers/sum/out.sql
--rw-r--r--   0        0        0      253 2024-04-28 00:02:48.513096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_bool_reducers_where_conj/out.sql
--rw-r--r--   0        0        0      144 2024-04-28 00:02:48.513096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_bool_reducers_where_simple/out.sql
--rw-r--r--   0        0        0      249 2024-04-28 00:02:48.513096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_bucket/out.sql
--rw-r--r--   0        0        0      111 2024-04-28 00:02:48.513096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_cast_float_to_int/out.sql
--rw-r--r--   0        0        0       55 2024-04-28 00:02:48.513096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_compile_toplevel/out.sql
--rw-r--r--   0        0        0      130 2024-04-28 00:02:48.513096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_cov/pop/out.sql
--rw-r--r--   0        0        0      131 2024-04-28 00:02:48.513096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_cov/sample/out.sql
--rw-r--r--   0        0        0      106 2024-04-28 00:02:48.513096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_day_of_week/date/index.sql
--rw-r--r--   0        0        0      107 2024-04-28 00:02:48.513096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_day_of_week/date/name.sql
--rw-r--r--   0        0        0      136 2024-04-28 00:02:48.513096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_day_of_week/datetime/index.sql
--rw-r--r--   0        0        0      137 2024-04-28 00:02:48.513096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_day_of_week/datetime/name.sql
--rw-r--r--   0        0        0      106 2024-04-28 00:02:48.513096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_day_of_week/string_date/index.sql
--rw-r--r--   0        0        0      107 2024-04-28 00:02:48.513096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_day_of_week/string_date/name.sql
--rw-r--r--   0        0        0      136 2024-04-28 00:02:48.513096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_day_of_week/string_timestamp/index.sql
--rw-r--r--   0        0        0      137 2024-04-28 00:02:48.513096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_day_of_week/string_timestamp/name.sql
--rw-r--r--   0        0        0      136 2024-04-28 00:02:48.513096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_day_of_week/timestamp/index.sql
--rw-r--r--   0        0        0      137 2024-04-28 00:02:48.513096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_day_of_week/timestamp/name.sql
--rw-r--r--   0        0        0      106 2024-04-28 00:02:48.513096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_day_of_week/timestamp_date/index.sql
--rw-r--r--   0        0        0      107 2024-04-28 00:02:48.513096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_day_of_week/timestamp_date/name.sql
--rw-r--r--   0        0        0      130 2024-04-28 00:02:48.513096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_divide_by_zero/floordiv/out.sql
--rw-r--r--   0        0        0      103 2024-04-28 00:02:48.513096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_divide_by_zero/truediv/out.sql
--rw-r--r--   0        0        0       50 2024-04-28 00:02:48.513096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_extract_temporal_from_timestamp/date/out.sql
--rw-r--r--   0        0        0       50 2024-04-28 00:02:48.513096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_extract_temporal_from_timestamp/time/out.sql
--rw-r--r--   0        0        0       67 2024-04-28 00:02:48.513096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_azimuth/out.sql
--rw-r--r--   0        0        0       74 2024-04-28 00:02:48.513096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_binary/contains/out.sql
--rw-r--r--   0        0        0       75 2024-04-28 00:02:48.513096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_binary/covered_by/out.sql
--rw-r--r--   0        0        0       72 2024-04-28 00:02:48.513096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_binary/covers/out.sql
--rw-r--r--   0        0        0       78 2024-04-28 00:02:48.513096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_binary/d_within/out.sql
--rw-r--r--   0        0        0       76 2024-04-28 00:02:48.513096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_binary/difference/out.sql
--rw-r--r--   0        0        0       74 2024-04-28 00:02:48.513096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_binary/disjoint/out.sql
--rw-r--r--   0        0        0       74 2024-04-28 00:02:48.513096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_binary/distance/out.sql
--rw-r--r--   0        0        0       72 2024-04-28 00:02:48.513096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_binary/geo_equals/out.sql
--rw-r--r--   0        0        0       78 2024-04-28 00:02:48.513096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_binary/intersection/out.sql
--rw-r--r--   0        0        0       76 2024-04-28 00:02:48.513096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_binary/intersects/out.sql
--rw-r--r--   0        0        0       77 2024-04-28 00:02:48.513096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_binary/max_distance/out.sql
--rw-r--r--   0        0        0       73 2024-04-28 00:02:48.513096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_binary/touches/out.sql
--rw-r--r--   0        0        0       71 2024-04-28 00:02:48.513096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_binary/union/out.sql
--rw-r--r--   0        0        0       72 2024-04-28 00:02:48.513096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_binary/within/out.sql
--rw-r--r--   0        0        0       67 2024-04-28 00:02:48.513096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_minmax/x_max/out.sql
--rw-r--r--   0        0        0       67 2024-04-28 00:02:48.513096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_minmax/x_min/out.sql
--rw-r--r--   0        0        0       67 2024-04-28 00:02:48.513096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_minmax/y_max/out.sql
--rw-r--r--   0        0        0       67 2024-04-28 00:02:48.513096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_minmax/y_min/out.sql
--rw-r--r--   0        0        0       71 2024-04-28 00:02:48.513096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_point/out.sql
--rw-r--r--   0        0        0       64 2024-04-28 00:02:48.513096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_simplify/out.sql
--rw-r--r--   0        0        0       55 2024-04-28 00:02:48.513096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_unary/aread/out.sql
--rw-r--r--   0        0        0       59 2024-04-28 00:02:48.517096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_unary/as_binary/out.sql
--rw-r--r--   0        0        0       57 2024-04-28 00:02:48.517096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_unary/as_text/out.sql
--rw-r--r--   0        0        0       62 2024-04-28 00:02:48.517096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_unary/buffer/out.sql
--rw-r--r--   0        0        0       59 2024-04-28 00:02:48.517096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_unary/centroid/out.sql
--rw-r--r--   0        0        0       59 2024-04-28 00:02:48.517096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_unary/end_point/out.sql
--rw-r--r--   0        0        0       63 2024-04-28 00:02:48.517096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_unary/geometry_type/out.sql
--rw-r--r--   0        0        0       57 2024-04-28 00:02:48.517096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_unary/length/out.sql
--rw-r--r--   0        0        0       60 2024-04-28 00:02:48.517096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_unary/npoints/out.sql
--rw-r--r--   0        0        0       60 2024-04-28 00:02:48.517096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_unary/perimeter/out.sql
--rw-r--r--   0        0        0       60 2024-04-28 00:02:48.517096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_unary/point_n/out.sql
--rw-r--r--   0        0        0       61 2024-04-28 00:02:48.517096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_unary/start_point/out.sql
--rw-r--r--   0        0        0       60 2024-04-28 00:02:48.517096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_unary_union/out.sql
--rw-r--r--   0        0        0       50 2024-04-28 00:02:48.517096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_xy/x/out.sql
--rw-r--r--   0        0        0       50 2024-04-28 00:02:48.517096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_xy/y/out.sql
--rw-r--r--   0        0        0      107 2024-04-28 00:02:48.517096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_hash/binary/out.sql
--rw-r--r--   0        0        0       66 2024-04-28 00:02:48.517096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_hash/string/out.sql
--rw-r--r--   0        0        0       69 2024-04-28 00:02:48.517096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_hashbytes/md5-test-binary/out.sql
--rw-r--r--   0        0        0       37 2024-04-28 00:02:48.517096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_hashbytes/md5-test-string/out.sql
--rw-r--r--   0        0        0       61 2024-04-28 00:02:48.517096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_hashbytes/sha1-test-binary/out.sql
--rw-r--r--   0        0        0       29 2024-04-28 00:02:48.517096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_hashbytes/sha1-test-string/out.sql
--rw-r--r--   0        0        0       64 2024-04-28 00:02:48.517096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_hashbytes/sha256-test-binary/out.sql
--rw-r--r--   0        0        0       32 2024-04-28 00:02:48.517096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_hashbytes/sha256-test-string/out.sql
--rw-r--r--   0        0        0       64 2024-04-28 00:02:48.517096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_hashbytes/sha512-test-binary/out.sql
--rw-r--r--   0        0        0       32 2024-04-28 00:02:48.517096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_hashbytes/sha512-test-string/out.sql
--rw-r--r--   0        0        0      404 2024-04-28 00:02:48.517096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_identical_to/out.sql
--rw-r--r--   0        0        0       46 2024-04-28 00:02:48.517096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_integer_to_timestamp/ms/out.sql
--rw-r--r--   0        0        0       78 2024-04-28 00:02:48.517096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_integer_to_timestamp/ns/out.sql
--rw-r--r--   0        0        0       46 2024-04-28 00:02:48.517096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_integer_to_timestamp/s/out.sql
--rw-r--r--   0        0        0       45 2024-04-28 00:02:48.517096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_integer_to_timestamp/us/out.sql
--rw-r--r--   0        0        0       71 2024-04-28 00:02:48.517096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_literal_string/escape_ascii_sequences/out.sql
--rw-r--r--   0        0        0       31 2024-04-28 00:02:48.517096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_literal_string/escape_backslash/out.sql
--rw-r--r--   0        0        0       31 2024-04-28 00:02:48.517096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_literal_string/escape_quote/out.sql
--rw-r--r--   0        0        0       63 2024-04-28 00:02:48.517096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_literal_string/not_escape_special_characters/out.sql
--rw-r--r--   0        0        0       68 2024-04-28 00:02:48.517096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_literal_timestamp_or_time/datetime/out.sql
--rw-r--r--   0        0        0       52 2024-04-28 00:02:48.517096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_literal_timestamp_or_time/string_time/out.sql
--rw-r--r--   0        0        0       68 2024-04-28 00:02:48.517096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_literal_timestamp_or_time/string_timestamp/out.sql
--rw-r--r--   0        0        0       52 2024-04-28 00:02:48.517096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_literal_timestamp_or_time/time/out.sql
--rw-r--r--   0        0        0       68 2024-04-28 00:02:48.517096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_literal_timestamp_or_time/timestamp/out.sql
--rw-r--r--   0        0        0       86 2024-04-28 00:02:48.517096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_literal_year/date/out.sql
--rw-r--r--   0        0        0      116 2024-04-28 00:02:48.517096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_literal_year/datetime/out.sql
--rw-r--r--   0        0        0       86 2024-04-28 00:02:48.517096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_literal_year/string_date/out.sql
--rw-r--r--   0        0        0      116 2024-04-28 00:02:48.517096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_literal_year/string_timestamp/out.sql
--rw-r--r--   0        0        0      116 2024-04-28 00:02:48.517096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_literal_year/timestamp/out.sql
--rw-r--r--   0        0        0       86 2024-04-28 00:02:48.517096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_literal_year/timestamp_date/out.sql
--rw-r--r--   0        0        0       46 2024-04-28 00:02:48.517096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_now/out.sql
--rw-r--r--   0        0        0      417 2024-04-28 00:02:48.517096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_projection_fusion_only_peeks_at_immediate_parent/out.sql
--rw-r--r--   0        0        0      443 2024-04-28 00:02:48.517096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_range_window_function/prec_foll/out.sql
--rw-r--r--   0        0        0      451 2024-04-28 00:02:48.517096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_range_window_function/prec_prec/out.sql
--rw-r--r--   0        0        0      122 2024-04-28 00:02:48.517096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_set_operation/difference/out.sql
--rw-r--r--   0        0        0      125 2024-04-28 00:02:48.517096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_set_operation/intersect/out.sql
--rw-r--r--   0        0        0      116 2024-04-28 00:02:48.517096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_set_operation/union_all/out.sql
--rw-r--r--   0        0        0      121 2024-04-28 00:02:48.517096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_set_operation/union_distinct/out.sql
--rw-r--r--   0        0        0      124 2024-04-28 00:02:48.517096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_substring/out.sql
--rw-r--r--   0        0        0       60 2024-04-28 00:02:48.517096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_temporal_truncate/day-date/out.sql
--rw-r--r--   0        0        0       65 2024-04-28 00:02:48.517096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_temporal_truncate/day-timestamp/out.sql
--rw-r--r--   0        0        0       61 2024-04-28 00:02:48.517096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_temporal_truncate/hour-time/out.sql
--rw-r--r--   0        0        0       66 2024-04-28 00:02:48.517096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_temporal_truncate/hour-timestamp/out.sql
--rw-r--r--   0        0        0       68 2024-04-28 00:02:48.521096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_temporal_truncate/micros-time/out.sql
--rw-r--r--   0        0        0       73 2024-04-28 00:02:48.521096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_temporal_truncate/micros-timestamp/out.sql
--rw-r--r--   0        0        0       68 2024-04-28 00:02:48.521096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_temporal_truncate/millis-time/out.sql
--rw-r--r--   0        0        0       73 2024-04-28 00:02:48.521096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_temporal_truncate/millis-timestamp/out.sql
--rw-r--r--   0        0        0       63 2024-04-28 00:02:48.521096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_temporal_truncate/minute-time/out.sql
--rw-r--r--   0        0        0       68 2024-04-28 00:02:48.521096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_temporal_truncate/minute-timestamp/out.sql
--rw-r--r--   0        0        0       62 2024-04-28 00:02:48.521096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_temporal_truncate/month-date/out.sql
--rw-r--r--   0        0        0       67 2024-04-28 00:02:48.521096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_temporal_truncate/month-timestamp/out.sql
--rw-r--r--   0        0        0       64 2024-04-28 00:02:48.521096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_temporal_truncate/quarter-date/out.sql
--rw-r--r--   0        0        0       69 2024-04-28 00:02:48.521096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_temporal_truncate/quarter-timestamp/out.sql
--rw-r--r--   0        0        0       63 2024-04-28 00:02:48.521096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_temporal_truncate/second-time/out.sql
--rw-r--r--   0        0        0       68 2024-04-28 00:02:48.521096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_temporal_truncate/second-timestamp/out.sql
--rw-r--r--   0        0        0       69 2024-04-28 00:02:48.521096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_temporal_truncate/week-date/out.sql
--rw-r--r--   0        0        0       74 2024-04-28 00:02:48.521096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_temporal_truncate/week-timestamp/out.sql
--rw-r--r--   0        0        0       61 2024-04-28 00:02:48.521096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_temporal_truncate/year-date/out.sql
--rw-r--r--   0        0        0       66 2024-04-28 00:02:48.521096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_temporal_truncate/year-timestamp/out.sql
--rw-r--r--   0        0        0      141 2024-04-28 00:02:48.521096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_to_timestamp_no_timezone/out.sql
--rw-r--r--   0        0        0      173 2024-04-28 00:02:48.521096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_to_timestamp_timezone/out.sql
--rw-r--r--   0        0        0      448 2024-04-28 00:02:48.521096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_trailing_range_window/days/out.sql
--rw-r--r--   0        0        0      420 2024-04-28 00:02:48.521096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_trailing_range_window/five/out.sql
--rw-r--r--   0        0        0      450 2024-04-28 00:02:48.521096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_trailing_range_window/hours/out.sql
--rw-r--r--   0        0        0      464 2024-04-28 00:02:48.521096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_trailing_range_window/micros/out.sql
--rw-r--r--   0        0        0      454 2024-04-28 00:02:48.521096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_trailing_range_window/minutes/out.sql
--rw-r--r--   0        0        0      416 2024-04-28 00:02:48.521096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_trailing_range_window/nanos/out.sql
--rw-r--r--   0        0        0      454 2024-04-28 00:02:48.521096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_trailing_range_window/seconds/out.sql
--rw-r--r--   0        0        0      485 2024-04-28 00:02:48.521096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_trailing_range_window/two_days/out.sql
--rw-r--r--   0        0        0      450 2024-04-28 00:02:48.521096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_trailing_range_window/week/out.sql
--rw-r--r--   0        0        0      237 2024-04-28 00:02:48.521096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_trailing_range_window/years/out.sql
--rw-r--r--   0        0        0      397 2024-04-28 00:02:48.521096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_union/False/out.sql
--rw-r--r--   0        0        0      402 2024-04-28 00:02:48.521096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_union/True/out.sql
--rw-r--r--   0        0        0      476 2024-04-28 00:02:48.521096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_union_cte/False-False/out.sql
--rw-r--r--   0        0        0      481 2024-04-28 00:02:48.521096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_union_cte/False-True/out.sql
--rw-r--r--   0        0        0      481 2024-04-28 00:02:48.521096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_union_cte/True-False/out.sql
--rw-r--r--   0        0        0      486 2024-04-28 00:02:48.521096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_union_cte/True-True/out.sql
--rw-r--r--   0        0        0      490 2024-04-28 00:02:48.521096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_unnest/out_one_unnest.sql
--rw-r--r--   0        0        0      905 2024-04-28 00:02:48.521096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_unnest/out_two_unnests.sql
--rw-r--r--   0        0        0      444 2024-04-28 00:02:48.521096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_window_function/current_foll/out.sql
--rw-r--r--   0        0        0      444 2024-04-28 00:02:48.521096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_window_function/prec_current/out.sql
--rw-r--r--   0        0        0      444 2024-04-28 00:02:48.521096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_window_function/prec_prec/out.sql
--rw-r--r--   0        0        0      104 2024-04-28 00:02:48.521096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_window_unbounded/following/out.sql
--rw-r--r--   0        0        0      104 2024-04-28 00:02:48.521096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_window_unbounded/preceding/out.sql
--rw-r--r--   0        0        0      955 2024-04-28 00:02:48.521096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/test_client.py
--rw-r--r--   0        0        0    20080 2024-04-28 00:02:48.521096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/test_compiler.py
--rw-r--r--   0        0        0     2961 2024-04-28 00:02:48.521096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/test_datatypes.py
--rw-r--r--   0        0        0        0 2024-04-28 00:02:48.521096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/udf/__init__.py
--rw-r--r--   0        0        0       58 2024-04-28 00:02:48.521096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/udf/snapshots/test_builtin/test_bqutil_fn_from_hex/out.sql
--rw-r--r--   0        0        0      124 2024-04-28 00:02:48.521096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/udf/snapshots/test_builtin/test_farm_fingerprint/out.sql
--rw-r--r--   0        0        0       76 2024-04-28 00:02:48.521096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/udf/snapshots/test_core/test_assign/out.js
--rw-r--r--   0        0        0       42 2024-04-28 00:02:48.521096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/udf/snapshots/test_core/test_binary_operators/add/out.js
--rw-r--r--   0        0        0       42 2024-04-28 00:02:48.521096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/udf/snapshots/test_core/test_binary_operators/div/out.js
--rw-r--r--   0        0        0       42 2024-04-28 00:02:48.521096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/udf/snapshots/test_core/test_binary_operators/mul/out.js
--rw-r--r--   0        0        0       42 2024-04-28 00:02:48.521096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/udf/snapshots/test_core/test_binary_operators/sub/out.js
--rw-r--r--   0        0        0      178 2024-04-28 00:02:48.521096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/udf/snapshots/test_core/test_class/out.js
--rw-r--r--   0        0        0      125 2024-04-28 00:02:48.521096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/udf/snapshots/test_core/test_class_with_properties/out.js
--rw-r--r--   0        0        0      139 2024-04-28 00:02:48.521096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/udf/snapshots/test_core/test_continue/out.js
--rw-r--r--   0        0        0      154 2024-04-28 00:02:48.521096 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/udf/snapshots/test_core/test_delete/out.js
--rw-r--r--   0        0        0       64 2024-04-28 00:02:48.525097 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/udf/snapshots/test_core/test_dict/out.js
--rw-r--r--   0        0        0       61 2024-04-28 00:02:48.525097 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/udf/snapshots/test_core/test_floordiv/out.js
--rw-r--r--   0        0        0       40 2024-04-28 00:02:48.525097 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/udf/snapshots/test_core/test_function_def/out.js
--rw-r--r--   0        0        0      249 2024-04-28 00:02:48.525097 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/udf/snapshots/test_core/test_lambda_with_splat/out.js
--rw-r--r--   0        0        0       44 2024-04-28 00:02:48.525097 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/udf/snapshots/test_core/test_len_rewrite/out.js
--rw-r--r--   0        0        0      138 2024-04-28 00:02:48.525097 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/udf/snapshots/test_core/test_list_comp/out.js
--rw-r--r--   0        0        0       80 2024-04-28 00:02:48.525097 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/udf/snapshots/test_core/test_logical_not/out.js
--rw-r--r--   0        0        0       56 2024-04-28 00:02:48.525097 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/udf/snapshots/test_core/test_missing_vararg/out.js
--rw-r--r--   0        0        0       58 2024-04-28 00:02:48.525097 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/udf/snapshots/test_core/test_pow/out.js
--rw-r--r--   0        0        0      323 2024-04-28 00:02:48.525097 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/udf/snapshots/test_core/test_scope_with_while/out.js
--rw-r--r--   0        0        0       85 2024-04-28 00:02:48.525097 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/udf/snapshots/test_core/test_set_to_object/out.js
--rw-r--r--   0        0        0       78 2024-04-28 00:02:48.525097 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/udf/snapshots/test_core/test_setitem/out.js
--rw-r--r--   0        0        0      134 2024-04-28 00:02:48.525097 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/udf/snapshots/test_core/test_splat/out.js
--rw-r--r--   0        0        0       49 2024-04-28 00:02:48.525097 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/udf/snapshots/test_core/test_str/out.js
--rw-r--r--   0        0        0      106 2024-04-28 00:02:48.525097 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/udf/snapshots/test_core/test_true_false_none/out.js
--rw-r--r--   0        0        0       59 2024-04-28 00:02:48.525097 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/udf/snapshots/test_core/test_tuple/out.js
--rw-r--r--   0        0        0       48 2024-04-28 00:02:48.525097 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/udf/snapshots/test_core/test_unary_minus/out.js
--rw-r--r--   0        0        0       48 2024-04-28 00:02:48.525097 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/udf/snapshots/test_core/test_unary_plus/out.js
--rw-r--r--   0        0        0       48 2024-04-28 00:02:48.525097 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/udf/snapshots/test_core/test_varargs/out.js
--rw-r--r--   0        0        0       51 2024-04-28 00:02:48.525097 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/udf/snapshots/test_core/test_variable_declaration/out.js
--rw-r--r--   0        0        0       40 2024-04-28 00:02:48.525097 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/udf/snapshots/test_core/test_yield/out.js
--rw-r--r--   0        0        0       40 2024-04-28 00:02:48.525097 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/udf/snapshots/test_core/test_yield_from/out.js
--rw-r--r--   0        0        0      798 2024-04-28 00:02:48.525097 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/udf/test_builtin.py
--rw-r--r--   0        0        0     6533 2024-04-28 00:02:48.525097 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/udf/test_core.py
--rw-r--r--   0        0        0     2002 2024-04-28 00:02:48.525097 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/udf/test_find.py
--rw-r--r--   0        0        0     1926 2024-04-28 00:02:48.525097 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/tests/unit/udf/test_usage.py
--rw-r--r--   0        0        0        0 2024-04-28 00:02:48.525097 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/udf/__init__.py
--rw-r--r--   0        0        0    17067 2024-04-28 00:02:48.525097 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/udf/core.py
--rw-r--r--   0        0        0     1599 2024-04-28 00:02:48.525097 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/udf/find.py
--rw-r--r--   0        0        0     1255 2024-04-28 00:02:48.525097 ibis_framework-9.0.0.dev686/ibis/backends/bigquery/udf/rewrite.py
--rw-r--r--   0        0        0    24789 2024-04-28 00:02:48.525097 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/__init__.py
--rw-r--r--   0        0        0    21888 2024-04-28 00:02:48.525097 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/compiler.py
--rw-r--r--   0        0        0     1649 2024-04-28 00:02:48.525097 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/converter.py
--rw-r--r--   0        0        0        0 2024-04-28 00:02:48.525097 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/__init__.py
--rw-r--r--   0        0        0     3297 2024-04-28 00:02:48.525097 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/conftest.py
--rw-r--r--   0        0        0      139 2024-04-28 00:02:48.525097 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_aggregations/test_reduction_where/count/out.sql
--rw-r--r--   0        0        0      135 2024-04-28 00:02:48.525097 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_aggregations/test_reduction_where/max/out.sql
--rw-r--r--   0        0        0      136 2024-04-28 00:02:48.525097 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_aggregations/test_reduction_where/mean/out.sql
--rw-r--r--   0        0        0      135 2024-04-28 00:02:48.525097 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_aggregations/test_reduction_where/min/out.sql
--rw-r--r--   0        0        0      150 2024-04-28 00:02:48.525097 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_aggregations/test_reduction_where/std/out.sql
--rw-r--r--   0        0        0      135 2024-04-28 00:02:48.525097 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_aggregations/test_reduction_where/sum/out.sql
--rw-r--r--   0        0        0      144 2024-04-28 00:02:48.525097 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_aggregations/test_reduction_where/var/out.sql
--rw-r--r--   0        0        0      149 2024-04-28 00:02:48.525097 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_aggregations/test_std_var_pop/std/out.sql
--rw-r--r--   0        0        0      143 2024-04-28 00:02:48.525097 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_aggregations/test_std_var_pop/var/out.sql
--rw-r--r--   0        0        0       61 2024-04-28 00:02:48.525097 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_functions/test_cast_double_col/float/out.sql
--rw-r--r--   0        0        0      119 2024-04-28 00:02:48.525097 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_functions/test_cast_double_col/float32/out.sql
--rw-r--r--   0        0        0      110 2024-04-28 00:02:48.525097 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_functions/test_cast_double_col/float64/out.sql
--rw-r--r--   0        0        0      115 2024-04-28 00:02:48.525097 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_functions/test_cast_double_col/int16/out.sql
--rw-r--r--   0        0        0      113 2024-04-28 00:02:48.525097 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_functions/test_cast_double_col/int8/out.sql
--rw-r--r--   0        0        0      113 2024-04-28 00:02:48.525097 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_functions/test_cast_string_col/date/out.sql
--rw-r--r--   0        0        0      115 2024-04-28 00:02:48.525097 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_functions/test_cast_string_col/int16/out.sql
--rw-r--r--   0        0        0      113 2024-04-28 00:02:48.525097 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_functions/test_cast_string_col/int8/out.sql
--rw-r--r--   0        0        0      142 2024-04-28 00:02:48.525097 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_functions/test_cast_string_col/mapstring_int64/out.sql
--rw-r--r--   0        0        0      108 2024-04-28 00:02:48.525097 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_functions/test_cast_string_col/string/out.sql
--rw-r--r--   0        0        0      171 2024-04-28 00:02:48.525097 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_functions/test_cast_string_col/structa_string_b_int64/out.sql
--rw-r--r--   0        0        0      122 2024-04-28 00:02:48.525097 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_functions/test_cast_string_col/timestamp/out.sql
--rw-r--r--   0        0        0      319 2024-04-28 00:02:48.525097 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_functions/test_column_regexp_extract/out.sql
--rw-r--r--   0        0        0      143 2024-04-28 00:02:48.525097 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_functions/test_column_regexp_replace/out.sql
--rw-r--r--   0        0        0       88 2024-04-28 00:02:48.529096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_functions/test_greatest_least/out1.sql
--rw-r--r--   0        0        0      103 2024-04-28 00:02:48.529096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_functions/test_greatest_least/out2.sql
--rw-r--r--   0        0        0       82 2024-04-28 00:02:48.529096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_functions/test_greatest_least/out3.sql
--rw-r--r--   0        0        0       97 2024-04-28 00:02:48.529096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_functions/test_greatest_least/out4.sql
--rw-r--r--   0        0        0      211 2024-04-28 00:02:48.529096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_functions/test_group_concat/comma_none/out.sql
--rw-r--r--   0        0        0      278 2024-04-28 00:02:48.529096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_functions/test_group_concat/comma_zero/out.sql
--rw-r--r--   0        0        0      211 2024-04-28 00:02:48.529096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_functions/test_group_concat/minus_none/out.sql
--rw-r--r--   0        0        0       94 2024-04-28 00:02:48.529096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_functions/test_hash/out.sql
--rw-r--r--   0        0        0       61 2024-04-28 00:02:48.529096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_functions/test_noop_cast/bigint_col/out.sql
--rw-r--r--   0        0        0       59 2024-04-28 00:02:48.529096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_functions/test_noop_cast/bool_col/out.sql
--rw-r--r--   0        0        0       66 2024-04-28 00:02:48.529096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_functions/test_noop_cast/date_string_col/out.sql
--rw-r--r--   0        0        0       61 2024-04-28 00:02:48.529096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_functions/test_noop_cast/double_col/out.sql
--rw-r--r--   0        0        0       60 2024-04-28 00:02:48.529096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_functions/test_noop_cast/float_col/out.sql
--rw-r--r--   0        0        0       53 2024-04-28 00:02:48.529096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_functions/test_noop_cast/id/out.sql
--rw-r--r--   0        0        0        5 2024-04-28 00:02:48.529096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_functions/test_noop_cast/index/out.sql
--rw-r--r--   0        0        0       58 2024-04-28 00:02:48.529096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_functions/test_noop_cast/int_col/out.sql
--rw-r--r--   0        0        0       56 2024-04-28 00:02:48.529096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_functions/test_noop_cast/month/out.sql
--rw-r--r--   0        0        0       63 2024-04-28 00:02:48.529096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_functions/test_noop_cast/smallint_col/out.sql
--rw-r--r--   0        0        0       61 2024-04-28 00:02:48.529096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_functions/test_noop_cast/string_col/out.sql
--rw-r--r--   0        0        0       64 2024-04-28 00:02:48.529096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_functions/test_noop_cast/timestamp_col/out.sql
--rw-r--r--   0        0        0       62 2024-04-28 00:02:48.529096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_functions/test_noop_cast/tinyint_col/out.sql
--rw-r--r--   0        0        0       55 2024-04-28 00:02:48.529096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_functions/test_noop_cast/year/out.sql
--rw-r--r--   0        0        0      113 2024-04-28 00:02:48.529096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_functions/test_string_column_find/out1.sql
--rw-r--r--   0        0        0      134 2024-04-28 00:02:48.529096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_functions/test_string_column_find/out2.sql
--rw-r--r--   0        0        0      116 2024-04-28 00:02:48.529096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_functions/test_string_column_find_in_set/out.sql
--rw-r--r--   0        0        0      112 2024-04-28 00:02:48.529096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_functions/test_string_column_like/out1.sql
--rw-r--r--   0        0        0      184 2024-04-28 00:02:48.529096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_functions/test_string_column_like/out2.sql
--rw-r--r--   0        0        0      204 2024-04-28 00:02:48.529096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_functions/test_string_column_substring/out1.sql
--rw-r--r--   0        0        0      214 2024-04-28 00:02:48.529096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_functions/test_string_column_substring/out2.sql
--rw-r--r--   0        0        0      119 2024-04-28 00:02:48.529096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_functions/test_timestamp_cast/out1.sql
--rw-r--r--   0        0        0      107 2024-04-28 00:02:48.529096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_functions/test_timestamp_cast/out2.sql
--rw-r--r--   0        0        0      110 2024-04-28 00:02:48.529096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_functions/test_timestamp_from_integer/out.sql
--rw-r--r--   0        0        0       34 2024-04-28 00:02:48.529096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_functions/test_timestamp_now/out.sql
--rw-r--r--   0        0        0      135 2024-04-28 00:02:48.529096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_functions/test_timestamp_truncate/d/out.sql
--rw-r--r--   0        0        0      143 2024-04-28 00:02:48.529096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_functions/test_timestamp_truncate/h/out.sql
--rw-r--r--   0        0        0      147 2024-04-28 00:02:48.529096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_functions/test_timestamp_truncate/m/out.sql
--rw-r--r--   0        0        0      147 2024-04-28 00:02:48.529096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_functions/test_timestamp_truncate/minute/out.sql
--rw-r--r--   0        0        0      138 2024-04-28 00:02:48.529096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_functions/test_timestamp_truncate/w/out.sql
--rw-r--r--   0        0        0      143 2024-04-28 00:02:48.529096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_functions/test_timestamp_truncate/y/out.sql
--rw-r--r--   0        0        0       87 2024-04-28 00:02:48.529096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_functions/test_translate_math_functions/abs/out.sql
--rw-r--r--   0        0        0      114 2024-04-28 00:02:48.529096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_functions/test_translate_math_functions/ceil/out.sql
--rw-r--r--   0        0        0       87 2024-04-28 00:02:48.529096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_functions/test_translate_math_functions/exp/out.sql
--rw-r--r--   0        0        0       86 2024-04-28 00:02:48.529096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_functions/test_translate_math_functions/log/out.sql
--rw-r--r--   0        0        0       91 2024-04-28 00:02:48.529096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_functions/test_translate_math_functions/log10/out.sql
--rw-r--r--   0        0        0       89 2024-04-28 00:02:48.529096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_functions/test_translate_math_functions/log2/out.sql
--rw-r--r--   0        0        0       91 2024-04-28 00:02:48.529096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_functions/test_translate_math_functions/round/out.sql
--rw-r--r--   0        0        0       97 2024-04-28 00:02:48.529096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_functions/test_translate_math_functions/round_0/out.sql
--rw-r--r--   0        0        0       97 2024-04-28 00:02:48.529096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_functions/test_translate_math_functions/round_2/out.sql
--rw-r--r--   0        0        0      121 2024-04-28 00:02:48.529096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_functions/test_translate_math_functions/sign/out.sql
--rw-r--r--   0        0        0       89 2024-04-28 00:02:48.529096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_functions/test_translate_math_functions/sqrt/out.sql
--rw-r--r--   0        0        0      122 2024-04-28 00:02:48.529096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_literals/test_fine_grained_timestamp_literals/micros/out.sql
--rw-r--r--   0        0        0      151 2024-04-28 00:02:48.529096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_literals/test_fine_grained_timestamp_literals/micros_tz/out.sql
--rw-r--r--   0        0        0      122 2024-04-28 00:02:48.529096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_literals/test_fine_grained_timestamp_literals/millis/out.sql
--rw-r--r--   0        0        0      151 2024-04-28 00:02:48.529096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_literals/test_fine_grained_timestamp_literals/millis_tz/out.sql
--rw-r--r--   0        0        0       10 2024-04-28 00:02:48.529096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_literals/test_string_literals/nested_quote/out.sql
--rw-r--r--   0        0        0       13 2024-04-28 00:02:48.529096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_literals/test_string_literals/nested_token/out.sql
--rw-r--r--   0        0        0        8 2024-04-28 00:02:48.529096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_literals/test_string_literals/simple/out.sql
--rw-r--r--   0        0        0       25 2024-04-28 00:02:48.529096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_literals/test_string_numeric_boolean_literals/false/out.sql
--rw-r--r--   0        0        0       21 2024-04-28 00:02:48.529096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_literals/test_string_numeric_boolean_literals/float/out.sql
--rw-r--r--   0        0        0       17 2024-04-28 00:02:48.529096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_literals/test_string_numeric_boolean_literals/int/out.sql
--rw-r--r--   0        0        0       36 2024-04-28 00:02:48.529096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_literals/test_string_numeric_boolean_literals/nested_quote/out.sql
--rw-r--r--   0        0        0       43 2024-04-28 00:02:48.529096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_literals/test_string_numeric_boolean_literals/nested_token/out.sql
--rw-r--r--   0        0        0       31 2024-04-28 00:02:48.533096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_literals/test_string_numeric_boolean_literals/simple/out.sql
--rw-r--r--   0        0        0       23 2024-04-28 00:02:48.533096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_literals/test_string_numeric_boolean_literals/true/out.sql
--rw-r--r--   0        0        0      102 2024-04-28 00:02:48.533096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_literals/test_timestamp_literals/expr0/out.sql
--rw-r--r--   0        0        0      102 2024-04-28 00:02:48.533096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_literals/test_timestamp_literals/expr1/out.sql
--rw-r--r--   0        0        0      102 2024-04-28 00:02:48.533096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_literals/test_timestamp_literals/expr2/out.sql
--rw-r--r--   0        0        0      104 2024-04-28 00:02:48.533096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_operators/test_between/out.sql
--rw-r--r--   0        0        0      110 2024-04-28 00:02:48.533096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_operators/test_binary_infix_operators/add/out.sql
--rw-r--r--   0        0        0      113 2024-04-28 00:02:48.533096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_operators/test_binary_infix_operators/eq/out.sql
--rw-r--r--   0        0        0      120 2024-04-28 00:02:48.533096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_operators/test_binary_infix_operators/ge/out.sql
--rw-r--r--   0        0        0      114 2024-04-28 00:02:48.533096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_operators/test_binary_infix_operators/gt/out.sql
--rw-r--r--   0        0        0      117 2024-04-28 00:02:48.533096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_operators/test_binary_infix_operators/le/out.sql
--rw-r--r--   0        0        0      111 2024-04-28 00:02:48.533096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_operators/test_binary_infix_operators/lt/out.sql
--rw-r--r--   0        0        0      115 2024-04-28 00:02:48.533096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_operators/test_binary_infix_operators/mul/out.sql
--rw-r--r--   0        0        0      117 2024-04-28 00:02:48.533096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_operators/test_binary_infix_operators/ne/out.sql
--rw-r--r--   0        0        0      118 2024-04-28 00:02:48.533096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_operators/test_binary_infix_operators/pow/out.sql
--rw-r--r--   0        0        0      115 2024-04-28 00:02:48.533096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_operators/test_binary_infix_operators/sub/out.sql
--rw-r--r--   0        0        0      113 2024-04-28 00:02:48.533096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_operators/test_binary_infix_operators/truediv/out.sql
--rw-r--r--   0        0        0      157 2024-04-28 00:02:48.533096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_operators/test_binary_infix_parenthesization/lambda0/out.sql
--rw-r--r--   0        0        0      117 2024-04-28 00:02:48.533096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_operators/test_binary_infix_parenthesization/lambda1/out.sql
--rw-r--r--   0        0        0      166 2024-04-28 00:02:48.533096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_operators/test_binary_infix_parenthesization/lambda2/out.sql
--rw-r--r--   0        0        0       92 2024-04-28 00:02:48.533096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_operators/test_negate/bool_col/out.sql
--rw-r--r--   0        0        0       94 2024-04-28 00:02:48.533096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_operators/test_negate/float_col/out.sql
--rw-r--r--   0        0        0       90 2024-04-28 00:02:48.533096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_operators/test_negate/int_col/out.sql
--rw-r--r--   0        0        0      198 2024-04-28 00:02:48.533096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_operators/test_search_case/out.sql
--rw-r--r--   0        0        0      168 2024-04-28 00:02:48.533096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_operators/test_simple_case/out.sql
--rw-r--r--   0        0        0      148 2024-04-28 00:02:48.533096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_select/test_array_join_in_subquery/out.sql
--rw-r--r--   0        0        0      227 2024-04-28 00:02:48.533096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_select/test_complex_array_expr_projection/out.sql
--rw-r--r--   0        0        0      256 2024-04-28 00:02:48.533096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_select/test_complex_join/out.sql
--rw-r--r--   0        0        0      150 2024-04-28 00:02:48.533096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_select/test_count_name/out.sql
--rw-r--r--   0        0        0      177 2024-04-28 00:02:48.533096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_select/test_ifelse_use_if/out.sql
--rw-r--r--   0        0        0      343 2024-04-28 00:02:48.533096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_select/test_isin_notin_in_select/out1.sql
--rw-r--r--   0        0        0      357 2024-04-28 00:02:48.533096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_select/test_isin_notin_in_select/out2.sql
--rw-r--r--   0        0        0      145 2024-04-28 00:02:48.533096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_select/test_isnull_case_expr_rewrite_failure/out.sql
--rw-r--r--   0        0        0      367 2024-04-28 00:02:48.533096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_select/test_join_self_reference/out.sql
--rw-r--r--   0        0        0      216 2024-04-28 00:02:48.533096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_select/test_named_from_filter_groupby/out1.sql
--rw-r--r--   0        0        0      216 2024-04-28 00:02:48.533096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_select/test_named_from_filter_groupby/out2.sql
--rw-r--r--   0        0        0       37 2024-04-28 00:02:48.533096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_select/test_physical_table_reference_translate/out.sql
--rw-r--r--   0        0        0       29 2024-04-28 00:02:48.533096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_select/test_scalar_exprs_no_table_refs/add/out.sql
--rw-r--r--   0        0        0       34 2024-04-28 00:02:48.533096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_select/test_scalar_exprs_no_table_refs/now/out.sql
--rw-r--r--   0        0        0       45 2024-04-28 00:02:48.533096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_select/test_self_reference_simple/out.sql
--rw-r--r--   0        0        0      409 2024-04-28 00:02:48.533096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_select/test_simple_joins/playerID-awardID-any_inner_join/out.sql
--rw-r--r--   0        0        0      414 2024-04-28 00:02:48.533096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_select/test_simple_joins/playerID-awardID-any_left_join/out.sql
--rw-r--r--   0        0        0      411 2024-04-28 00:02:48.533096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_select/test_simple_joins/playerID-awardID-inner_join/out.sql
--rw-r--r--   0        0        0      416 2024-04-28 00:02:48.533096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_select/test_simple_joins/playerID-awardID-left_join/out.sql
--rw-r--r--   0        0        0      410 2024-04-28 00:02:48.533096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_select/test_simple_joins/playerID-playerID-any_inner_join/out.sql
--rw-r--r--   0        0        0      415 2024-04-28 00:02:48.533096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_select/test_simple_joins/playerID-playerID-any_left_join/out.sql
--rw-r--r--   0        0        0      412 2024-04-28 00:02:48.533096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_select/test_simple_joins/playerID-playerID-inner_join/out.sql
--rw-r--r--   0        0        0      417 2024-04-28 00:02:48.533096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_select/test_simple_joins/playerID-playerID-left_join/out.sql
--rw-r--r--   0        0        0      428 2024-04-28 00:02:48.533096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_select/test_simple_scalar_aggregates/out.sql
--rw-r--r--   0        0        0      561 2024-04-28 00:02:48.533096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_select/test_table_column_unbox/out.sql
--rw-r--r--   0        0        0      307 2024-04-28 00:02:48.533096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_select/test_timestamp_extract_field/out.sql
--rw-r--r--   0        0        0      380 2024-04-28 00:02:48.533096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_select/test_where_simple_comparisons/out.sql
--rw-r--r--   0        0        0      363 2024-04-28 00:02:48.533096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_select/test_where_with_between/out.sql
--rw-r--r--   0        0        0      119 2024-04-28 00:02:48.533096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/snapshots/test_select/test_where_with_timestamp/out.sql
--rw-r--r--   0        0        0     5009 2024-04-28 00:02:48.533096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/test_aggregations.py
--rw-r--r--   0        0        0    10212 2024-04-28 00:02:48.533096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/test_client.py
--rw-r--r--   0        0        0     9902 2024-04-28 00:02:48.533096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/test_datatypes.py
--rw-r--r--   0        0        0    13893 2024-04-28 00:02:48.533096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/test_functions.py
--rw-r--r--   0        0        0     1812 2024-04-28 00:02:48.537096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/test_literals.py
--rw-r--r--   0        0        0     7148 2024-04-28 00:02:48.537096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/test_operators.py
--rw-r--r--   0        0        0    10602 2024-04-28 00:02:48.537096 ibis_framework-9.0.0.dev686/ibis/backends/clickhouse/tests/test_select.py
--rw-r--r--   0        0        0    22199 2024-04-28 00:02:48.537096 ibis_framework-9.0.0.dev686/ibis/backends/conftest.py
--rw-r--r--   0        0        0     5819 2024-04-28 00:02:48.537096 ibis_framework-9.0.0.dev686/ibis/backends/dask/__init__.py
--rw-r--r--   0        0        0     3022 2024-04-28 00:02:48.537096 ibis_framework-9.0.0.dev686/ibis/backends/dask/convert.py
--rw-r--r--   0        0        0    14510 2024-04-28 00:02:48.537096 ibis_framework-9.0.0.dev686/ibis/backends/dask/executor.py
--rw-r--r--   0        0        0     6623 2024-04-28 00:02:48.537096 ibis_framework-9.0.0.dev686/ibis/backends/dask/helpers.py
--rw-r--r--   0        0        0     1944 2024-04-28 00:02:48.537096 ibis_framework-9.0.0.dev686/ibis/backends/dask/kernels.py
--rw-r--r--   0        0        0        0 2024-04-28 00:02:48.537096 ibis_framework-9.0.0.dev686/ibis/backends/dask/tests/__init__.py
--rw-r--r--   0        0        0    11339 2024-04-28 00:02:48.537096 ibis_framework-9.0.0.dev686/ibis/backends/dask/tests/conftest.py
--rw-r--r--   0        0        0     5854 2024-04-28 00:02:48.537096 ibis_framework-9.0.0.dev686/ibis/backends/dask/tests/test_arrays.py
--rw-r--r--   0        0        0     5242 2024-04-28 00:02:48.537096 ibis_framework-9.0.0.dev686/ibis/backends/dask/tests/test_cast.py
--rw-r--r--   0        0        0     3483 2024-04-28 00:02:48.537096 ibis_framework-9.0.0.dev686/ibis/backends/dask/tests/test_client.py
--rw-r--r--   0        0        0     1049 2024-04-28 00:02:48.537096 ibis_framework-9.0.0.dev686/ibis/backends/dask/tests/test_core.py
--rw-r--r--   0        0        0     5782 2024-04-28 00:02:48.537096 ibis_framework-9.0.0.dev686/ibis/backends/dask/tests/test_functions.py
--rw-r--r--   0        0        0    12046 2024-04-28 00:02:48.537096 ibis_framework-9.0.0.dev686/ibis/backends/dask/tests/test_join.py
--rw-r--r--   0        0        0     2609 2024-04-28 00:02:48.537096 ibis_framework-9.0.0.dev686/ibis/backends/dask/tests/test_maps.py
--rw-r--r--   0        0        0    28046 2024-04-28 00:02:48.537096 ibis_framework-9.0.0.dev686/ibis/backends/dask/tests/test_operations.py
--rw-r--r--   0        0        0     3860 2024-04-28 00:02:48.537096 ibis_framework-9.0.0.dev686/ibis/backends/dask/tests/test_strings.py
--rw-r--r--   0        0        0     2548 2024-04-28 00:02:48.537096 ibis_framework-9.0.0.dev686/ibis/backends/dask/tests/test_structs.py
--rw-r--r--   0        0        0     6771 2024-04-28 00:02:48.537096 ibis_framework-9.0.0.dev686/ibis/backends/dask/tests/test_temporal.py
--rw-r--r--   0        0        0    11510 2024-04-28 00:02:48.537096 ibis_framework-9.0.0.dev686/ibis/backends/dask/tests/test_udf.py
--rw-r--r--   0        0        0    16812 2024-04-28 00:02:48.537096 ibis_framework-9.0.0.dev686/ibis/backends/dask/tests/test_window.py
--rw-r--r--   0        0        0    21983 2024-04-28 00:02:48.537096 ibis_framework-9.0.0.dev686/ibis/backends/datafusion/__init__.py
--rw-r--r--   0        0        0    17333 2024-04-28 00:02:48.537096 ibis_framework-9.0.0.dev686/ibis/backends/datafusion/compiler.py
--rw-r--r--   0        0        0        0 2024-04-28 00:02:48.537096 ibis_framework-9.0.0.dev686/ibis/backends/datafusion/tests/__init__.py
--rw-r--r--   0        0        0     1318 2024-04-28 00:02:48.537096 ibis_framework-9.0.0.dev686/ibis/backends/datafusion/tests/conftest.py
--rw-r--r--   0        0        0     1091 2024-04-28 00:02:48.537096 ibis_framework-9.0.0.dev686/ibis/backends/datafusion/tests/test_connect.py
--rw-r--r--   0        0        0     1214 2024-04-28 00:02:48.537096 ibis_framework-9.0.0.dev686/ibis/backends/datafusion/tests/test_register.py
--rw-r--r--   0        0        0      799 2024-04-28 00:02:48.537096 ibis_framework-9.0.0.dev686/ibis/backends/datafusion/tests/test_select.py
--rw-r--r--   0        0        0      177 2024-04-28 00:02:48.537096 ibis_framework-9.0.0.dev686/ibis/backends/datafusion/tests/test_string.py
--rw-r--r--   0        0        0      760 2024-04-28 00:02:48.537096 ibis_framework-9.0.0.dev686/ibis/backends/datafusion/tests/test_temporal.py
--rw-r--r--   0        0        0     2146 2024-04-28 00:02:48.537096 ibis_framework-9.0.0.dev686/ibis/backends/datafusion/tests/test_udf.py
--rw-r--r--   0        0        0     3603 2024-04-28 00:02:48.537096 ibis_framework-9.0.0.dev686/ibis/backends/datafusion/udfs.py
--rw-r--r--   0        0        0     5929 2024-04-28 00:02:48.537096 ibis_framework-9.0.0.dev686/ibis/backends/druid/__init__.py
--rw-r--r--   0        0        0     6456 2024-04-28 00:02:48.537096 ibis_framework-9.0.0.dev686/ibis/backends/druid/compiler.py
--rw-r--r--   0        0        0        0 2024-04-28 00:02:48.537096 ibis_framework-9.0.0.dev686/ibis/backends/druid/tests/__init__.py
--rw-r--r--   0        0        0     4698 2024-04-28 00:02:48.537096 ibis_framework-9.0.0.dev686/ibis/backends/druid/tests/conftest.py
--rw-r--r--   0        0        0    51819 2024-04-28 00:02:48.537096 ibis_framework-9.0.0.dev686/ibis/backends/duckdb/__init__.py
--rw-r--r--   0        0        0    17152 2024-04-28 00:02:48.537096 ibis_framework-9.0.0.dev686/ibis/backends/duckdb/compiler.py
--rw-r--r--   0        0        0      240 2024-04-28 00:02:48.537096 ibis_framework-9.0.0.dev686/ibis/backends/duckdb/converter.py
--rw-r--r--   0        0        0        0 2024-04-28 00:02:48.537096 ibis_framework-9.0.0.dev686/ibis/backends/duckdb/tests/__init__.py
--rw-r--r--   0        0        0     3987 2024-04-28 00:02:48.541096 ibis_framework-9.0.0.dev686/ibis/backends/duckdb/tests/conftest.py
--rw-r--r--   0        0        0       37 2024-04-28 00:02:48.541096 ibis_framework-9.0.0.dev686/ibis/backends/duckdb/tests/snapshots/test_client/test_to_other_sql/out.sql
--rw-r--r--   0        0        0       54 2024-04-28 00:02:48.541096 ibis_framework-9.0.0.dev686/ibis/backends/duckdb/tests/snapshots/test_datatypes/test_cast_to_floating_point_type/float32/out.sql
--rw-r--r--   0        0        0       56 2024-04-28 00:02:48.541096 ibis_framework-9.0.0.dev686/ibis/backends/duckdb/tests/snapshots/test_datatypes/test_cast_to_floating_point_type/float64/out.sql
--rw-r--r--   0        0        0       74 2024-04-28 00:02:48.541096 ibis_framework-9.0.0.dev686/ibis/backends/duckdb/tests/snapshots/test_datatypes/test_cast_uints/uint16/out.sql
--rw-r--r--   0        0        0       73 2024-04-28 00:02:48.541096 ibis_framework-9.0.0.dev686/ibis/backends/duckdb/tests/snapshots/test_datatypes/test_cast_uints/uint32/out.sql
--rw-r--r--   0        0        0       72 2024-04-28 00:02:48.541096 ibis_framework-9.0.0.dev686/ibis/backends/duckdb/tests/snapshots/test_datatypes/test_cast_uints/uint64/out.sql
--rw-r--r--   0        0        0       72 2024-04-28 00:02:48.541096 ibis_framework-9.0.0.dev686/ibis/backends/duckdb/tests/snapshots/test_datatypes/test_cast_uints/uint8/out.sql
--rw-r--r--   0        0        0       92 2024-04-28 00:02:48.541096 ibis_framework-9.0.0.dev686/ibis/backends/duckdb/tests/snapshots/test_geospatial/test_geospatial_dwithin/out.sql
--rw-r--r--   0        0        0       57 2024-04-28 00:02:48.541096 ibis_framework-9.0.0.dev686/ibis/backends/duckdb/tests/snapshots/test_geospatial/test_geospatial_unary_snapshot/as_text/out.sql
--rw-r--r--   0        0        0       58 2024-04-28 00:02:48.541096 ibis_framework-9.0.0.dev686/ibis/backends/duckdb/tests/snapshots/test_geospatial/test_geospatial_unary_snapshot/n_points/out.sql
--rw-r--r--   0        0        0       87 2024-04-28 00:02:48.541096 ibis_framework-9.0.0.dev686/ibis/backends/duckdb/tests/snapshots/test_geospatial/test_literal_geospatial_explicit/expr0/out.sql
--rw-r--r--   0        0        0       83 2024-04-28 00:02:48.541096 ibis_framework-9.0.0.dev686/ibis/backends/duckdb/tests/snapshots/test_geospatial/test_literal_geospatial_explicit/expr0-POINT_1_0/out.sql
--rw-r--r--   0        0        0       87 2024-04-28 00:02:48.541096 ibis_framework-9.0.0.dev686/ibis/backends/duckdb/tests/snapshots/test_geospatial/test_literal_geospatial_explicit/expr1/out.sql
--rw-r--r--   0        0        0      102 2024-04-28 00:02:48.541096 ibis_framework-9.0.0.dev686/ibis/backends/duckdb/tests/snapshots/test_geospatial/test_literal_geospatial_inferred/shp0-0_0/out.sql
--rw-r--r--   0        0        0      102 2024-04-28 00:02:48.541096 ibis_framework-9.0.0.dev686/ibis/backends/duckdb/tests/snapshots/test_geospatial/test_literal_geospatial_inferred/shp1-1_1/out.sql
--rw-r--r--   0        0        0      102 2024-04-28 00:02:48.541096 ibis_framework-9.0.0.dev686/ibis/backends/duckdb/tests/snapshots/test_geospatial/test_literal_geospatial_inferred/shp2-2_2/out.sql
--rw-r--r--   0        0        0      117 2024-04-28 00:02:48.541096 ibis_framework-9.0.0.dev686/ibis/backends/duckdb/tests/snapshots/test_geospatial/test_literal_geospatial_inferred/shp3-0_0_1_1_2_2/out.sql
--rw-r--r--   0        0        0      117 2024-04-28 00:02:48.541096 ibis_framework-9.0.0.dev686/ibis/backends/duckdb/tests/snapshots/test_geospatial/test_literal_geospatial_inferred/shp4-2_2_1_1_0_0/out.sql
--rw-r--r--   0        0        0      121 2024-04-28 00:02:48.541096 ibis_framework-9.0.0.dev686/ibis/backends/duckdb/tests/snapshots/test_geospatial/test_literal_geospatial_inferred/shp5-0_0_1_1_2_2_0_0/out.sql
--rw-r--r--   0        0        0      128 2024-04-28 00:02:48.541096 ibis_framework-9.0.0.dev686/ibis/backends/duckdb/tests/snapshots/test_geospatial/test_literal_geospatial_inferred/shp6-0_0_1_1_2_2_0_0/out.sql
--rw-r--r--   0        0        0      141 2024-04-28 00:02:48.541096 ibis_framework-9.0.0.dev686/ibis/backends/duckdb/tests/snapshots/test_geospatial/test_literal_geospatial_inferred/shp7-0_0_1_1_2_2_2_2_1_1_0_0/out.sql
--rw-r--r--   0        0        0      123 2024-04-28 00:02:48.541096 ibis_framework-9.0.0.dev686/ibis/backends/duckdb/tests/snapshots/test_geospatial/test_literal_geospatial_inferred/shp8-0_0_1_1_2_2/out.sql
--rw-r--r--   0        0        0     8840 2024-04-28 00:02:48.541096 ibis_framework-9.0.0.dev686/ibis/backends/duckdb/tests/test_client.py
--rw-r--r--   0        0        0     3612 2024-04-28 00:02:48.541096 ibis_framework-9.0.0.dev686/ibis/backends/duckdb/tests/test_datatypes.py
--rw-r--r--   0        0        0    11305 2024-04-28 00:02:48.541096 ibis_framework-9.0.0.dev686/ibis/backends/duckdb/tests/test_geospatial.py
--rw-r--r--   0        0        0    16393 2024-04-28 00:02:48.541096 ibis_framework-9.0.0.dev686/ibis/backends/duckdb/tests/test_register.py
--rw-r--r--   0        0        0     2171 2024-04-28 00:02:48.541096 ibis_framework-9.0.0.dev686/ibis/backends/duckdb/tests/test_udf.py
--rw-r--r--   0        0        0    15343 2024-04-28 00:02:48.541096 ibis_framework-9.0.0.dev686/ibis/backends/exasol/__init__.py
--rw-r--r--   0        0        0     8424 2024-04-28 00:02:48.541096 ibis_framework-9.0.0.dev686/ibis/backends/exasol/compiler.py
--rw-r--r--   0        0        0     1260 2024-04-28 00:02:48.541096 ibis_framework-9.0.0.dev686/ibis/backends/exasol/converter.py
--rw-r--r--   0        0        0        0 2024-04-28 00:02:48.541096 ibis_framework-9.0.0.dev686/ibis/backends/exasol/tests/__init__.py
--rw-r--r--   0        0        0     3304 2024-04-28 00:02:48.541096 ibis_framework-9.0.0.dev686/ibis/backends/exasol/tests/conftest.py
--rw-r--r--   0        0        0    33532 2024-04-28 00:02:48.541096 ibis_framework-9.0.0.dev686/ibis/backends/flink/__init__.py
--rw-r--r--   0        0        0    20169 2024-04-28 00:02:48.541096 ibis_framework-9.0.0.dev686/ibis/backends/flink/compiler.py
--rw-r--r--   0        0        0     6885 2024-04-28 00:02:48.541096 ibis_framework-9.0.0.dev686/ibis/backends/flink/datatypes.py
--rw-r--r--   0        0        0    10797 2024-04-28 00:02:48.541096 ibis_framework-9.0.0.dev686/ibis/backends/flink/ddl.py
--rw-r--r--   0        0        0        0 2024-04-28 00:02:48.541096 ibis_framework-9.0.0.dev686/ibis/backends/flink/tests/__init__.py
--rw-r--r--   0        0        0     5889 2024-04-28 00:02:48.541096 ibis_framework-9.0.0.dev686/ibis/backends/flink/tests/conftest.py
--rw-r--r--   0        0        0      237 2024-04-28 00:02:48.541096 ibis_framework-9.0.0.dev686/ibis/backends/flink/tests/snapshots/test_compiler/test_complex_filtered_agg/out.sql
--rw-r--r--   0        0        0      246 2024-04-28 00:02:48.541096 ibis_framework-9.0.0.dev686/ibis/backends/flink/tests/snapshots/test_compiler/test_complex_groupby_aggregation/out.sql
--rw-r--r--   0        0        0      220 2024-04-28 00:02:48.541096 ibis_framework-9.0.0.dev686/ibis/backends/flink/tests/snapshots/test_compiler/test_complex_projections/out.sql
--rw-r--r--   0        0        0       92 2024-04-28 00:02:48.541096 ibis_framework-9.0.0.dev686/ibis/backends/flink/tests/snapshots/test_compiler/test_count_star/out.sql
--rw-r--r--   0        0        0       65 2024-04-28 00:02:48.541096 ibis_framework-9.0.0.dev686/ibis/backends/flink/tests/snapshots/test_compiler/test_extract_fields/day/out.sql
--rw-r--r--   0        0        0       58 2024-04-28 00:02:48.541096 ibis_framework-9.0.0.dev686/ibis/backends/flink/tests/snapshots/test_compiler/test_extract_fields/day_of_year/out.sql
--rw-r--r--   0        0        0       66 2024-04-28 00:02:48.541096 ibis_framework-9.0.0.dev686/ibis/backends/flink/tests/snapshots/test_compiler/test_extract_fields/hour/out.sql
--rw-r--r--   0        0        0       68 2024-04-28 00:02:48.541096 ibis_framework-9.0.0.dev686/ibis/backends/flink/tests/snapshots/test_compiler/test_extract_fields/minute/out.sql
--rw-r--r--   0        0        0       67 2024-04-28 00:02:48.541096 ibis_framework-9.0.0.dev686/ibis/backends/flink/tests/snapshots/test_compiler/test_extract_fields/month/out.sql
--rw-r--r--   0        0        0       69 2024-04-28 00:02:48.541096 ibis_framework-9.0.0.dev686/ibis/backends/flink/tests/snapshots/test_compiler/test_extract_fields/quarter/out.sql
--rw-r--r--   0        0        0       68 2024-04-28 00:02:48.541096 ibis_framework-9.0.0.dev686/ibis/backends/flink/tests/snapshots/test_compiler/test_extract_fields/second/out.sql
--rw-r--r--   0        0        0       66 2024-04-28 00:02:48.541096 ibis_framework-9.0.0.dev686/ibis/backends/flink/tests/snapshots/test_compiler/test_extract_fields/week_of_year/out.sql
--rw-r--r--   0        0        0       66 2024-04-28 00:02:48.541096 ibis_framework-9.0.0.dev686/ibis/backends/flink/tests/snapshots/test_compiler/test_extract_fields/year/out.sql
--rw-r--r--   0        0        0      260 2024-04-28 00:02:48.541096 ibis_framework-9.0.0.dev686/ibis/backends/flink/tests/snapshots/test_compiler/test_filter/out.sql
--rw-r--r--   0        0        0      225 2024-04-28 00:02:48.545096 ibis_framework-9.0.0.dev686/ibis/backends/flink/tests/snapshots/test_compiler/test_having/out.sql
--rw-r--r--   0        0        0      132 2024-04-28 00:02:48.545096 ibis_framework-9.0.0.dev686/ibis/backends/flink/tests/snapshots/test_compiler/test_simple_filtered_agg/out.sql
--rw-r--r--   0        0        0       55 2024-04-28 00:02:48.545096 ibis_framework-9.0.0.dev686/ibis/backends/flink/tests/snapshots/test_compiler/test_sum/out.sql
--rw-r--r--   0        0        0      117 2024-04-28 00:02:48.545096 ibis_framework-9.0.0.dev686/ibis/backends/flink/tests/snapshots/test_compiler/test_timestamp_from_unix/timestamp_ms/out.sql
--rw-r--r--   0        0        0      112 2024-04-28 00:02:48.545096 ibis_framework-9.0.0.dev686/ibis/backends/flink/tests/snapshots/test_compiler/test_timestamp_from_unix/timestamp_s/out.sql
--rw-r--r--   0        0        0      202 2024-04-28 00:02:48.545096 ibis_framework-9.0.0.dev686/ibis/backends/flink/tests/snapshots/test_compiler/test_value_counts/out.sql
--rw-r--r--   0        0        0      275 2024-04-28 00:02:48.545096 ibis_framework-9.0.0.dev686/ibis/backends/flink/tests/snapshots/test_compiler/test_window_aggregation/out.sql
--rw-r--r--   0        0        0      719 2024-04-28 00:02:48.545096 ibis_framework-9.0.0.dev686/ibis/backends/flink/tests/snapshots/test_compiler/test_window_topn/out.sql
--rw-r--r--   0        0        0      129 2024-04-28 00:02:48.545096 ibis_framework-9.0.0.dev686/ibis/backends/flink/tests/snapshots/test_compiler/test_windowing_tvf/cumulate/out.sql
--rw-r--r--   0        0        0      124 2024-04-28 00:02:48.545096 ibis_framework-9.0.0.dev686/ibis/backends/flink/tests/snapshots/test_compiler/test_windowing_tvf/hop/out.sql
--rw-r--r--   0        0        0       99 2024-04-28 00:02:48.545096 ibis_framework-9.0.0.dev686/ibis/backends/flink/tests/snapshots/test_compiler/test_windowing_tvf/tumble/out.sql
--rw-r--r--   0        0        0      160 2024-04-28 00:02:48.545096 ibis_framework-9.0.0.dev686/ibis/backends/flink/tests/snapshots/test_window/test_range_window/out.sql
--rw-r--r--   0        0        0      139 2024-04-28 00:02:48.545096 ibis_framework-9.0.0.dev686/ibis/backends/flink/tests/snapshots/test_window/test_rows_window/out.sql
--rw-r--r--   0        0        0     3949 2024-04-28 00:02:48.545096 ibis_framework-9.0.0.dev686/ibis/backends/flink/tests/test_compiler.py
--rw-r--r--   0        0        0     1181 2024-04-28 00:02:48.545096 ibis_framework-9.0.0.dev686/ibis/backends/flink/tests/test_datatypes.py
--rw-r--r--   0        0        0    16229 2024-04-28 00:02:48.545096 ibis_framework-9.0.0.dev686/ibis/backends/flink/tests/test_ddl.py
--rw-r--r--   0        0        0     4795 2024-04-28 00:02:48.545096 ibis_framework-9.0.0.dev686/ibis/backends/flink/tests/test_join.py
--rw-r--r--   0        0        0     1732 2024-04-28 00:02:48.545096 ibis_framework-9.0.0.dev686/ibis/backends/flink/tests/test_memtable.py
--rw-r--r--   0        0        0      632 2024-04-28 00:02:48.545096 ibis_framework-9.0.0.dev686/ibis/backends/flink/tests/test_udf.py
--rw-r--r--   0        0        0     1869 2024-04-28 00:02:48.545096 ibis_framework-9.0.0.dev686/ibis/backends/flink/tests/test_window.py
--rw-r--r--   0        0        0    11772 2024-04-28 00:02:48.545096 ibis_framework-9.0.0.dev686/ibis/backends/flink/utils.py
--rw-r--r--   0        0        0    40717 2024-04-28 00:02:48.545096 ibis_framework-9.0.0.dev686/ibis/backends/impala/__init__.py
--rw-r--r--   0        0        0    10149 2024-04-28 00:02:48.545096 ibis_framework-9.0.0.dev686/ibis/backends/impala/client.py
--rw-r--r--   0        0        0    11890 2024-04-28 00:02:48.545096 ibis_framework-9.0.0.dev686/ibis/backends/impala/compiler.py
--rw-r--r--   0        0        0    17678 2024-04-28 00:02:48.545096 ibis_framework-9.0.0.dev686/ibis/backends/impala/ddl.py
--rw-r--r--   0        0        0     8596 2024-04-28 00:02:48.545096 ibis_framework-9.0.0.dev686/ibis/backends/impala/metadata.py
--rw-r--r--   0        0        0        0 2024-04-28 00:02:48.545096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/__init__.py
--rw-r--r--   0        0        0     6916 2024-04-28 00:02:48.545096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/conftest.py
--rw-r--r--   0        0        0      212 2024-04-28 00:02:48.545096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/mocks.py
--rw-r--r--   0        0        0      138 2024-04-28 00:02:48.545096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_analytic_functions/test_analytic_exprs/first/out.sql
--rw-r--r--   0        0        0      129 2024-04-28 00:02:48.545096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_analytic_functions/test_analytic_exprs/lag_arg/out.sql
--rw-r--r--   0        0        0      123 2024-04-28 00:02:48.545096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_analytic_functions/test_analytic_exprs/lag_default/out.sql
--rw-r--r--   0        0        0      132 2024-04-28 00:02:48.545096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_analytic_functions/test_analytic_exprs/lag_explicit_default/out.sql
--rw-r--r--   0        0        0      136 2024-04-28 00:02:48.545096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_analytic_functions/test_analytic_exprs/last/out.sql
--rw-r--r--   0        0        0      131 2024-04-28 00:02:48.545096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_analytic_functions/test_analytic_exprs/lead_arg/out.sql
--rw-r--r--   0        0        0      125 2024-04-28 00:02:48.545096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_analytic_functions/test_analytic_exprs/lead_default/out.sql
--rw-r--r--   0        0        0      134 2024-04-28 00:02:48.545096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_analytic_functions/test_analytic_exprs/lead_explicit_default/out.sql
--rw-r--r--   0        0        0      119 2024-04-28 00:02:48.545096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_analytic_functions/test_analytic_exprs/ntile/out.sql
--rw-r--r--   0        0        0      126 2024-04-28 00:02:48.545096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_analytic_functions/test_analytic_exprs/percent_rank/out.sql
--rw-r--r--   0        0        0      695 2024-04-28 00:02:48.545096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_bucket_histogram/test_bucket_assign_labels/out.sql
--rw-r--r--   0        0        0      331 2024-04-28 00:02:48.545096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_bucket_histogram/test_bucket_to_case/close_extreme_false/out.sql
--rw-r--r--   0        0        0      331 2024-04-28 00:02:48.545096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_bucket_histogram/test_bucket_to_case/close_extreme_false_closed_right/out.sql
--rw-r--r--   0        0        0      399 2024-04-28 00:02:48.545096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_bucket_histogram/test_bucket_to_case/close_extreme_false_include_under_include_over/out.sql
--rw-r--r--   0        0        0      332 2024-04-28 00:02:48.545096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_bucket_histogram/test_bucket_to_case/closed_right/out.sql
--rw-r--r--   0        0        0      365 2024-04-28 00:02:48.545096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_bucket_histogram/test_bucket_to_case/closed_right_close_extreme_false_include_under/out.sql
--rw-r--r--   0        0        0      158 2024-04-28 00:02:48.545096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_bucket_histogram/test_bucket_to_case/closed_right_include_over_include_under/out.sql
--rw-r--r--   0        0        0      332 2024-04-28 00:02:48.545096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_bucket_histogram/test_bucket_to_case/default/out.sql
--rw-r--r--   0        0        0      158 2024-04-28 00:02:48.545096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_bucket_histogram/test_bucket_to_case/include_over_include_under0/out.sql
--rw-r--r--   0        0        0      184 2024-04-28 00:02:48.545096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_bucket_histogram/test_bucket_to_case/include_over_include_under1/out.sql
--rw-r--r--   0        0        0      189 2024-04-28 00:02:48.545096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_bucket_histogram/test_bucket_to_case/include_over_include_under2/out.sql
--rw-r--r--   0        0        0      365 2024-04-28 00:02:48.545096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_bucket_histogram/test_bucket_to_case/include_under/out.sql
--rw-r--r--   0        0        0      399 2024-04-28 00:02:48.545096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_bucket_histogram/test_bucket_to_case/include_under_include_over/out.sql
--rw-r--r--   0        0        0       89 2024-04-28 00:02:48.549096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_case_exprs/test_decimal_fillna_cast_arg/fillna_l_extendedprice/out.sql
--rw-r--r--   0        0        0       91 2024-04-28 00:02:48.549096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_case_exprs/test_decimal_fillna_cast_arg/fillna_l_extendedprice_double/out.sql
--rw-r--r--   0        0        0       84 2024-04-28 00:02:48.549096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_case_exprs/test_decimal_fillna_cast_arg/fillna_l_quantity/out.sql
--rw-r--r--   0        0        0      110 2024-04-28 00:02:48.549096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_case_exprs/test_identical_to/out.sql
--rw-r--r--   0        0        0       80 2024-04-28 00:02:48.549096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_case_exprs/test_identical_to_special_case/out.sql
--rw-r--r--   0        0        0      102 2024-04-28 00:02:48.549096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_case_exprs/test_ifelse_use_if/out.sql
--rw-r--r--   0        0        0       88 2024-04-28 00:02:48.549096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_case_exprs/test_isnull_1_0/out1.sql
--rw-r--r--   0        0        0       98 2024-04-28 00:02:48.549096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_case_exprs/test_isnull_1_0/out2.sql
--rw-r--r--   0        0        0      148 2024-04-28 00:02:48.549096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_case_exprs/test_nullif_ifnull/nullif_boolean/out.sql
--rw-r--r--   0        0        0      118 2024-04-28 00:02:48.549096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_case_exprs/test_nullif_ifnull/nullif_input/out.sql
--rw-r--r--   0        0        0      152 2024-04-28 00:02:48.549096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_case_exprs/test_nullif_ifnull/nullif_negate_boolean/out.sql
--rw-r--r--   0        0        0      198 2024-04-28 00:02:48.549096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_case_exprs/test_search_case/out.sql
--rw-r--r--   0        0        0      139 2024-04-28 00:02:48.549096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_case_exprs/test_simple_case/out.sql
--rw-r--r--   0        0        0      103 2024-04-28 00:02:48.549096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_coalesce_greater_least/test_varargs_functions/coalesce_columns/out.sql
--rw-r--r--   0        0        0       94 2024-04-28 00:02:48.549096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_coalesce_greater_least/test_varargs_functions/coalesce_scalar/out.sql
--rw-r--r--   0        0        0      103 2024-04-28 00:02:48.549096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_coalesce_greater_least/test_varargs_functions/greatest_columns/out.sql
--rw-r--r--   0        0        0       94 2024-04-28 00:02:48.549096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_coalesce_greater_least/test_varargs_functions/greatest_scalar/out.sql
--rw-r--r--   0        0        0       97 2024-04-28 00:02:48.549096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_coalesce_greater_least/test_varargs_functions/least_columns/out.sql
--rw-r--r--   0        0        0       88 2024-04-28 00:02:48.549096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_coalesce_greater_least/test_varargs_functions/least_scalar/out.sql
--rw-r--r--   0        0        0       50 2024-04-28 00:02:48.549096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_ddl_compilation/test_add_partition/out.sql
--rw-r--r--   0        0        0       48 2024-04-28 00:02:48.549096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_ddl_compilation/test_add_partition_string_key/out.sql
--rw-r--r--   0        0        0       80 2024-04-28 00:02:48.549096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_ddl_compilation/test_add_partition_with_props/out.sql
--rw-r--r--   0        0        0       80 2024-04-28 00:02:48.549096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_ddl_compilation/test_alter_partition_properties/out1.sql
--rw-r--r--   0        0        0       66 2024-04-28 00:02:48.549096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_ddl_compilation/test_alter_partition_properties/out2.sql
--rw-r--r--   0        0        0       93 2024-04-28 00:02:48.549096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_ddl_compilation/test_alter_partition_properties/out3.sql
--rw-r--r--   0        0        0       82 2024-04-28 00:02:48.549096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_ddl_compilation/test_alter_partition_properties/out4.sql
--rw-r--r--   0        0        0       80 2024-04-28 00:02:48.549096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_ddl_compilation/test_alter_table_properties/out1.sql
--rw-r--r--   0        0        0       66 2024-04-28 00:02:48.549096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_ddl_compilation/test_alter_table_properties/out2.sql
--rw-r--r--   0        0        0       93 2024-04-28 00:02:48.549096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_ddl_compilation/test_alter_table_properties/out3.sql
--rw-r--r--   0        0        0       82 2024-04-28 00:02:48.549096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_ddl_compilation/test_alter_table_properties/out4.sql
--rw-r--r--   0        0        0       90 2024-04-28 00:02:48.549096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_ddl_compilation/test_avro_other_formats/out.sql
--rw-r--r--   0        0        0       47 2024-04-28 00:02:48.549096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_ddl_compilation/test_cache_table_pool_name/out1.sql
--rw-r--r--   0        0        0       47 2024-04-28 00:02:48.549096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_ddl_compilation/test_cache_table_pool_name/out2.sql
--rw-r--r--   0        0        0      114 2024-04-28 00:02:48.549096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_ddl_compilation/test_create_external_table_as/out.sql
--rw-r--r--   0        0        0      491 2024-04-28 00:02:48.549096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_ddl_compilation/test_create_external_table_avro/out.sql
--rw-r--r--   0        0        0      241 2024-04-28 00:02:48.549096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_ddl_compilation/test_create_table_delimited/out.sql
--rw-r--r--   0        0        0      128 2024-04-28 00:02:48.549096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_ddl_compilation/test_create_table_like_parquet/out.sql
--rw-r--r--   0        0        0      382 2024-04-28 00:02:48.549096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_ddl_compilation/test_create_table_parquet/out.sql
--rw-r--r--   0        0        0      106 2024-04-28 00:02:48.549096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_ddl_compilation/test_create_table_parquet_like_other/out.sql
--rw-r--r--   0        0        0      140 2024-04-28 00:02:48.549096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_ddl_compilation/test_create_table_parquet_with_schema/out.sql
--rw-r--r--   0        0        0      126 2024-04-28 00:02:48.549096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_ddl_compilation/test_create_table_with_location_compile/out.sql
--rw-r--r--   0        0        0       51 2024-04-28 00:02:48.549096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_ddl_compilation/test_drop_partition/out.sql
--rw-r--r--   0        0        0       22 2024-04-28 00:02:48.549096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_ddl_compilation/test_drop_table_compile/out1.sql
--rw-r--r--   0        0        0       32 2024-04-28 00:02:48.549096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_ddl_compilation/test_drop_table_compile/out2.sql
--rw-r--r--   0        0        0      102 2024-04-28 00:02:48.549096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_ddl_compilation/test_load_data_partitioned/out1.sql
--rw-r--r--   0        0        0      112 2024-04-28 00:02:48.549096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_ddl_compilation/test_load_data_partitioned/out2.sql
--rw-r--r--   0        0        0       71 2024-04-28 00:02:48.549096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_ddl_compilation/test_load_data_unpartitioned/out1.sql
--rw-r--r--   0        0        0       81 2024-04-28 00:02:48.549096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_ddl_compilation/test_load_data_unpartitioned/out2.sql
--rw-r--r--   0        0        0      385 2024-04-28 00:02:48.549096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_ddl_compilation/test_no_overwrite/out.sql
--rw-r--r--   0        0        0       88 2024-04-28 00:02:48.549096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_ddl_compilation/test_select_basics/out1.sql
--rw-r--r--   0        0        0       93 2024-04-28 00:02:48.549096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_ddl_compilation/test_select_basics/out2.sql
--rw-r--r--   0        0        0      214 2024-04-28 00:02:48.549096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_exprs/test_filter_with_analytic/out.sql
--rw-r--r--   0        0        0      161 2024-04-28 00:02:48.549096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_exprs/test_named_from_filter_group_by/abc.sql
--rw-r--r--   0        0        0      161 2024-04-28 00:02:48.549096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_exprs/test_named_from_filter_group_by/foo.sql
--rw-r--r--   0        0        0      132 2024-04-28 00:02:48.549096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_exprs/test_nunique_where/out.sql
--rw-r--r--   0        0        0      111 2024-04-28 00:02:48.549096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_exprs/test_where_with_timestamp/out.sql
--rw-r--r--   0        0        0       83 2024-04-28 00:02:48.549096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_in_not_in/test_field_in_literals/isin/out.sql
--rw-r--r--   0        0        0      102 2024-04-28 00:02:48.549096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_in_not_in/test_field_in_literals/notin/out.sql
--rw-r--r--   0        0        0      196 2024-04-28 00:02:48.549096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_in_not_in/test_isin_notin_in_select/isin/out.sql
--rw-r--r--   0        0        0      210 2024-04-28 00:02:48.549096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_in_not_in/test_isin_notin_in_select/notin/out.sql
--rw-r--r--   0        0        0       85 2024-04-28 00:02:48.549096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_in_not_in/test_literal_in_fields/isin/out.sql
--rw-r--r--   0        0        0      104 2024-04-28 00:02:48.549096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_in_not_in/test_literal_in_fields/notin/out.sql
--rw-r--r--   0        0        0      161 2024-04-28 00:02:48.549096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_sql/test_group_by_with_window_preserves_range/out.sql
--rw-r--r--   0        0        0      114 2024-04-28 00:02:48.549096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_sql/test_is_parens/isnull/out.sql
--rw-r--r--   0        0        0      102 2024-04-28 00:02:48.549096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_sql/test_is_parens/notnull/out.sql
--rw-r--r--   0        0        0      130 2024-04-28 00:02:48.553096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_sql/test_is_parens_identical_to/out.sql
--rw-r--r--   0        0        0     1165 2024-04-28 00:02:48.553096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_sql/test_join_aliasing/out.sql
--rw-r--r--   0        0        0     1547 2024-04-28 00:02:48.553096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_sql/test_join_key_name/out.sql
--rw-r--r--   0        0        0      785 2024-04-28 00:02:48.553096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_sql/test_join_key_name2/out.sql
--rw-r--r--   0        0        0      111 2024-04-28 00:02:48.553096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_sql/test_join_no_predicates_for_impala/cross_join/out.sql
--rw-r--r--   0        0        0      121 2024-04-28 00:02:48.553096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_sql/test_join_no_predicates_for_impala/inner_join/out.sql
--rw-r--r--   0        0        0      126 2024-04-28 00:02:48.553096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_sql/test_join_no_predicates_for_impala/left_join/out.sql
--rw-r--r--   0        0        0      126 2024-04-28 00:02:48.553096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_sql/test_join_no_predicates_for_impala/outer_join/out.sql
--rw-r--r--   0        0        0      183 2024-04-28 00:02:48.553096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_sql/test_join_with_nested_or_condition/out.sql
--rw-r--r--   0        0        0      241 2024-04-28 00:02:48.553096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_sql/test_join_with_nested_xor_condition/out.sql
--rw-r--r--   0        0        0      397 2024-04-28 00:02:48.553096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_sql/test_limit_cte_extract/out.sql
--rw-r--r--   0        0        0       93 2024-04-28 00:02:48.553096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_sql/test_logically_negate_complex_boolean_expr/out.sql
--rw-r--r--   0        0        0      268 2024-04-28 00:02:48.553096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_sql/test_multiple_filters/out.sql
--rw-r--r--   0        0        0      289 2024-04-28 00:02:48.553096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_sql/test_multiple_filters2/out.sql
--rw-r--r--   0        0        0      337 2024-04-28 00:02:48.553096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_sql/test_nested_join_base/out.sql
--rw-r--r--   0        0        0      914 2024-04-28 00:02:48.553096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_sql/test_nested_join_multiple_ctes/out.sql
--rw-r--r--   0        0        0      557 2024-04-28 00:02:48.553096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_sql/test_nested_joins_single_cte/out.sql
--rw-r--r--   0        0        0       71 2024-04-28 00:02:48.553096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_sql/test_relabel_projection/out.sql
--rw-r--r--   0        0        0      148 2024-04-28 00:02:48.553096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_string_builtins/test_find/out.sql
--rw-r--r--   0        0        0       97 2024-04-28 00:02:48.553096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_string_builtins/test_string_builtins/ascii_str/out.sql
--rw-r--r--   0        0        0      406 2024-04-28 00:02:48.553096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_string_builtins/test_string_builtins/capitalize/out.sql
--rw-r--r--   0        0        0      109 2024-04-28 00:02:48.553096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_string_builtins/test_string_builtins/extract_host/out.sql
--rw-r--r--   0        0        0      111 2024-04-28 00:02:48.553096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_string_builtins/test_string_builtins/find/out.sql
--rw-r--r--   0        0        0      131 2024-04-28 00:02:48.553096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_string_builtins/test_string_builtins/find_in_set_multiple/out.sql
--rw-r--r--   0        0        0      126 2024-04-28 00:02:48.553096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_string_builtins/test_string_builtins/find_in_set_single/out.sql
--rw-r--r--   0        0        0      121 2024-04-28 00:02:48.553096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_string_builtins/test_string_builtins/find_with_offset/out.sql
--rw-r--r--   0        0        0       99 2024-04-28 00:02:48.553096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_string_builtins/test_string_builtins/length/out.sql
--rw-r--r--   0        0        0      112 2024-04-28 00:02:48.553096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_string_builtins/test_string_builtins/like/out.sql
--rw-r--r--   0        0        0      184 2024-04-28 00:02:48.553096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_string_builtins/test_string_builtins/like_multiple/out.sql
--rw-r--r--   0        0        0       95 2024-04-28 00:02:48.553096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_string_builtins/test_string_builtins/lower/out.sql
--rw-r--r--   0        0        0      105 2024-04-28 00:02:48.553096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_string_builtins/test_string_builtins/lpad_char/out.sql
--rw-r--r--   0        0        0      107 2024-04-28 00:02:48.553096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_string_builtins/test_string_builtins/lpad_default/out.sql
--rw-r--r--   0        0        0       92 2024-04-28 00:02:48.553096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_string_builtins/test_string_builtins/lstrip/out.sql
--rw-r--r--   0        0        0      133 2024-04-28 00:02:48.553096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_string_builtins/test_string_builtins/re_extract/out.sql
--rw-r--r--   0        0        0      141 2024-04-28 00:02:48.553096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_string_builtins/test_string_builtins/re_replace/out.sql
--rw-r--r--   0        0        0      115 2024-04-28 00:02:48.553096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_string_builtins/test_string_builtins/re_search/out.sql
--rw-r--r--   0        0        0       99 2024-04-28 00:02:48.553096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_string_builtins/test_string_builtins/repeat/out.sql
--rw-r--r--   0        0        0       95 2024-04-28 00:02:48.553096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_string_builtins/test_string_builtins/reverse/out.sql
--rw-r--r--   0        0        0      115 2024-04-28 00:02:48.553096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_string_builtins/test_string_builtins/rlike/out.sql
--rw-r--r--   0        0        0      105 2024-04-28 00:02:48.553096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_string_builtins/test_string_builtins/rpad_char/out.sql
--rw-r--r--   0        0        0      107 2024-04-28 00:02:48.553096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_string_builtins/test_string_builtins/rpad_default/out.sql
--rw-r--r--   0        0        0       92 2024-04-28 00:02:48.553096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_string_builtins/test_string_builtins/rstrip/out.sql
--rw-r--r--   0        0        0       90 2024-04-28 00:02:48.553096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_string_builtins/test_string_builtins/strip/out.sql
--rw-r--r--   0        0        0      146 2024-04-28 00:02:48.553096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_string_builtins/test_string_builtins/strright/out.sql
--rw-r--r--   0        0        0      176 2024-04-28 00:02:48.553096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_string_builtins/test_string_builtins/substr_0_3/out.sql
--rw-r--r--   0        0        0      170 2024-04-28 00:02:48.553096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_string_builtins/test_string_builtins/substr_2/out.sql
--rw-r--r--   0        0        0      119 2024-04-28 00:02:48.553096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_string_builtins/test_string_builtins/translate/out.sql
--rw-r--r--   0        0        0       95 2024-04-28 00:02:48.553096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_string_builtins/test_string_builtins/upper/out.sql
--rw-r--r--   0        0        0       54 2024-04-28 00:02:48.553096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_string_builtins/test_string_join/out.sql
--rw-r--r--   0        0        0      171 2024-04-28 00:02:48.553096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_udf/test_create_uda/False/out.sql
--rw-r--r--   0        0        0      196 2024-04-28 00:02:48.553096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_udf/test_create_uda/True/out.sql
--rw-r--r--   0        0        0       99 2024-04-28 00:02:48.553096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_udf/test_create_udf/out.sql
--rw-r--r--   0        0        0      115 2024-04-28 00:02:48.553096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_udf/test_create_udf_type_conversions/out.sql
--rw-r--r--   0        0        0       51 2024-04-28 00:02:48.553096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_udf/test_delete_udf_aggregate/out.sql
--rw-r--r--   0        0        0       48 2024-04-28 00:02:48.553096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_udf/test_delete_udf_db/out.sql
--rw-r--r--   0        0        0       51 2024-04-28 00:02:48.553096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_udf/test_delete_udf_if_exists/out.sql
--rw-r--r--   0        0        0       41 2024-04-28 00:02:48.553096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_udf/test_delete_udf_simple/out.sql
--rw-r--r--   0        0        0       32 2024-04-28 00:02:48.553096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_udf/test_list_udafs/out.sql
--rw-r--r--   0        0        0       48 2024-04-28 00:02:48.553096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_udf/test_list_udafs_like/out.sql
--rw-r--r--   0        0        0       22 2024-04-28 00:02:48.553096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_udf/test_list_udf/out.sql
--rw-r--r--   0        0        0       38 2024-04-28 00:02:48.557096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_udf/test_list_udfs_like/out.sql
--rw-r--r--   0        0        0       73 2024-04-28 00:02:48.557096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_udf/test_sql_generation/out.sql
--rw-r--r--   0        0        0       52 2024-04-28 00:02:48.557096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_udf/test_sql_generation_from_infoclass/out.sql
--rw-r--r--   0        0        0       87 2024-04-28 00:02:48.557096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_hash/out.sql
--rw-r--r--   0        0        0       93 2024-04-28 00:02:48.557096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric/log_with_base/out.sql
--rw-r--r--   0        0        0      124 2024-04-28 00:02:48.557096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric/round_expr/out.sql
--rw-r--r--   0        0        0      107 2024-04-28 00:02:48.557096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric/round_no_args/out.sql
--rw-r--r--   0        0        0       97 2024-04-28 00:02:48.557096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric/round_two/out.sql
--rw-r--r--   0        0        0       97 2024-04-28 00:02:48.557096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric/round_zero/out.sql
--rw-r--r--   0        0        0      105 2024-04-28 00:02:48.557096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric/sign_double/out.sql
--rw-r--r--   0        0        0       87 2024-04-28 00:02:48.557096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric/sign_float/out.sql
--rw-r--r--   0        0        0      108 2024-04-28 00:02:48.557096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric/sign_tinyint/out.sql
--rw-r--r--   0        0        0       87 2024-04-28 00:02:48.557096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric_unary_builtins/double_col-abs/out.sql
--rw-r--r--   0        0        0      104 2024-04-28 00:02:48.557096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric_unary_builtins/double_col-approx_median/out.sql
--rw-r--r--   0        0        0      103 2024-04-28 00:02:48.557096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric_unary_builtins/double_col-approx_nunique/out.sql
--rw-r--r--   0        0        0      105 2024-04-28 00:02:48.557096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric_unary_builtins/double_col-ceil/out.sql
--rw-r--r--   0        0        0       87 2024-04-28 00:02:48.557096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric_unary_builtins/double_col-exp/out.sql
--rw-r--r--   0        0        0      107 2024-04-28 00:02:48.557096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric_unary_builtins/double_col-floor/out.sql
--rw-r--r--   0        0        0       85 2024-04-28 00:02:48.557096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric_unary_builtins/double_col-ln/out.sql
--rw-r--r--   0        0        0       86 2024-04-28 00:02:48.557096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric_unary_builtins/double_col-log/out.sql
--rw-r--r--   0        0        0       91 2024-04-28 00:02:48.557096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric_unary_builtins/double_col-log10/out.sql
--rw-r--r--   0        0        0       89 2024-04-28 00:02:48.557096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric_unary_builtins/double_col-log2/out.sql
--rw-r--r--   0        0        0       99 2024-04-28 00:02:48.557096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric_unary_builtins/double_col-nullif_zero/out.sql
--rw-r--r--   0        0        0       23 2024-04-28 00:02:48.557096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric_unary_builtins/double_col-nullifzero/out.sql
--rw-r--r--   0        0        0       89 2024-04-28 00:02:48.557096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric_unary_builtins/double_col-sqrt/out.sql
--rw-r--r--   0        0        0       90 2024-04-28 00:02:48.557096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric_unary_builtins/double_col-zero_ifnull/out.sql
--rw-r--r--   0        0        0       25 2024-04-28 00:02:48.557096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric_unary_builtins/double_col-zeroifnull/out.sql
--rw-r--r--   0        0        0       81 2024-04-28 00:02:48.557096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric_unary_builtins/int_col-abs/out.sql
--rw-r--r--   0        0        0       98 2024-04-28 00:02:48.557096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric_unary_builtins/int_col-approx_median/out.sql
--rw-r--r--   0        0        0       97 2024-04-28 00:02:48.557096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric_unary_builtins/int_col-approx_nunique/out.sql
--rw-r--r--   0        0        0       99 2024-04-28 00:02:48.557096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric_unary_builtins/int_col-ceil/out.sql
--rw-r--r--   0        0        0       81 2024-04-28 00:02:48.557096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric_unary_builtins/int_col-exp/out.sql
--rw-r--r--   0        0        0      101 2024-04-28 00:02:48.557096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric_unary_builtins/int_col-floor/out.sql
--rw-r--r--   0        0        0       79 2024-04-28 00:02:48.557096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric_unary_builtins/int_col-ln/out.sql
--rw-r--r--   0        0        0       80 2024-04-28 00:02:48.557096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric_unary_builtins/int_col-log/out.sql
--rw-r--r--   0        0        0       85 2024-04-28 00:02:48.557096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric_unary_builtins/int_col-log10/out.sql
--rw-r--r--   0        0        0       83 2024-04-28 00:02:48.557096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric_unary_builtins/int_col-log2/out.sql
--rw-r--r--   0        0        0       93 2024-04-28 00:02:48.557096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric_unary_builtins/int_col-nullif_zero/out.sql
--rw-r--r--   0        0        0       20 2024-04-28 00:02:48.557096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric_unary_builtins/int_col-nullifzero/out.sql
--rw-r--r--   0        0        0       83 2024-04-28 00:02:48.557096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric_unary_builtins/int_col-sqrt/out.sql
--rw-r--r--   0        0        0       87 2024-04-28 00:02:48.557096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric_unary_builtins/int_col-zero_ifnull/out.sql
--rw-r--r--   0        0        0       22 2024-04-28 00:02:48.557096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric_unary_builtins/int_col-zeroifnull/out.sql
--rw-r--r--   0        0        0      144 2024-04-28 00:02:48.557096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_reduction_where/avg/out.sql
--rw-r--r--   0        0        0      147 2024-04-28 00:02:48.557096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_reduction_where/count/out.sql
--rw-r--r--   0        0        0      143 2024-04-28 00:02:48.557096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_reduction_where/max/out.sql
--rw-r--r--   0        0        0      143 2024-04-28 00:02:48.557096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_reduction_where/min/out.sql
--rw-r--r--   0        0        0      158 2024-04-28 00:02:48.557096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_reduction_where/stddev_pop/out.sql
--rw-r--r--   0        0        0      159 2024-04-28 00:02:48.557096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_reduction_where/stddev_samp/out.sql
--rw-r--r--   0        0        0      143 2024-04-28 00:02:48.557096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_reduction_where/sum/out.sql
--rw-r--r--   0        0        0      157 2024-04-28 00:02:48.557096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_reduction_where/var_pop/out.sql
--rw-r--r--   0        0        0      153 2024-04-28 00:02:48.557096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_reduction_where/var_samp/out.sql
--rw-r--r--   0        0        0       73 2024-04-28 00:02:48.557096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_any_all/all/out.sql
--rw-r--r--   0        0        0       73 2024-04-28 00:02:48.557096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_any_all/any/out.sql
--rw-r--r--   0        0        0       92 2024-04-28 00:02:48.557096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_any_all/not_all/out.sql
--rw-r--r--   0        0        0       92 2024-04-28 00:02:48.557096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_any_all/not_any/out.sql
--rw-r--r--   0        0        0       79 2024-04-28 00:02:48.557096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_between/out.sql
--rw-r--r--   0        0        0       67 2024-04-28 00:02:48.557096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_binary_infix_operators/add/out.sql
--rw-r--r--   0        0        0       95 2024-04-28 00:02:48.557096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_binary_infix_operators/and/out.sql
--rw-r--r--   0        0        0       70 2024-04-28 00:02:48.557096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_binary_infix_operators/div/out.sql
--rw-r--r--   0        0        0       70 2024-04-28 00:02:48.557096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_binary_infix_operators/eq/out.sql
--rw-r--r--   0        0        0       77 2024-04-28 00:02:48.561096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_binary_infix_operators/ge/out.sql
--rw-r--r--   0        0        0       71 2024-04-28 00:02:48.561096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_binary_infix_operators/gt/out.sql
--rw-r--r--   0        0        0       74 2024-04-28 00:02:48.561096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_binary_infix_operators/le/out.sql
--rw-r--r--   0        0        0       68 2024-04-28 00:02:48.561096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_binary_infix_operators/lt/out.sql
--rw-r--r--   0        0        0       72 2024-04-28 00:02:48.561096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_binary_infix_operators/mul/out.sql
--rw-r--r--   0        0        0       74 2024-04-28 00:02:48.561096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_binary_infix_operators/ne/out.sql
--rw-r--r--   0        0        0       93 2024-04-28 00:02:48.561096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_binary_infix_operators/or/out.sql
--rw-r--r--   0        0        0       75 2024-04-28 00:02:48.561096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_binary_infix_operators/pow/out.sql
--rw-r--r--   0        0        0       72 2024-04-28 00:02:48.561096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_binary_infix_operators/sub/out.sql
--rw-r--r--   0        0        0      138 2024-04-28 00:02:48.561096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_binary_infix_operators/xor/out.sql
--rw-r--r--   0        0        0       76 2024-04-28 00:02:48.561096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_binary_infix_parenthesization/function_call/out.sql
--rw-r--r--   0        0        0      105 2024-04-28 00:02:48.561096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_binary_infix_parenthesization/negation/out.sql
--rw-r--r--   0        0        0       96 2024-04-28 00:02:48.561096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_binary_infix_parenthesization/parens_left/out.sql
--rw-r--r--   0        0        0       79 2024-04-28 00:02:48.561096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_casts/a-int16/out.sql
--rw-r--r--   0        0        0       74 2024-04-28 00:02:48.561096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_casts/a-int32/out.sql
--rw-r--r--   0        0        0       77 2024-04-28 00:02:48.561096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_casts/a-int64/out.sql
--rw-r--r--   0        0        0       78 2024-04-28 00:02:48.561096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_casts/a-string/out.sql
--rw-r--r--   0        0        0       77 2024-04-28 00:02:48.561096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_casts/d-int8/out.sql
--rw-r--r--   0        0        0       79 2024-04-28 00:02:48.561096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_casts/g-double/out.sql
--rw-r--r--   0        0        0       84 2024-04-28 00:02:48.561096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_casts/g-timestamp/out.sql
--rw-r--r--   0        0        0       15 2024-04-28 00:02:48.561096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_correlated_predicate_subquery/out.sql
--rw-r--r--   0        0        0      189 2024-04-28 00:02:48.561096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_correlated_predicate_subquery/out1.sql
--rw-r--r--   0        0        0      189 2024-04-28 00:02:48.561096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_correlated_predicate_subquery/out2.sql
--rw-r--r--   0        0        0       94 2024-04-28 00:02:48.561096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_decimal_casts/column/out.sql
--rw-r--r--   0        0        0       83 2024-04-28 00:02:48.561096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_decimal_casts/literal/out.sql
--rw-r--r--   0        0        0       93 2024-04-28 00:02:48.561096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_isnull_notnull/compound_isnull/out.sql
--rw-r--r--   0        0        0       64 2024-04-28 00:02:48.561096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_isnull_notnull/isnull/out.sql
--rw-r--r--   0        0        0       69 2024-04-28 00:02:48.561096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_isnull_notnull/notnull/out.sql
--rw-r--r--   0        0        0       41 2024-04-28 00:02:48.561096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_literals/embedded_double_quote/out.sql
--rw-r--r--   0        0        0       34 2024-04-28 00:02:48.561096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_literals/embedded_single_quote/out.sql
--rw-r--r--   0        0        0       25 2024-04-28 00:02:48.561096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_literals/false/out.sql
--rw-r--r--   0        0        0       21 2024-04-28 00:02:48.561096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_literals/float/out.sql
--rw-r--r--   0        0        0       17 2024-04-28 00:02:48.561096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_literals/int/out.sql
--rw-r--r--   0        0        0       31 2024-04-28 00:02:48.561096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_literals/simple/out.sql
--rw-r--r--   0        0        0       23 2024-04-28 00:02:48.561096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_literals/true/out.sql
--rw-r--r--   0        0        0       70 2024-04-28 00:02:48.561096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_misc_conditionals/out.sql
--rw-r--r--   0        0        0       68 2024-04-28 00:02:48.561096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_named_expressions/cast/out.sql
--rw-r--r--   0        0        0       83 2024-04-28 00:02:48.561096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_named_expressions/compound_expr/out.sql
--rw-r--r--   0        0        0       58 2024-04-28 00:02:48.561096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_named_expressions/spaces/out.sql
--rw-r--r--   0        0        0       67 2024-04-28 00:02:48.561096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_negate/a/out.sql
--rw-r--r--   0        0        0       67 2024-04-28 00:02:48.561096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_negate/f/out.sql
--rw-r--r--   0        0        0       67 2024-04-28 00:02:48.561096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_negate/h/out.sql
--rw-r--r--   0        0        0      152 2024-04-28 00:02:48.561096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_sql_extract/out.sql
--rw-r--r--   0        0        0       99 2024-04-28 00:02:48.561096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_timestamp_day_of_week/full_name/out.sql
--rw-r--r--   0        0        0      115 2024-04-28 00:02:48.561096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_timestamp_day_of_week/index/out.sql
--rw-r--r--   0        0        0      121 2024-04-28 00:02:48.561096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_timestamp_deltas/days/out1.sql
--rw-r--r--   0        0        0       83 2024-04-28 00:02:48.561096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_timestamp_deltas/days/out2.sql
--rw-r--r--   0        0        0      122 2024-04-28 00:02:48.561096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_timestamp_deltas/hours/out1.sql
--rw-r--r--   0        0        0       84 2024-04-28 00:02:48.561096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_timestamp_deltas/hours/out2.sql
--rw-r--r--   0        0        0      124 2024-04-28 00:02:48.561096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_timestamp_deltas/minutes/out1.sql
--rw-r--r--   0        0        0       86 2024-04-28 00:02:48.561096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_timestamp_deltas/minutes/out2.sql
--rw-r--r--   0        0        0      123 2024-04-28 00:02:48.561096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_timestamp_deltas/months/out1.sql
--rw-r--r--   0        0        0       85 2024-04-28 00:02:48.561096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_timestamp_deltas/months/out2.sql
--rw-r--r--   0        0        0      124 2024-04-28 00:02:48.561096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_timestamp_deltas/seconds/out1.sql
--rw-r--r--   0        0        0       86 2024-04-28 00:02:48.561096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_timestamp_deltas/seconds/out2.sql
--rw-r--r--   0        0        0      122 2024-04-28 00:02:48.561096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_timestamp_deltas/weeks/out1.sql
--rw-r--r--   0        0        0       84 2024-04-28 00:02:48.561096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_timestamp_deltas/weeks/out2.sql
--rw-r--r--   0        0        0      122 2024-04-28 00:02:48.561096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_timestamp_deltas/years/out1.sql
--rw-r--r--   0        0        0       84 2024-04-28 00:02:48.561096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_timestamp_deltas/years/out2.sql
--rw-r--r--   0        0        0       78 2024-04-28 00:02:48.561096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_timestamp_extract_field/day/out.sql
--rw-r--r--   0        0        0       80 2024-04-28 00:02:48.561096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_timestamp_extract_field/hour/out.sql
--rw-r--r--   0        0        0      104 2024-04-28 00:02:48.561096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_timestamp_extract_field/microsecond/out.sql
--rw-r--r--   0        0        0      101 2024-04-28 00:02:48.565096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_timestamp_extract_field/millisecond/out.sql
--rw-r--r--   0        0        0       84 2024-04-28 00:02:48.565096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_timestamp_extract_field/minute/out.sql
--rw-r--r--   0        0        0       82 2024-04-28 00:02:48.565096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_timestamp_extract_field/month/out.sql
--rw-r--r--   0        0        0       84 2024-04-28 00:02:48.565096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_timestamp_extract_field/second/out.sql
--rw-r--r--   0        0        0       80 2024-04-28 00:02:48.565096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_timestamp_extract_field/year/out.sql
--rw-r--r--   0        0        0      145 2024-04-28 00:02:48.565096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_timestamp_from_integer/default/out.sql
--rw-r--r--   0        0        0      170 2024-04-28 00:02:48.565096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_timestamp_from_integer/ms/out.sql
--rw-r--r--   0        0        0      173 2024-04-28 00:02:48.565096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_timestamp_from_integer/us/out.sql
--rw-r--r--   0        0        0       77 2024-04-28 00:02:48.565096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_timestamp_literals/pd_timestamp/out.sql
--rw-r--r--   0        0        0       77 2024-04-28 00:02:48.565096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_timestamp_literals/pydatetime/out.sql
--rw-r--r--   0        0        0       77 2024-04-28 00:02:48.565096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_timestamp_literals/timestamp_function/out.sql
--rw-r--r--   0        0        0       48 2024-04-28 00:02:48.565096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_value_exprs/test_timestamp_now/out.sql
--rw-r--r--   0        0        0      621 2024-04-28 00:02:48.565096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_window/test_add_default_order_by/out.sql
--rw-r--r--   0        0        0      239 2024-04-28 00:02:48.565096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_window/test_aggregate_in_projection/out.sql
--rw-r--r--   0        0        0       95 2024-04-28 00:02:48.565096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_window/test_cumulative_functions/max/out1.sql
--rw-r--r--   0        0        0       95 2024-04-28 00:02:48.565096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_window/test_cumulative_functions/max/out2.sql
--rw-r--r--   0        0        0       95 2024-04-28 00:02:48.565096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_window/test_cumulative_functions/mean/out1.sql
--rw-r--r--   0        0        0       95 2024-04-28 00:02:48.565096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_window/test_cumulative_functions/mean/out2.sql
--rw-r--r--   0        0        0       95 2024-04-28 00:02:48.565096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_window/test_cumulative_functions/min/out1.sql
--rw-r--r--   0        0        0       95 2024-04-28 00:02:48.565096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_window/test_cumulative_functions/min/out2.sql
--rw-r--r--   0        0        0       95 2024-04-28 00:02:48.565096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_window/test_cumulative_functions/sum/out1.sql
--rw-r--r--   0        0        0       95 2024-04-28 00:02:48.565096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_window/test_cumulative_functions/sum/out2.sql
--rw-r--r--   0        0        0      180 2024-04-28 00:02:48.565096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_window/test_multiple_windows/out.sql
--rw-r--r--   0        0        0      151 2024-04-28 00:02:48.565096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_window/test_nested_analytic_function/out.sql
--rw-r--r--   0        0        0      104 2024-04-28 00:02:48.565096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_window/test_order_by_desc/out1.sql
--rw-r--r--   0        0        0      188 2024-04-28 00:02:48.565096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_window/test_order_by_desc/out2.sql
--rw-r--r--   0        0        0      203 2024-04-28 00:02:48.565096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_window/test_propagate_nested_windows/out.sql
--rw-r--r--   0        0        0      177 2024-04-28 00:02:48.565096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_window/test_rank_functions/out.sql
--rw-r--r--   0        0        0      244 2024-04-28 00:02:48.565096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_window/test_row_number_does_not_require_order_by/out1.sql
--rw-r--r--   0        0        0      252 2024-04-28 00:02:48.565096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_window/test_row_number_does_not_require_order_by/out2.sql
--rw-r--r--   0        0        0      244 2024-04-28 00:02:48.565096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_window/test_row_number_properly_composes_with_arithmetic/out.sql
--rw-r--r--   0        0        0       95 2024-04-28 00:02:48.565096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_window/test_window_frame_specs/cumulative/out.sql
--rw-r--r--   0        0        0       95 2024-04-28 00:02:48.565096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_window/test_window_frame_specs/foll_0/out.sql
--rw-r--r--   0        0        0      137 2024-04-28 00:02:48.565096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_window/test_window_frame_specs/foll_10_5/out.sql
--rw-r--r--   0        0        0      144 2024-04-28 00:02:48.565096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_window/test_window_frame_specs/foll_2/out.sql
--rw-r--r--   0        0        0      136 2024-04-28 00:02:48.565096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_window/test_window_frame_specs/foll_2_prec_0/out.sql
--rw-r--r--   0        0        0      137 2024-04-28 00:02:48.565096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_window/test_window_frame_specs/foll_5_10/out.sql
--rw-r--r--   0        0        0      144 2024-04-28 00:02:48.565096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_window/test_window_frame_specs/prec_0/out.sql
--rw-r--r--   0        0        0      144 2024-04-28 00:02:48.565096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_window/test_window_frame_specs/prec_5/out.sql
--rw-r--r--   0        0        0      136 2024-04-28 00:02:48.565096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_window/test_window_frame_specs/prec_5_foll_0/out.sql
--rw-r--r--   0        0        0      136 2024-04-28 00:02:48.565096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_window/test_window_frame_specs/prec_5_foll_2/out.sql
--rw-r--r--   0        0        0      137 2024-04-28 00:02:48.565096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/snapshots/test_window/test_window_frame_specs/trailing_10/out.sql
--rw-r--r--   0        0        0     1215 2024-04-28 00:02:48.565096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/test_analytic_functions.py
--rw-r--r--   0        0        0     2861 2024-04-28 00:02:48.565096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/test_bucket_histogram.py
--rw-r--r--   0        0        0     2846 2024-04-28 00:02:48.565096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/test_case_exprs.py
--rw-r--r--   0        0        0     6938 2024-04-28 00:02:48.565096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/test_client.py
--rw-r--r--   0        0        0     1076 2024-04-28 00:02:48.565096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/test_coalesce_greater_least.py
--rw-r--r--   0        0        0     8439 2024-04-28 00:02:48.565096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/test_ddl.py
--rw-r--r--   0        0        0     8913 2024-04-28 00:02:48.565096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/test_ddl_compilation.py
--rw-r--r--   0        0        0    19387 2024-04-28 00:02:48.565096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/test_exprs.py
--rw-r--r--   0        0        0     1135 2024-04-28 00:02:48.565096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/test_in_not_in.py
--rw-r--r--   0        0        0     3857 2024-04-28 00:02:48.565096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/test_metadata.py
--rw-r--r--   0        0        0     2486 2024-04-28 00:02:48.565096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/test_parquet_ddl.py
--rw-r--r--   0        0        0     7271 2024-04-28 00:02:48.565096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/test_partition.py
--rw-r--r--   0        0        0     1704 2024-04-28 00:02:48.565096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/test_patched.py
--rw-r--r--   0        0        0     9252 2024-04-28 00:02:48.565096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/test_sql.py
--rw-r--r--   0        0        0     2586 2024-04-28 00:02:48.565096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/test_string_builtins.py
--rw-r--r--   0        0        0    16540 2024-04-28 00:02:48.565096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/test_udf.py
--rw-r--r--   0        0        0     3800 2024-04-28 00:02:48.565096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/test_unary_builtins.py
--rw-r--r--   0        0        0     7357 2024-04-28 00:02:48.565096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/test_value_exprs.py
--rw-r--r--   0        0        0     4311 2024-04-28 00:02:48.565096 ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/test_window.py
--rw-r--r--   0        0        0     7085 2024-04-28 00:02:48.569096 ibis_framework-9.0.0.dev686/ibis/backends/impala/udf.py
--rw-r--r--   0        0        0    19562 2024-04-28 00:02:48.569096 ibis_framework-9.0.0.dev686/ibis/backends/mssql/__init__.py
--rw-r--r--   0        0        0    16722 2024-04-28 00:02:48.569096 ibis_framework-9.0.0.dev686/ibis/backends/mssql/compiler.py
--rw-r--r--   0        0        0        0 2024-04-28 00:02:48.569096 ibis_framework-9.0.0.dev686/ibis/backends/mssql/tests/__init__.py
--rw-r--r--   0        0        0     1632 2024-04-28 00:02:48.569096 ibis_framework-9.0.0.dev686/ibis/backends/mssql/tests/conftest.py
--rw-r--r--   0        0        0     4042 2024-04-28 00:02:48.569096 ibis_framework-9.0.0.dev686/ibis/backends/mssql/tests/test_client.py
--rw-r--r--   0        0        0    17410 2024-04-28 00:02:48.569096 ibis_framework-9.0.0.dev686/ibis/backends/mysql/__init__.py
--rw-r--r--   0        0        0    12812 2024-04-28 00:02:48.569096 ibis_framework-9.0.0.dev686/ibis/backends/mysql/compiler.py
--rw-r--r--   0        0        0      855 2024-04-28 00:02:48.569096 ibis_framework-9.0.0.dev686/ibis/backends/mysql/converter.py
--rw-r--r--   0        0        0        0 2024-04-28 00:02:48.569096 ibis_framework-9.0.0.dev686/ibis/backends/mysql/tests/__init__.py
--rw-r--r--   0        0        0     2434 2024-04-28 00:02:48.569096 ibis_framework-9.0.0.dev686/ibis/backends/mysql/tests/conftest.py
--rw-r--r--   0        0        0     6746 2024-04-28 00:02:48.569096 ibis_framework-9.0.0.dev686/ibis/backends/mysql/tests/test_client.py
--rw-r--r--   0        0        0    21050 2024-04-28 00:02:48.569096 ibis_framework-9.0.0.dev686/ibis/backends/oracle/__init__.py
--rw-r--r--   0        0        0    16627 2024-04-28 00:02:48.569096 ibis_framework-9.0.0.dev686/ibis/backends/oracle/compiler.py
--rw-r--r--   0        0        0      451 2024-04-28 00:02:48.569096 ibis_framework-9.0.0.dev686/ibis/backends/oracle/converter.py
--rw-r--r--   0        0        0        0 2024-04-28 00:02:48.569096 ibis_framework-9.0.0.dev686/ibis/backends/oracle/tests/__init__.py
--rw-r--r--   0        0        0     4929 2024-04-28 00:02:48.569096 ibis_framework-9.0.0.dev686/ibis/backends/oracle/tests/conftest.py
--rw-r--r--   0        0        0     2007 2024-04-28 00:02:48.569096 ibis_framework-9.0.0.dev686/ibis/backends/oracle/tests/test_client.py
--rw-r--r--   0        0        0      674 2024-04-28 00:02:48.569096 ibis_framework-9.0.0.dev686/ibis/backends/oracle/tests/test_datatypes.py
--rw-r--r--   0        0        0    10823 2024-04-28 00:02:48.569096 ibis_framework-9.0.0.dev686/ibis/backends/pandas/__init__.py
--rw-r--r--   0        0        0     3122 2024-04-28 00:02:48.569096 ibis_framework-9.0.0.dev686/ibis/backends/pandas/convert.py
--rw-r--r--   0        0        0    26726 2024-04-28 00:02:48.569096 ibis_framework-9.0.0.dev686/ibis/backends/pandas/executor.py
--rw-r--r--   0        0        0     7176 2024-04-28 00:02:48.569096 ibis_framework-9.0.0.dev686/ibis/backends/pandas/helpers.py
--rw-r--r--   0        0        0    16401 2024-04-28 00:02:48.569096 ibis_framework-9.0.0.dev686/ibis/backends/pandas/kernels.py
--rw-r--r--   0        0        0    10602 2024-04-28 00:02:48.569096 ibis_framework-9.0.0.dev686/ibis/backends/pandas/rewrites.py
--rw-r--r--   0        0        0        0 2024-04-28 00:02:48.569096 ibis_framework-9.0.0.dev686/ibis/backends/pandas/tests/__init__.py
--rw-r--r--   0        0        0     9051 2024-04-28 00:02:48.569096 ibis_framework-9.0.0.dev686/ibis/backends/pandas/tests/conftest.py
--rw-r--r--   0        0        0     6431 2024-04-28 00:02:48.569096 ibis_framework-9.0.0.dev686/ibis/backends/pandas/tests/test_arrays.py
--rw-r--r--   0        0        0     5603 2024-04-28 00:02:48.569096 ibis_framework-9.0.0.dev686/ibis/backends/pandas/tests/test_cast.py
--rw-r--r--   0        0        0     2288 2024-04-28 00:02:48.569096 ibis_framework-9.0.0.dev686/ibis/backends/pandas/tests/test_client.py
--rw-r--r--   0        0        0     1763 2024-04-28 00:02:48.569096 ibis_framework-9.0.0.dev686/ibis/backends/pandas/tests/test_core.py
--rw-r--r--   0        0        0     8751 2024-04-28 00:02:48.569096 ibis_framework-9.0.0.dev686/ibis/backends/pandas/tests/test_functions.py
--rw-r--r--   0        0        0     1943 2024-04-28 00:02:48.569096 ibis_framework-9.0.0.dev686/ibis/backends/pandas/tests/test_helpers.py
--rw-r--r--   0        0        0    21472 2024-04-28 00:02:48.569096 ibis_framework-9.0.0.dev686/ibis/backends/pandas/tests/test_join.py
--rw-r--r--   0        0        0     2813 2024-04-28 00:02:48.569096 ibis_framework-9.0.0.dev686/ibis/backends/pandas/tests/test_maps.py
--rw-r--r--   0        0        0    25980 2024-04-28 00:02:48.569096 ibis_framework-9.0.0.dev686/ibis/backends/pandas/tests/test_operations.py
--rw-r--r--   0        0        0     5619 2024-04-28 00:02:48.569096 ibis_framework-9.0.0.dev686/ibis/backends/pandas/tests/test_strings.py
--rw-r--r--   0        0        0     2517 2024-04-28 00:02:48.569096 ibis_framework-9.0.0.dev686/ibis/backends/pandas/tests/test_structs.py
--rw-r--r--   0        0        0     5977 2024-04-28 00:02:48.569096 ibis_framework-9.0.0.dev686/ibis/backends/pandas/tests/test_temporal.py
--rw-r--r--   0        0        0    12929 2024-04-28 00:02:48.569096 ibis_framework-9.0.0.dev686/ibis/backends/pandas/tests/test_udf.py
--rw-r--r--   0        0        0    20362 2024-04-28 00:02:48.569096 ibis_framework-9.0.0.dev686/ibis/backends/pandas/tests/test_window.py
--rw-r--r--   0        0        0      741 2024-04-28 00:02:48.569096 ibis_framework-9.0.0.dev686/ibis/backends/pandas/udf.py
--rw-r--r--   0        0        0    18177 2024-04-28 00:02:48.569096 ibis_framework-9.0.0.dev686/ibis/backends/polars/__init__.py
--rw-r--r--   0        0        0    37948 2024-04-28 00:02:48.569096 ibis_framework-9.0.0.dev686/ibis/backends/polars/compiler.py
--rw-r--r--   0        0        0        0 2024-04-28 00:02:48.569096 ibis_framework-9.0.0.dev686/ibis/backends/polars/tests/__init__.py
--rw-r--r--   0        0        0     1831 2024-04-28 00:02:48.569096 ibis_framework-9.0.0.dev686/ibis/backends/polars/tests/conftest.py
--rw-r--r--   0        0        0      724 2024-04-28 00:02:48.569096 ibis_framework-9.0.0.dev686/ibis/backends/polars/tests/test_join.py
--rw-r--r--   0        0        0     3347 2024-04-28 00:02:48.569096 ibis_framework-9.0.0.dev686/ibis/backends/polars/tests/test_udf.py
--rw-r--r--   0        0        0    26566 2024-04-28 00:02:48.569096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/__init__.py
--rw-r--r--   0        0        0    20715 2024-04-28 00:02:48.569096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/compiler.py
--rw-r--r--   0        0        0      615 2024-04-28 00:02:48.569096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/converter.py
--rw-r--r--   0        0        0        0 2024-04-28 00:02:48.569096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/__init__.py
--rw-r--r--   0        0        0     2890 2024-04-28 00:02:48.573096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/conftest.py
--rw-r--r--   0        0        0       56 2024-04-28 00:02:48.573096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_client/test_compile_toplevel/out.sql
--rw-r--r--   0        0        0      246 2024-04-28 00:02:48.573096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_client/test_timezone_from_column/out.sql
--rw-r--r--   0        0        0      668 2024-04-28 00:02:48.573096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_functions/test_analytic_functions/out.sql
--rw-r--r--   0        0        0      108 2024-04-28 00:02:48.573096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_functions/test_cast/double_to_int16/out.sql
--rw-r--r--   0        0        0      107 2024-04-28 00:02:48.573096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_functions/test_cast/double_to_int8/out.sql
--rw-r--r--   0        0        0      109 2024-04-28 00:02:48.573096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_functions/test_cast/string_to_decimal_no_params/out.sql
--rw-r--r--   0        0        0      121 2024-04-28 00:02:48.573096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_functions/test_cast/string_to_decimal_params/out.sql
--rw-r--r--   0        0        0      118 2024-04-28 00:02:48.573096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_functions/test_cast/string_to_double/out.sql
--rw-r--r--   0        0        0      106 2024-04-28 00:02:48.573096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_functions/test_cast/string_to_float/out.sql
--rw-r--r--   0        0        0      113 2024-04-28 00:02:48.573096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_functions/test_date_cast/out.sql
--rw-r--r--   0        0        0       64 2024-04-28 00:02:48.573096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_functions/test_timestamp_cast_noop/out1.sql
--rw-r--r--   0        0        0      102 2024-04-28 00:02:48.573096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_functions/test_timestamp_cast_noop/out2.sql
--rw-r--r--   0        0        0      476 2024-04-28 00:02:48.573096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_functions/test_union_cte/False/out.sql
--rw-r--r--   0        0        0      468 2024-04-28 00:02:48.573096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_functions/test_union_cte/True/out.sql
--rw-r--r--   0        0        0      338 2024-04-28 00:02:48.573096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_geospatial/test_geo_equals/out1.sql
--rw-r--r--   0        0        0       97 2024-04-28 00:02:48.573096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_geospatial/test_geo_equals/out2.sql
--rw-r--r--   0        0        0       82 2024-04-28 00:02:48.573096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_geospatial/test_geo_equals/out3.sql
--rw-r--r--   0        0        0      229 2024-04-28 00:02:48.573096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_geospatial/test_geo_literals_smoke/linestring-geography/out.sql
--rw-r--r--   0        0        0      229 2024-04-28 00:02:48.573096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_geospatial/test_geo_literals_smoke/linestring-geometry/out.sql
--rw-r--r--   0        0        0      223 2024-04-28 00:02:48.573096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_geospatial/test_geo_literals_smoke/linestring-none/out.sql
--rw-r--r--   0        0        0      229 2024-04-28 00:02:48.573096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_geospatial/test_geo_literals_smoke/linestring-srid/out.sql
--rw-r--r--   0        0        0      377 2024-04-28 00:02:48.573096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_geospatial/test_geo_literals_smoke/multilinestring-geography/out.sql
--rw-r--r--   0        0        0      377 2024-04-28 00:02:48.573096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_geospatial/test_geo_literals_smoke/multilinestring-geometry/out.sql
--rw-r--r--   0        0        0      371 2024-04-28 00:02:48.573096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_geospatial/test_geo_literals_smoke/multilinestring-none/out.sql
--rw-r--r--   0        0        0      377 2024-04-28 00:02:48.573096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_geospatial/test_geo_literals_smoke/multilinestring-srid/out.sql
--rw-r--r--   0        0        0      257 2024-04-28 00:02:48.573096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_geospatial/test_geo_literals_smoke/multipoint-geography/out.sql
--rw-r--r--   0        0        0      257 2024-04-28 00:02:48.573096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_geospatial/test_geo_literals_smoke/multipoint-geometry/out.sql
--rw-r--r--   0        0        0      251 2024-04-28 00:02:48.573096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_geospatial/test_geo_literals_smoke/multipoint-none/out.sql
--rw-r--r--   0        0        0      257 2024-04-28 00:02:48.573096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_geospatial/test_geo_literals_smoke/multipoint-srid/out.sql
--rw-r--r--   0        0        0      486 2024-04-28 00:02:48.573096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_geospatial/test_geo_literals_smoke/multipolygon-geography/out.sql
--rw-r--r--   0        0        0      486 2024-04-28 00:02:48.573096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_geospatial/test_geo_literals_smoke/multipolygon-geometry/out.sql
--rw-r--r--   0        0        0      474 2024-04-28 00:02:48.573096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_geospatial/test_geo_literals_smoke/multipolygon-none/out.sql
--rw-r--r--   0        0        0      486 2024-04-28 00:02:48.573096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_geospatial/test_geo_literals_smoke/multipolygon-srid/out.sql
--rw-r--r--   0        0        0      153 2024-04-28 00:02:48.573096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_geospatial/test_geo_literals_smoke/point-geography/out.sql
--rw-r--r--   0        0        0      153 2024-04-28 00:02:48.573096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_geospatial/test_geo_literals_smoke/point-geometry/out.sql
--rw-r--r--   0        0        0      147 2024-04-28 00:02:48.573096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_geospatial/test_geo_literals_smoke/point-none/out.sql
--rw-r--r--   0        0        0      153 2024-04-28 00:02:48.573096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_geospatial/test_geo_literals_smoke/point-srid/out.sql
--rw-r--r--   0        0        0      441 2024-04-28 00:02:48.573096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_geospatial/test_geo_literals_smoke/polygon-geography/out.sql
--rw-r--r--   0        0        0      441 2024-04-28 00:02:48.573096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_geospatial/test_geo_literals_smoke/polygon-geometry/out.sql
--rw-r--r--   0        0        0      429 2024-04-28 00:02:48.573096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_geospatial/test_geo_literals_smoke/polygon-none/out.sql
--rw-r--r--   0        0        0      441 2024-04-28 00:02:48.573096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_geospatial/test_geo_literals_smoke/polygon-srid/out.sql
--rw-r--r--   0        0        0      281 2024-04-28 00:02:48.573096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_geospatial/test_geo_literals_smoke/polygon_single-geography/out.sql
--rw-r--r--   0        0        0      281 2024-04-28 00:02:48.573096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_geospatial/test_geo_literals_smoke/polygon_single-geometry/out.sql
--rw-r--r--   0        0        0      275 2024-04-28 00:02:48.573096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_geospatial/test_geo_literals_smoke/polygon_single-none/out.sql
--rw-r--r--   0        0        0      281 2024-04-28 00:02:48.573096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_geospatial/test_geo_literals_smoke/polygon_single-srid/out.sql
--rw-r--r--   0        0        0      124 2024-04-28 00:02:48.573096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_geospatial/test_geo_ops_smoke/linestring_contains/out.sql
--rw-r--r--   0        0        0      106 2024-04-28 00:02:48.573096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_geospatial/test_geo_ops_smoke/linestring_end_point/out.sql
--rw-r--r--   0        0        0       91 2024-04-28 00:02:48.573096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_geospatial/test_geo_ops_smoke/linestring_length/out.sql
--rw-r--r--   0        0        0      110 2024-04-28 00:02:48.573096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_geospatial/test_geo_ops_smoke/linestring_start_point/out.sql
--rw-r--r--   0        0        0       97 2024-04-28 00:02:48.573096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_geospatial/test_geo_ops_smoke/multipolygon_n_points/out.sql
--rw-r--r--   0        0        0      106 2024-04-28 00:02:48.573096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_geospatial/test_geo_ops_smoke/point_set_srid/out.sql
--rw-r--r--   0        0        0       77 2024-04-28 00:02:48.573096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_geospatial/test_geo_ops_smoke/point_srid/out.sql
--rw-r--r--   0        0        0       71 2024-04-28 00:02:48.573096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_geospatial/test_geo_ops_smoke/point_x/out.sql
--rw-r--r--   0        0        0       71 2024-04-28 00:02:48.573096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_geospatial/test_geo_ops_smoke/point_y/out.sql
--rw-r--r--   0        0        0       81 2024-04-28 00:02:48.573096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_geospatial/test_geo_ops_smoke/polygon_area/out.sql
--rw-r--r--   0        0        0       91 2024-04-28 00:02:48.573096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_geospatial/test_geo_ops_smoke/polygon_perimeter/out.sql
--rw-r--r--   0        0        0      103 2024-04-28 00:02:48.573096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_geospatial/test_literal_geospatial_explicit/expr0/out.sql
--rw-r--r--   0        0        0      109 2024-04-28 00:02:48.573096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_geospatial/test_literal_geospatial_explicit/expr1/out.sql
--rw-r--r--   0        0        0      109 2024-04-28 00:02:48.573096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_geospatial/test_literal_geospatial_explicit/expr2/out.sql
--rw-r--r--   0        0        0      109 2024-04-28 00:02:48.577096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_geospatial/test_literal_geospatial_explicit/expr3/out.sql
--rw-r--r--   0        0        0      109 2024-04-28 00:02:48.577096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_geospatial/test_literal_geospatial_explicit/expr4/out.sql
--rw-r--r--   0        0        0      109 2024-04-28 00:02:48.577096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_geospatial/test_literal_geospatial_explicit/expr5/out.sql
--rw-r--r--   0        0        0      109 2024-04-28 00:02:48.577096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_geospatial/test_literal_geospatial_explicit/expr6/out.sql
--rw-r--r--   0        0        0      109 2024-04-28 00:02:48.577096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_geospatial/test_literal_geospatial_explicit/expr7/out.sql
--rw-r--r--   0        0        0      139 2024-04-28 00:02:48.577096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_geospatial/test_literal_geospatial_inferred/shp0/out.sql
--rw-r--r--   0        0        0      139 2024-04-28 00:02:48.577096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_geospatial/test_literal_geospatial_inferred/shp1/out.sql
--rw-r--r--   0        0        0      139 2024-04-28 00:02:48.577096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_geospatial/test_literal_geospatial_inferred/shp2/out.sql
--rw-r--r--   0        0        0      199 2024-04-28 00:02:48.577096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_geospatial/test_literal_geospatial_inferred/shp3/out.sql
--rw-r--r--   0        0        0      199 2024-04-28 00:02:48.577096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_geospatial/test_literal_geospatial_inferred/shp4/out.sql
--rw-r--r--   0        0        0      215 2024-04-28 00:02:48.577096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_geospatial/test_literal_geospatial_inferred/shp5/out.sql
--rw-r--r--   0        0        0      243 2024-04-28 00:02:48.577096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_geospatial/test_literal_geospatial_inferred/shp6/out.sql
--rw-r--r--   0        0        0      295 2024-04-28 00:02:48.577096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_geospatial/test_literal_geospatial_inferred/shp7/out.sql
--rw-r--r--   0        0        0      199 2024-04-28 00:02:48.577096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_geospatial/test_literal_geospatial_inferred/shp8/out.sql
--rw-r--r--   0        0        0       80 2024-04-28 00:02:48.577096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_postgis/test_select_linestring_geodata/out.sql
--rw-r--r--   0        0        0       84 2024-04-28 00:02:48.577096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_postgis/test_select_multipolygon_geodata/out.sql
--rw-r--r--   0        0        0       70 2024-04-28 00:02:48.577096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_postgis/test_select_point_geodata/out.sql
--rw-r--r--   0        0        0       74 2024-04-28 00:02:48.577096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/snapshots/test_postgis/test_select_polygon_geodata/out.sql
--rw-r--r--   0        0        0    11978 2024-04-28 00:02:48.577096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/test_client.py
--rw-r--r--   0        0        0    38504 2024-04-28 00:02:48.577096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/test_functions.py
--rw-r--r--   0        0        0    16855 2024-04-28 00:02:48.577096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/test_geospatial.py
--rw-r--r--   0        0        0      416 2024-04-28 00:02:48.577096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/test_json.py
--rw-r--r--   0        0        0     8981 2024-04-28 00:02:48.577096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/test_postgis.py
--rw-r--r--   0        0        0      488 2024-04-28 00:02:48.577096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/test_string.py
--rw-r--r--   0        0        0     5226 2024-04-28 00:02:48.577096 ibis_framework-9.0.0.dev686/ibis/backends/postgres/tests/test_udf.py
--rw-r--r--   0        0        0    28018 2024-04-28 00:02:48.577096 ibis_framework-9.0.0.dev686/ibis/backends/pyspark/__init__.py
--rw-r--r--   0        0        0    16853 2024-04-28 00:02:48.577096 ibis_framework-9.0.0.dev686/ibis/backends/pyspark/compiler.py
--rw-r--r--   0        0        0     1263 2024-04-28 00:02:48.577096 ibis_framework-9.0.0.dev686/ibis/backends/pyspark/converter.py
--rw-r--r--   0        0        0     4491 2024-04-28 00:02:48.577096 ibis_framework-9.0.0.dev686/ibis/backends/pyspark/datatypes.py
--rw-r--r--   0        0        0        0 2024-04-28 00:02:48.577096 ibis_framework-9.0.0.dev686/ibis/backends/pyspark/tests/__init__.py
--rw-r--r--   0        0        0    11808 2024-04-28 00:02:48.577096 ibis_framework-9.0.0.dev686/ibis/backends/pyspark/tests/conftest.py
--rw-r--r--   0        0        0      665 2024-04-28 00:02:48.577096 ibis_framework-9.0.0.dev686/ibis/backends/pyspark/tests/test_aggregation.py
--rw-r--r--   0        0        0     4658 2024-04-28 00:02:48.577096 ibis_framework-9.0.0.dev686/ibis/backends/pyspark/tests/test_array.py
--rw-r--r--   0        0        0     3805 2024-04-28 00:02:48.577096 ibis_framework-9.0.0.dev686/ibis/backends/pyspark/tests/test_basic.py
--rw-r--r--   0        0        0      744 2024-04-28 00:02:48.577096 ibis_framework-9.0.0.dev686/ibis/backends/pyspark/tests/test_client.py
--rw-r--r--   0        0        0     4895 2024-04-28 00:02:48.577096 ibis_framework-9.0.0.dev686/ibis/backends/pyspark/tests/test_ddl.py
--rw-r--r--   0        0        0      801 2024-04-28 00:02:48.577096 ibis_framework-9.0.0.dev686/ibis/backends/pyspark/tests/test_null.py
--rw-r--r--   0        0        0     2340 2024-04-28 00:02:48.577096 ibis_framework-9.0.0.dev686/ibis/backends/pyspark/tests/test_window.py
--rw-r--r--   0        0        0    20011 2024-04-28 00:02:48.577096 ibis_framework-9.0.0.dev686/ibis/backends/risingwave/__init__.py
--rw-r--r--   0        0        0     2866 2024-04-28 00:02:48.577096 ibis_framework-9.0.0.dev686/ibis/backends/risingwave/compiler.py
--rw-r--r--   0        0        0        0 2024-04-28 00:02:48.577096 ibis_framework-9.0.0.dev686/ibis/backends/risingwave/tests/__init__.py
--rw-r--r--   0        0        0     2386 2024-04-28 00:02:48.577096 ibis_framework-9.0.0.dev686/ibis/backends/risingwave/tests/conftest.py
--rw-r--r--   0        0        0       56 2024-04-28 00:02:48.577096 ibis_framework-9.0.0.dev686/ibis/backends/risingwave/tests/snapshots/test_client/test_compile_toplevel/out.sql
--rw-r--r--   0        0        0      349 2024-04-28 00:02:48.577096 ibis_framework-9.0.0.dev686/ibis/backends/risingwave/tests/snapshots/test_functions/test_analytic_functions/out.sql
--rw-r--r--   0        0        0      476 2024-04-28 00:02:48.577096 ibis_framework-9.0.0.dev686/ibis/backends/risingwave/tests/snapshots/test_functions/test_union_cte/False/out.sql
--rw-r--r--   0        0        0      468 2024-04-28 00:02:48.577096 ibis_framework-9.0.0.dev686/ibis/backends/risingwave/tests/snapshots/test_functions/test_union_cte/True/out.sql
--rw-r--r--   0        0        0     3958 2024-04-28 00:02:48.577096 ibis_framework-9.0.0.dev686/ibis/backends/risingwave/tests/test_client.py
--rw-r--r--   0        0        0    25762 2024-04-28 00:02:48.577096 ibis_framework-9.0.0.dev686/ibis/backends/risingwave/tests/test_functions.py
--rw-r--r--   0        0        0      416 2024-04-28 00:02:48.577096 ibis_framework-9.0.0.dev686/ibis/backends/risingwave/tests/test_json.py
--rw-r--r--   0        0        0     3431 2024-04-28 00:02:48.577096 ibis_framework-9.0.0.dev686/ibis/backends/risingwave/tests/test_streaming.py
--rw-r--r--   0        0        0    40562 2024-04-28 00:02:48.581096 ibis_framework-9.0.0.dev686/ibis/backends/snowflake/__init__.py
--rw-r--r--   0        0        0    22505 2024-04-28 00:02:48.581096 ibis_framework-9.0.0.dev686/ibis/backends/snowflake/compiler.py
--rw-r--r--   0        0        0     2914 2024-04-28 00:02:48.581096 ibis_framework-9.0.0.dev686/ibis/backends/snowflake/converter.py
--rw-r--r--   0        0        0        0 2024-04-28 00:02:48.581096 ibis_framework-9.0.0.dev686/ibis/backends/snowflake/tests/__init__.py
--rw-r--r--   0        0        0     7154 2024-04-28 00:02:48.581096 ibis_framework-9.0.0.dev686/ibis/backends/snowflake/tests/conftest.py
--rw-r--r--   0        0        0      200 2024-04-28 00:02:48.581096 ibis_framework-9.0.0.dev686/ibis/backends/snowflake/tests/snapshots/test_compiler/test_more_than_one_quantile/two_quantiles.sql
--rw-r--r--   0        0        0     9617 2024-04-28 00:02:48.581096 ibis_framework-9.0.0.dev686/ibis/backends/snowflake/tests/test_client.py
--rw-r--r--   0        0        0      413 2024-04-28 00:02:48.581096 ibis_framework-9.0.0.dev686/ibis/backends/snowflake/tests/test_compiler.py
--rw-r--r--   0        0        0     4070 2024-04-28 00:02:48.581096 ibis_framework-9.0.0.dev686/ibis/backends/snowflake/tests/test_datatypes.py
--rw-r--r--   0        0        0     8161 2024-04-28 00:02:48.581096 ibis_framework-9.0.0.dev686/ibis/backends/snowflake/tests/test_udf.py
--rw-r--r--   0        0        0    18353 2024-04-28 00:02:48.581096 ibis_framework-9.0.0.dev686/ibis/backends/sql/__init__.py
--rw-r--r--   0        0        0    48749 2024-04-28 00:02:48.581096 ibis_framework-9.0.0.dev686/ibis/backends/sql/compiler.py
--rw-r--r--   0        0        0    37074 2024-04-28 00:02:48.581096 ibis_framework-9.0.0.dev686/ibis/backends/sql/datatypes.py
--rw-r--r--   0        0        0     3520 2024-04-28 00:02:48.581096 ibis_framework-9.0.0.dev686/ibis/backends/sql/ddl.py
--rw-r--r--   0        0        0    15417 2024-04-28 00:02:48.581096 ibis_framework-9.0.0.dev686/ibis/backends/sql/dialects.py
--rw-r--r--   0        0        0    10377 2024-04-28 00:02:48.581096 ibis_framework-9.0.0.dev686/ibis/backends/sql/rewrites.py
--rw-r--r--   0        0        0        0 2024-04-28 00:02:48.581096 ibis_framework-9.0.0.dev686/ibis/backends/sql/tests/__init__.py
--rw-r--r--   0        0        0      550 2024-04-28 00:02:48.581096 ibis_framework-9.0.0.dev686/ibis/backends/sql/tests/test_compiler.py
--rw-r--r--   0        0        0     2565 2024-04-28 00:02:48.581096 ibis_framework-9.0.0.dev686/ibis/backends/sql/tests/test_datatypes.py
--rw-r--r--   0        0        0    18588 2024-04-28 00:02:48.581096 ibis_framework-9.0.0.dev686/ibis/backends/sqlite/__init__.py
--rw-r--r--   0        0        0    16633 2024-04-28 00:02:48.581096 ibis_framework-9.0.0.dev686/ibis/backends/sqlite/compiler.py
--rw-r--r--   0        0        0      751 2024-04-28 00:02:48.581096 ibis_framework-9.0.0.dev686/ibis/backends/sqlite/converter.py
--rw-r--r--   0        0        0        0 2024-04-28 00:02:48.581096 ibis_framework-9.0.0.dev686/ibis/backends/sqlite/tests/__init__.py
--rw-r--r--   0        0        0     2514 2024-04-28 00:02:48.581096 ibis_framework-9.0.0.dev686/ibis/backends/sqlite/tests/conftest.py
--rw-r--r--   0        0        0     2515 2024-04-28 00:02:48.581096 ibis_framework-9.0.0.dev686/ibis/backends/sqlite/tests/test_client.py
--rw-r--r--   0        0        0     3053 2024-04-28 00:02:48.581096 ibis_framework-9.0.0.dev686/ibis/backends/sqlite/tests/test_types.py
--rw-r--r--   0        0        0     9971 2024-04-28 00:02:48.581096 ibis_framework-9.0.0.dev686/ibis/backends/sqlite/udf.py
--rw-r--r--   0        0        0        0 2024-04-28 00:02:48.581096 ibis_framework-9.0.0.dev686/ibis/backends/tests/__init__.py
--rw-r--r--   0        0        0    12286 2024-04-28 00:02:48.581096 ibis_framework-9.0.0.dev686/ibis/backends/tests/base.py
--rw-r--r--   0        0        0     2862 2024-04-28 00:02:48.581096 ibis_framework-9.0.0.dev686/ibis/backends/tests/data.py
--rw-r--r--   0        0        0     5168 2024-04-28 00:02:48.581096 ibis_framework-9.0.0.dev686/ibis/backends/tests/errors.py
--rw-r--r--   0        0        0      600 2024-04-28 00:02:48.581096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_generic/test_many_subqueries/bigquery/out.sql
--rw-r--r--   0        0        0      714 2024-04-28 00:02:48.581096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_generic/test_many_subqueries/clickhouse/out.sql
--rw-r--r--   0        0        0      714 2024-04-28 00:02:48.581096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_generic/test_many_subqueries/datafusion/out.sql
--rw-r--r--   0        0        0      714 2024-04-28 00:02:48.581096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_generic/test_many_subqueries/druid/out.sql
--rw-r--r--   0        0        0      748 2024-04-28 00:02:48.581096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_generic/test_many_subqueries/duckdb/out.sql
--rw-r--r--   0        0        0      600 2024-04-28 00:02:48.581096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_generic/test_many_subqueries/exasol/out.sql
--rw-r--r--   0        0        0      622 2024-04-28 00:02:48.581096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_generic/test_many_subqueries/flink/out.sql
--rw-r--r--   0        0        0      622 2024-04-28 00:02:48.581096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_generic/test_many_subqueries/impala/out.sql
--rw-r--r--   0        0        0      702 2024-04-28 00:02:48.581096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_generic/test_many_subqueries/mssql/out.sql
--rw-r--r--   0        0        0      702 2024-04-28 00:02:48.581096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_generic/test_many_subqueries/mysql/out.sql
--rw-r--r--   0        0        0      598 2024-04-28 00:02:48.581096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_generic/test_many_subqueries/oracle/out.sql
--rw-r--r--   0        0        0      714 2024-04-28 00:02:48.581096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_generic/test_many_subqueries/postgres/out.sql
--rw-r--r--   0        0        0      622 2024-04-28 00:02:48.585096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_generic/test_many_subqueries/pyspark/out.sql
--rw-r--r--   0        0        0      714 2024-04-28 00:02:48.585096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_generic/test_many_subqueries/risingwave/out.sql
--rw-r--r--   0        0        0      600 2024-04-28 00:02:48.585096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_generic/test_many_subqueries/snowflake/out.sql
--rw-r--r--   0        0        0      736 2024-04-28 00:02:48.585096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_generic/test_many_subqueries/sqlite/out.sql
--rw-r--r--   0        0        0      714 2024-04-28 00:02:48.585096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_generic/test_many_subqueries/trino/out.sql
--rw-r--r--   0        0        0      324 2024-04-28 00:02:48.585096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_join/test_complex_join_agg/out.sql
--rw-r--r--   0        0        0      323 2024-04-28 00:02:48.585096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_cte_refs_in_topo_order/bigquery/out.sql
--rw-r--r--   0        0        0      323 2024-04-28 00:02:48.585096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_cte_refs_in_topo_order/clickhouse/out.sql
--rw-r--r--   0        0        0      323 2024-04-28 00:02:48.585096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_cte_refs_in_topo_order/datafusion/out.sql
--rw-r--r--   0        0        0      323 2024-04-28 00:02:48.585096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_cte_refs_in_topo_order/druid/out.sql
--rw-r--r--   0        0        0      323 2024-04-28 00:02:48.585096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_cte_refs_in_topo_order/duckdb/out.sql
--rw-r--r--   0        0        0      323 2024-04-28 00:02:48.585096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_cte_refs_in_topo_order/exasol/out.sql
--rw-r--r--   0        0        0      323 2024-04-28 00:02:48.585096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_cte_refs_in_topo_order/flink/out.sql
--rw-r--r--   0        0        0      323 2024-04-28 00:02:48.585096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_cte_refs_in_topo_order/impala/out.sql
--rw-r--r--   0        0        0      326 2024-04-28 00:02:48.585096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_cte_refs_in_topo_order/mssql/out.sql
--rw-r--r--   0        0        0      323 2024-04-28 00:02:48.585096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_cte_refs_in_topo_order/mysql/out.sql
--rw-r--r--   0        0        0      305 2024-04-28 00:02:48.585096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_cte_refs_in_topo_order/oracle/out.sql
--rw-r--r--   0        0        0      323 2024-04-28 00:02:48.585096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_cte_refs_in_topo_order/postgres/out.sql
--rw-r--r--   0        0        0      323 2024-04-28 00:02:48.585096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_cte_refs_in_topo_order/pyspark/out.sql
--rw-r--r--   0        0        0      323 2024-04-28 00:02:48.585096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_cte_refs_in_topo_order/risingwave/out.sql
--rw-r--r--   0        0        0      323 2024-04-28 00:02:48.585096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_cte_refs_in_topo_order/snowflake/out.sql
--rw-r--r--   0        0        0      323 2024-04-28 00:02:48.585096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_cte_refs_in_topo_order/sqlite/out.sql
--rw-r--r--   0        0        0      323 2024-04-28 00:02:48.585096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_cte_refs_in_topo_order/trino/out.sql
--rw-r--r--   0        0        0      395 2024-04-28 00:02:48.585096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_group_by_has_index/bigquery/out.sql
--rw-r--r--   0        0        0      691 2024-04-28 00:02:48.585096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_group_by_has_index/clickhouse/out.sql
--rw-r--r--   0        0        0      514 2024-04-28 00:02:48.585096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_group_by_has_index/datafusion/out.sql
--rw-r--r--   0        0        0      395 2024-04-28 00:02:48.585096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_group_by_has_index/druid/out.sql
--rw-r--r--   0        0        0      395 2024-04-28 00:02:48.585096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_group_by_has_index/duckdb/out.sql
--rw-r--r--   0        0        0      395 2024-04-28 00:02:48.585096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_group_by_has_index/exasol/out.sql
--rw-r--r--   0        0        0      691 2024-04-28 00:02:48.585096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_group_by_has_index/flink/out.sql
--rw-r--r--   0        0        0      395 2024-04-28 00:02:48.585096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_group_by_has_index/impala/out.sql
--rw-r--r--   0        0        0      691 2024-04-28 00:02:48.585096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_group_by_has_index/mssql/out.sql
--rw-r--r--   0        0        0      395 2024-04-28 00:02:48.585096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_group_by_has_index/mysql/out.sql
--rw-r--r--   0        0        0      688 2024-04-28 00:02:48.585096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_group_by_has_index/oracle/out.sql
--rw-r--r--   0        0        0      395 2024-04-28 00:02:48.585096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_group_by_has_index/postgres/out.sql
--rw-r--r--   0        0        0      395 2024-04-28 00:02:48.585096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_group_by_has_index/pyspark/out.sql
--rw-r--r--   0        0        0      395 2024-04-28 00:02:48.585096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_group_by_has_index/risingwave/out.sql
--rw-r--r--   0        0        0      395 2024-04-28 00:02:48.585096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_group_by_has_index/snowflake/out.sql
--rw-r--r--   0        0        0      395 2024-04-28 00:02:48.585096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_group_by_has_index/sqlite/out.sql
--rw-r--r--   0        0        0      395 2024-04-28 00:02:48.585096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_group_by_has_index/trino/out.sql
--rw-r--r--   0        0        0      137 2024-04-28 00:02:48.585096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_isin_bug/bigquery/out.sql
--rw-r--r--   0        0        0      138 2024-04-28 00:02:48.585096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_isin_bug/clickhouse/out.sql
--rw-r--r--   0        0        0      138 2024-04-28 00:02:48.585096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_isin_bug/datafusion/out.sql
--rw-r--r--   0        0        0      138 2024-04-28 00:02:48.585096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_isin_bug/druid/out.sql
--rw-r--r--   0        0        0      155 2024-04-28 00:02:48.585096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_isin_bug/duckdb/out.sql
--rw-r--r--   0        0        0      138 2024-04-28 00:02:48.585096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_isin_bug/exasol/out.sql
--rw-r--r--   0        0        0      138 2024-04-28 00:02:48.585096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_isin_bug/flink/out.sql
--rw-r--r--   0        0        0      138 2024-04-28 00:02:48.585096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_isin_bug/impala/out.sql
--rw-r--r--   0        0        0      238 2024-04-28 00:02:48.585096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_isin_bug/mssql/out.sql
--rw-r--r--   0        0        0      138 2024-04-28 00:02:48.585096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_isin_bug/mysql/out.sql
--rw-r--r--   0        0        0      132 2024-04-28 00:02:48.585096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_isin_bug/oracle/out.sql
--rw-r--r--   0        0        0      138 2024-04-28 00:02:48.585096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_isin_bug/postgres/out.sql
--rw-r--r--   0        0        0      138 2024-04-28 00:02:48.585096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_isin_bug/pyspark/out.sql
--rw-r--r--   0        0        0      138 2024-04-28 00:02:48.585096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_isin_bug/risingwave/out.sql
--rw-r--r--   0        0        0      138 2024-04-28 00:02:48.585096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_isin_bug/snowflake/out.sql
--rw-r--r--   0        0        0      138 2024-04-28 00:02:48.585096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_isin_bug/sqlite/out.sql
--rw-r--r--   0        0        0      138 2024-04-28 00:02:48.585096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_isin_bug/trino/out.sql
--rw-r--r--   0        0        0      190 2024-04-28 00:02:48.585096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_selects_with_impure_operations_not_merged/bigquery-random/out.sql
--rw-r--r--   0        0        0      208 2024-04-28 00:02:48.585096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_selects_with_impure_operations_not_merged/bigquery-uuid/out.sql
--rw-r--r--   0        0        0      226 2024-04-28 00:02:48.589096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_selects_with_impure_operations_not_merged/clickhouse-random/out.sql
--rw-r--r--   0        0        0      228 2024-04-28 00:02:48.589096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_selects_with_impure_operations_not_merged/clickhouse-uuid/out.sql
--rw-r--r--   0        0        0      212 2024-04-28 00:02:48.589096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_selects_with_impure_operations_not_merged/datafusion-random/out.sql
--rw-r--r--   0        0        0      208 2024-04-28 00:02:48.589096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_selects_with_impure_operations_not_merged/datafusion-uuid/out.sql
--rw-r--r--   0        0        0      212 2024-04-28 00:02:48.589096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_selects_with_impure_operations_not_merged/druid-random/out.sql
--rw-r--r--   0        0        0      212 2024-04-28 00:02:48.589096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_selects_with_impure_operations_not_merged/duckdb-random/out.sql
--rw-r--r--   0        0        0      208 2024-04-28 00:02:48.589096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_selects_with_impure_operations_not_merged/duckdb-uuid/out.sql
--rw-r--r--   0        0        0      212 2024-04-28 00:02:48.589096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_selects_with_impure_operations_not_merged/exasol-random/out.sql
--rw-r--r--   0        0        0      190 2024-04-28 00:02:48.589096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_selects_with_impure_operations_not_merged/flink-random/out.sql
--rw-r--r--   0        0        0      190 2024-04-28 00:02:48.589096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_selects_with_impure_operations_not_merged/flink-uuid/out.sql
--rw-r--r--   0        0        0      260 2024-04-28 00:02:48.589096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_selects_with_impure_operations_not_merged/impala-random/out.sql
--rw-r--r--   0        0        0      190 2024-04-28 00:02:48.589096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_selects_with_impure_operations_not_merged/impala-uuid/out.sql
--rw-r--r--   0        0        0      191 2024-04-28 00:02:48.589096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_selects_with_impure_operations_not_merged/mssql-random/out.sql
--rw-r--r--   0        0        0      193 2024-04-28 00:02:48.589096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_selects_with_impure_operations_not_merged/mssql-uuid/out.sql
--rw-r--r--   0        0        0      208 2024-04-28 00:02:48.589096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_selects_with_impure_operations_not_merged/mysql-random/out.sql
--rw-r--r--   0        0        0      208 2024-04-28 00:02:48.589096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_selects_with_impure_operations_not_merged/mysql-uuid/out.sql
--rw-r--r--   0        0        0      228 2024-04-28 00:02:48.589096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_selects_with_impure_operations_not_merged/oracle-random/out.sql
--rw-r--r--   0        0        0      212 2024-04-28 00:02:48.589096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_selects_with_impure_operations_not_merged/postgres-random/out.sql
--rw-r--r--   0        0        0      230 2024-04-28 00:02:48.589096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_selects_with_impure_operations_not_merged/postgres-uuid/out.sql
--rw-r--r--   0        0        0      190 2024-04-28 00:02:48.589096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_selects_with_impure_operations_not_merged/pyspark-random/out.sql
--rw-r--r--   0        0        0      190 2024-04-28 00:02:48.589096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_selects_with_impure_operations_not_merged/pyspark-uuid/out.sql
--rw-r--r--   0        0        0      212 2024-04-28 00:02:48.589096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_selects_with_impure_operations_not_merged/risingwave-random/out.sql
--rw-r--r--   0        0        0      277 2024-04-28 00:02:48.589096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_selects_with_impure_operations_not_merged/snowflake-random/out.sql
--rw-r--r--   0        0        0      205 2024-04-28 00:02:48.589096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_selects_with_impure_operations_not_merged/snowflake-uuid/out.sql
--rw-r--r--   0        0        0      315 2024-04-28 00:02:48.589096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_selects_with_impure_operations_not_merged/sqlite-random/out.sql
--rw-r--r--   0        0        0      191 2024-04-28 00:02:48.589096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_selects_with_impure_operations_not_merged/sqlite-uuid/out.sql
--rw-r--r--   0        0        0      190 2024-04-28 00:02:48.589096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_selects_with_impure_operations_not_merged/trino-random/out.sql
--rw-r--r--   0        0        0      190 2024-04-28 00:02:48.589096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_selects_with_impure_operations_not_merged/trino-uuid/out.sql
--rw-r--r--   0        0        0     7749 2024-04-28 00:02:48.589096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_union_aliasing/bigquery/out.sql
--rw-r--r--   0        0        0     3811 2024-04-28 00:02:48.589096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_union_aliasing/clickhouse/out.sql
--rw-r--r--   0        0        0     2820 2024-04-28 00:02:48.589096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_union_aliasing/duckdb/out.sql
--rw-r--r--   0        0        0     2886 2024-04-28 00:02:48.589096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_union_aliasing/postgres/out.sql
--rw-r--r--   0        0        0     3045 2024-04-28 00:02:48.589096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_union_aliasing/pyspark/out.sql
--rw-r--r--   0        0        0     7878 2024-04-28 00:02:48.589096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_sql/test_union_aliasing/trino/out.sql
--rw-r--r--   0        0        0       55 2024-04-28 00:02:48.589096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_temporal/test_temporal_literal_sql/bigquery-date/out.sql
--rw-r--r--   0        0        0       98 2024-04-28 00:02:48.589096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_temporal/test_temporal_literal_sql/bigquery-timestamp/out.sql
--rw-r--r--   0        0        0       60 2024-04-28 00:02:48.589096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_temporal/test_temporal_literal_sql/clickhouse-date/out.sql
--rw-r--r--   0        0        0      119 2024-04-28 00:02:48.589096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_temporal/test_temporal_literal_sql/clickhouse-timestamp/out.sql
--rw-r--r--   0        0        0       71 2024-04-28 00:02:48.589096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_temporal/test_temporal_literal_sql/datafusion-date/out.sql
--rw-r--r--   0        0        0      133 2024-04-28 00:02:48.589096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_temporal/test_temporal_literal_sql/datafusion-timestamp/out.sql
--rw-r--r--   0        0        0       67 2024-04-28 00:02:48.589096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_temporal/test_temporal_literal_sql/druid-date/out.sql
--rw-r--r--   0        0        0      108 2024-04-28 00:02:48.589096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_temporal/test_temporal_literal_sql/druid-timestamp/out.sql
--rw-r--r--   0        0        0       61 2024-04-28 00:02:48.589096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_temporal/test_temporal_literal_sql/duckdb-date/out.sql
--rw-r--r--   0        0        0      103 2024-04-28 00:02:48.589096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_temporal/test_temporal_literal_sql/duckdb-timestamp/out.sql
--rw-r--r--   0        0        0       66 2024-04-28 00:02:48.589096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_temporal/test_temporal_literal_sql/exasol-date/out.sql
--rw-r--r--   0        0        0      105 2024-04-28 00:02:48.589096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_temporal/test_temporal_literal_sql/exasol-timestamp/out.sql
--rw-r--r--   0        0        0       52 2024-04-28 00:02:48.589096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_temporal/test_temporal_literal_sql/impala-date/out.sql
--rw-r--r--   0        0        0       89 2024-04-28 00:02:48.589096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_temporal/test_temporal_literal_sql/impala-timestamp/out.sql
--rw-r--r--   0        0        0       65 2024-04-28 00:02:48.589096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_temporal/test_temporal_literal_sql/mssql-date/out.sql
--rw-r--r--   0        0        0      111 2024-04-28 00:02:48.589096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_temporal/test_temporal_literal_sql/mssql-timestamp/out.sql
--rw-r--r--   0        0        0       58 2024-04-28 00:02:48.589096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_temporal/test_temporal_literal_sql/mysql-date/out.sql
--rw-r--r--   0        0        0      100 2024-04-28 00:02:48.589096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_temporal/test_temporal_literal_sql/mysql-timestamp/out.sql
--rw-r--r--   0        0        0       77 2024-04-28 00:02:48.589096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_temporal/test_temporal_literal_sql/oracle-date/out.sql
--rw-r--r--   0        0        0      134 2024-04-28 00:02:48.589096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_temporal/test_temporal_literal_sql/oracle-timestamp/out.sql
--rw-r--r--   0        0        0       61 2024-04-28 00:02:48.589096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_temporal/test_temporal_literal_sql/postgres-date/out.sql
--rw-r--r--   0        0        0      108 2024-04-28 00:02:48.589096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_temporal/test_temporal_literal_sql/postgres-timestamp/out.sql
--rw-r--r--   0        0        0       61 2024-04-28 00:02:48.589096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_temporal/test_temporal_literal_sql/pyspark-date/out.sql
--rw-r--r--   0        0        0      108 2024-04-28 00:02:48.589096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_temporal/test_temporal_literal_sql/pyspark-timestamp/out.sql
--rw-r--r--   0        0        0       66 2024-04-28 00:02:48.589096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_temporal/test_temporal_literal_sql/risingwave-date/out.sql
--rw-r--r--   0        0        0      108 2024-04-28 00:02:48.593096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_temporal/test_temporal_literal_sql/risingwave-timestamp/out.sql
--rw-r--r--   0        0        0       67 2024-04-28 00:02:48.593096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_temporal/test_temporal_literal_sql/snowflake-date/out.sql
--rw-r--r--   0        0        0      113 2024-04-28 00:02:48.593096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_temporal/test_temporal_literal_sql/snowflake-timestamp/out.sql
--rw-r--r--   0        0        0       52 2024-04-28 00:02:48.593096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_temporal/test_temporal_literal_sql/sqlite-date/out.sql
--rw-r--r--   0        0        0       89 2024-04-28 00:02:48.593096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_temporal/test_temporal_literal_sql/sqlite-timestamp/out.sql
--rw-r--r--   0        0        0       71 2024-04-28 00:02:48.593096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_temporal/test_temporal_literal_sql/trino-date/out.sql
--rw-r--r--   0        0        0      132 2024-04-28 00:02:48.593096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_temporal/test_temporal_literal_sql/trino-timestamp/out.sql
--rw-r--r--   0        0        0       49 2024-04-28 00:02:48.593096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_temporal/test_time_literal_sql/0-bigquery/out.sql
--rw-r--r--   0        0        0       71 2024-04-28 00:02:48.593096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_temporal/test_time_literal_sql/0-clickhouse/out.sql
--rw-r--r--   0        0        0       61 2024-04-28 00:02:48.593096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_temporal/test_time_literal_sql/0-datafusion/out.sql
--rw-r--r--   0        0        0       61 2024-04-28 00:02:48.593096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_temporal/test_time_literal_sql/0-druid/out.sql
--rw-r--r--   0        0        0       57 2024-04-28 00:02:48.593096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_temporal/test_time_literal_sql/0-duckdb/out.sql
--rw-r--r--   0        0        0       61 2024-04-28 00:02:48.593096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_temporal/test_time_literal_sql/0-exasol/out.sql
--rw-r--r--   0        0        0       66 2024-04-28 00:02:48.593096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_temporal/test_time_literal_sql/0-impala/out.sql
--rw-r--r--   0        0        0       65 2024-04-28 00:02:48.593096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_temporal/test_time_literal_sql/0-mssql/out.sql
--rw-r--r--   0        0        0       56 2024-04-28 00:02:48.593096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_temporal/test_time_literal_sql/0-mysql/out.sql
--rw-r--r--   0        0        0       92 2024-04-28 00:02:48.593096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_temporal/test_time_literal_sql/0-oracle/out.sql
--rw-r--r--   0        0        0      109 2024-04-28 00:02:48.593096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_temporal/test_time_literal_sql/0-postgres/out.sql
--rw-r--r--   0        0        0       61 2024-04-28 00:02:48.593096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_temporal/test_time_literal_sql/0-risingwave/out.sql
--rw-r--r--   0        0        0       64 2024-04-28 00:02:48.593096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_temporal/test_time_literal_sql/0-snowflake/out.sql
--rw-r--r--   0        0        0       47 2024-04-28 00:02:48.593096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_temporal/test_time_literal_sql/0-sqlite/out.sql
--rw-r--r--   0        0        0       61 2024-04-28 00:02:48.593096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_temporal/test_time_literal_sql/0-trino/out.sql
--rw-r--r--   0        0        0       57 2024-04-28 00:02:48.593096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_temporal/test_time_literal_sql/234567-bigquery/out.sql
--rw-r--r--   0        0        0       86 2024-04-28 00:02:48.593096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_temporal/test_time_literal_sql/234567-clickhouse/out.sql
--rw-r--r--   0        0        0       76 2024-04-28 00:02:48.593096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_temporal/test_time_literal_sql/234567-datafusion/out.sql
--rw-r--r--   0        0        0       76 2024-04-28 00:02:48.593096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_temporal/test_time_literal_sql/234567-druid/out.sql
--rw-r--r--   0        0        0       70 2024-04-28 00:02:48.593096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_temporal/test_time_literal_sql/234567-duckdb/out.sql
--rw-r--r--   0        0        0       76 2024-04-28 00:02:48.593096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_temporal/test_time_literal_sql/234567-exasol/out.sql
--rw-r--r--   0        0        0       81 2024-04-28 00:02:48.593096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_temporal/test_time_literal_sql/234567-impala/out.sql
--rw-r--r--   0        0        0       73 2024-04-28 00:02:48.593096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_temporal/test_time_literal_sql/234567-mssql/out.sql
--rw-r--r--   0        0        0       69 2024-04-28 00:02:48.593096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_temporal/test_time_literal_sql/234567-mysql/out.sql
--rw-r--r--   0        0        0      107 2024-04-28 00:02:48.593096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_temporal/test_time_literal_sql/234567-oracle/out.sql
--rw-r--r--   0        0        0      122 2024-04-28 00:02:48.593096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_temporal/test_time_literal_sql/234567-postgres/out.sql
--rw-r--r--   0        0        0       76 2024-04-28 00:02:48.593096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_temporal/test_time_literal_sql/234567-risingwave/out.sql
--rw-r--r--   0        0        0       80 2024-04-28 00:02:48.593096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_temporal/test_time_literal_sql/234567-snowflake/out.sql
--rw-r--r--   0        0        0       62 2024-04-28 00:02:48.593096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_temporal/test_time_literal_sql/234567-sqlite/out.sql
--rw-r--r--   0        0        0       76 2024-04-28 00:02:48.593096 ibis_framework-9.0.0.dev686/ibis/backends/tests/snapshots/test_temporal/test_time_literal_sql/234567-trino/out.sql
--rw-r--r--   0        0        0        0 2024-04-28 00:02:48.593096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/__init__.py
--rw-r--r--   0        0        0     4714 2024-04-28 00:02:48.593096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/conftest.py
--rw-r--r--   0        0        0      335 2024-04-28 00:02:48.593096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_compiler/test_agg_and_non_agg_filter/out.sql
--rw-r--r--   0        0        0      372 2024-04-28 00:02:48.593096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_compiler/test_agg_filter/out.sql
--rw-r--r--   0        0        0      372 2024-04-28 00:02:48.593096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_compiler/test_agg_filter_with_alias/out.sql
--rw-r--r--   0        0        0      582 2024-04-28 00:02:48.593096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_compiler/test_column_distinct/decompiled.py
--rw-r--r--   0        0        0      104 2024-04-28 00:02:48.593096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_compiler/test_column_distinct/out.sql
--rw-r--r--   0        0        0      128 2024-04-28 00:02:48.593096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_compiler/test_column_expr_default_name/decompiled.py
--rw-r--r--   0        0        0       94 2024-04-28 00:02:48.593096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_compiler/test_column_expr_default_name/out.sql
--rw-r--r--   0        0        0      142 2024-04-28 00:02:48.593096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_compiler/test_column_expr_retains_name/decompiled.py
--rw-r--r--   0        0        0       82 2024-04-28 00:02:48.593096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_compiler/test_column_expr_retains_name/out.sql
--rw-r--r--   0        0        0      649 2024-04-28 00:02:48.593096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_compiler/test_count_distinct/decompiled.py
--rw-r--r--   0        0        0      484 2024-04-28 00:02:48.593096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_compiler/test_count_distinct/out.sql
--rw-r--r--   0        0        0      898 2024-04-28 00:02:48.593096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_compiler/test_difference_project_column/decompiled.py
--rw-r--r--   0        0        0      475 2024-04-28 00:02:48.593096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_compiler/test_difference_project_column/out.sql
--rw-r--r--   0        0        0      236 2024-04-28 00:02:48.593096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_compiler/test_having_from_filter/decompiled.py
--rw-r--r--   0        0        0      304 2024-04-28 00:02:48.593096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_compiler/test_having_from_filter/out.sql
--rw-r--r--   0        0        0      320 2024-04-28 00:02:48.593096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_compiler/test_having_size/out.sql
--rw-r--r--   0        0        0      903 2024-04-28 00:02:48.593096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_compiler/test_intersect_project_column/decompiled.py
--rw-r--r--   0        0        0      478 2024-04-28 00:02:48.593096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_compiler/test_intersect_project_column/out.sql
--rw-r--r--   0        0        0      737 2024-04-28 00:02:48.593096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_compiler/test_multiple_count_distinct/decompiled.py
--rw-r--r--   0        0        0      180 2024-04-28 00:02:48.593096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_compiler/test_multiple_count_distinct/out.sql
--rw-r--r--   0        0        0      377 2024-04-28 00:02:48.593096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_compiler/test_pushdown_with_or/out.sql
--rw-r--r--   0        0        0      314 2024-04-28 00:02:48.593096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_compiler/test_simple_agg_filter/out.sql
--rw-r--r--   0        0        0      407 2024-04-28 00:02:48.593096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_compiler/test_subquery_where_location/decompiled.py
--rw-r--r--   0        0        0      378 2024-04-28 00:02:48.593096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_compiler/test_subquery_where_location/out.sql
--rw-r--r--   0        0        0      851 2024-04-28 00:02:48.593096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_compiler/test_table_difference/decompiled.py
--rw-r--r--   0        0        0      491 2024-04-28 00:02:48.593096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_compiler/test_table_difference/out.sql
--rw-r--r--   0        0        0      617 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_compiler/test_table_distinct/decompiled.py
--rw-r--r--   0        0        0      124 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_compiler/test_table_distinct/out.sql
--rw-r--r--   0        0        0      446 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_compiler/test_table_drop_with_filter/decompiled.py
--rw-r--r--   0        0        0      355 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_compiler/test_table_drop_with_filter/out.sql
--rw-r--r--   0        0        0      850 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_compiler/test_table_intersect/decompiled.py
--rw-r--r--   0        0        0      494 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_compiler/test_table_intersect/out.sql
--rw-r--r--   0        0        0      846 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_compiler/test_union/decompiled.py
--rw-r--r--   0        0        0      490 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_compiler/test_union/out.sql
--rw-r--r--   0        0        0      124 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_compiler/test_union_order_by/decompiled.py
--rw-r--r--   0        0        0      228 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_compiler/test_union_order_by/out.sql
--rw-r--r--   0        0        0      857 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_compiler/test_union_project_column/decompiled.py
--rw-r--r--   0        0        0      478 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_compiler/test_union_project_column/out.sql
--rw-r--r--   0        0        0      661 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_aggregate_count_joined/decompiled.py
--rw-r--r--   0        0        0      290 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_aggregate_count_joined/out.sql
--rw-r--r--   0        0        0      195 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_aggregate_having/explicit.sql
--rw-r--r--   0        0        0      243 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_aggregate_having/inline.sql
--rw-r--r--   0        0        0      267 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_aggregate_projection_alias_bug/out.sql
--rw-r--r--   0        0        0      366 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_aggregate_projection_subquery/agg_filtered.sql
--rw-r--r--   0        0        0      405 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_aggregate_projection_subquery/agg_filtered2.sql
--rw-r--r--   0        0        0      252 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_aggregate_projection_subquery/filtered.sql
--rw-r--r--   0        0        0      231 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_aggregate_projection_subquery/proj.sql
--rw-r--r--   0        0        0      369 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_anti_join/decompiled.py
--rw-r--r--   0        0        0      145 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_anti_join/out.sql
--rw-r--r--   0        0        0      197 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_bool_bool/decompiled.py
--rw-r--r--   0        0        0      154 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_bool_bool/out.sql
--rw-r--r--   0        0        0      574 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_bug_duplicated_where/out.sql
--rw-r--r--   0        0        0     1096 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_bug_project_multiple_times/out.sql
--rw-r--r--   0        0        0      930 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_case_in_projection/decompiled.py
--rw-r--r--   0        0        0      388 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_case_in_projection/out.sql
--rw-r--r--   0        0        0      609 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_chain_limit_doesnt_collapse/result.sql
--rw-r--r--   0        0        0      384 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_complex_union/result.sql
--rw-r--r--   0        0        0      374 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_double_nested_subquery_no_aliases/out.sql
--rw-r--r--   0        0        0      202 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_endswith/decompiled.py
--rw-r--r--   0        0        0       67 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_endswith/out.sql
--rw-r--r--   0        0        0      173 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_exists_subquery/out.sql
--rw-r--r--   0        0        0      245 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_filter_inside_exists/out.sql
--rw-r--r--   0        0        0      172 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_filter_predicates/out.sql
--rw-r--r--   0        0        0      513 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_filter_self_join_analysis_bug/result.sql
--rw-r--r--   0        0        0      379 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_filter_subquery_derived_reduction/expr3.sql
--rw-r--r--   0        0        0      440 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_filter_subquery_derived_reduction/expr4.sql
--rw-r--r--   0        0        0      242 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_fuse_projections/decompiled.py
--rw-r--r--   0        0        0      148 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_fuse_projections/project.sql
--rw-r--r--   0        0        0      190 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_fuse_projections/project_filter.sql
--rw-r--r--   0        0        0       79 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_identifier_quoting/out.sql
--rw-r--r--   0        0        0      131 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_incorrect_predicate_pushdown/result.sql
--rw-r--r--   0        0        0       99 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_incorrect_predicate_pushdown_with_literal/result.sql
--rw-r--r--   0        0        0     1188 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_join_between_joins/decompiled.py
--rw-r--r--   0        0        0      403 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_join_between_joins/out.sql
--rw-r--r--   0        0        0      641 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_join_filtered_tables_no_pushdown/out.sql
--rw-r--r--   0        0        0     1369 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_join_just_materialized/decompiled.py
--rw-r--r--   0        0        0      490 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_join_just_materialized/out.sql
--rw-r--r--   0        0        0      430 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_join_projection_subquery_bug/out.sql
--rw-r--r--   0        0        0      565 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_join_with_conditional_aggregate/result.sql
--rw-r--r--   0        0        0      192 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_join_with_limited_table/out.sql
--rw-r--r--   0        0        0      397 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_limit_cte_extract/out.sql
--rw-r--r--   0        0        0     1981 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_limit_with_self_join/decompiled.py
--rw-r--r--   0        0        0     1051 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_limit_with_self_join/out.sql
--rw-r--r--   0        0        0      429 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_loj_subquery_filter_handling/out.sql
--rw-r--r--   0        0        0      540 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_multiple_joins/decompiled.py
--rw-r--r--   0        0        0      247 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_multiple_joins/out.sql
--rw-r--r--   0        0        0      552 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_multiple_limits/decompiled.py
--rw-r--r--   0        0        0       99 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_multiple_limits/out.sql
--rw-r--r--   0        0        0       72 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_nameless_table/decompiled.py
--rw-r--r--   0        0        0       23 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_nameless_table/out.sql
--rw-r--r--   0        0        0      696 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_order_by_on_limit_yield_subquery/decompiled.py
--rw-r--r--   0        0        0      212 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_order_by_on_limit_yield_subquery/out.sql
--rw-r--r--   0        0        0       98 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_projection_filter_fuse/out.sql
--rw-r--r--   0        0        0      174 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_scalar_subquery_different_table/out.sql
--rw-r--r--   0        0        0      213 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_select_sql/agg_explicit_column/decompiled.py
--rw-r--r--   0        0        0       84 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_select_sql/agg_explicit_column/out.sql
--rw-r--r--   0        0        0      227 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_select_sql/agg_string_columns/decompiled.py
--rw-r--r--   0        0        0      106 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_select_sql/agg_string_columns/out.sql
--rw-r--r--   0        0        0      161 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_select_sql/aggregate_table_count_metric/decompiled.py
--rw-r--r--   0        0        0       60 2024-04-28 00:02:48.597096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_select_sql/aggregate_table_count_metric/out.sql
--rw-r--r--   0        0        0      183 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_select_sql/filter_then_limit/decompiled.py
--rw-r--r--   0        0        0      131 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_select_sql/filter_then_limit/out.sql
--rw-r--r--   0        0        0      163 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_select_sql/limit_simple/decompiled.py
--rw-r--r--   0        0        0       40 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_select_sql/limit_simple/out.sql
--rw-r--r--   0        0        0      197 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_select_sql/limit_then_filter/decompiled.py
--rw-r--r--   0        0        0      167 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_select_sql/limit_then_filter/out.sql
--rw-r--r--   0        0        0      166 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_select_sql/limit_with_offset/decompiled.py
--rw-r--r--   0        0        0       49 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_select_sql/limit_with_offset/out.sql
--rw-r--r--   0        0        0      195 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_select_sql/mixed_columns_ascending/decompiled.py
--rw-r--r--   0        0        0       58 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_select_sql/mixed_columns_ascending/out.sql
--rw-r--r--   0        0        0      138 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_select_sql/self_reference_simple/decompiled.py
--rw-r--r--   0        0        0       34 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_select_sql/self_reference_simple/out.sql
--rw-r--r--   0        0        0      177 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_select_sql/single_column/decompiled.py
--rw-r--r--   0        0        0      108 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_select_sql/single_column/out.sql
--rw-r--r--   0        0        0      328 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_select_sql/test_physical_table_reference_translate/decompiled.py
--rw-r--r--   0        0        0       26 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_select_sql/test_physical_table_reference_translate/out.sql
--rw-r--r--   0        0        0      485 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_self_join_subquery_distinct_equal/out.sql
--rw-r--r--   0        0        0      369 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_semi_join/decompiled.py
--rw-r--r--   0        0        0      145 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_semi_join/out.sql
--rw-r--r--   0        0        0      402 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_simple_joins/decompiled.py
--rw-r--r--   0        0        0      146 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_simple_joins/inner.sql
--rw-r--r--   0        0        0      180 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_simple_joins/inner_two_preds.sql
--rw-r--r--   0        0        0      151 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_simple_joins/left.sql
--rw-r--r--   0        0        0      151 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_simple_joins/outer.sql
--rw-r--r--   0        0        0      141 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_sort_then_group_by_propagates_keys/result1.sql
--rw-r--r--   0        0        0      141 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_sort_then_group_by_propagates_keys/result2.sql
--rw-r--r--   0        0        0      204 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_startswith/decompiled.py
--rw-r--r--   0        0        0       72 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_startswith/out.sql
--rw-r--r--   0        0        0     1324 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_subquery_factor_correlated_subquery/out.sql
--rw-r--r--   0        0        0      178 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_subquery_in_filter_predicate/expr.sql
--rw-r--r--   0        0        0      341 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_subquery_in_filter_predicate/expr2.sql
--rw-r--r--   0        0        0      629 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_subquery_in_union/decompiled.py
--rw-r--r--   0        0        0      420 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_subquery_in_union/out.sql
--rw-r--r--   0        0        0      512 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_subquery_used_for_self_join/out.sql
--rw-r--r--   0        0        0      656 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_topk_analysis_bug/out.sql
--rw-r--r--   0        0        0      369 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_topk_operation/e1.sql
--rw-r--r--   0        0        0      381 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_topk_operation/e2.sql
--rw-r--r--   0        0        0     1178 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_topk_predicate_pushdown_bug/out.sql
--rw-r--r--   0        0        0      223 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_topk_to_aggregate/out.sql
--rw-r--r--   0        0        0      841 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_tpch_self_join_failure/out.sql
--rw-r--r--   0        0        0      728 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_where_analyze_scalar_op/decompiled.py
--rw-r--r--   0        0        0      591 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_where_analyze_scalar_op/out.sql
--rw-r--r--   0        0        0      458 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_where_no_pushdown_possible/decompiled.py
--rw-r--r--   0        0        0      339 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_where_no_pushdown_possible/out.sql
--rw-r--r--   0        0        0      438 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_where_with_between/decompiled.py
--rw-r--r--   0        0        0      264 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_where_with_between/out.sql
--rw-r--r--   0        0        0      469 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_where_with_join/decompiled.py
--rw-r--r--   0        0        0      396 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_select_sql/test_where_with_join/out.sql
--rw-r--r--   0        0        0      243 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_aggregate/having_count/out.sql
--rw-r--r--   0        0        0      195 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_aggregate/having_sum/out.sql
--rw-r--r--   0        0        0       84 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_aggregate/single/out.sql
--rw-r--r--   0        0        0      106 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_aggregate/two/out.sql
--rw-r--r--   0        0        0      121 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_between/out.sql
--rw-r--r--   0        0        0      156 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_boolean_conjunction/and/out.sql
--rw-r--r--   0        0        0      155 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_boolean_conjunction/or/out.sql
--rw-r--r--   0        0        0      211 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_coalesce/out.sql
--rw-r--r--   0        0        0       91 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_comparisons/eq/out.sql
--rw-r--r--   0        0        0       92 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_comparisons/ge/out.sql
--rw-r--r--   0        0        0       91 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_comparisons/gt/out.sql
--rw-r--r--   0        0        0       92 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_comparisons/le/out.sql
--rw-r--r--   0        0        0       91 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_comparisons/lt/out.sql
--rw-r--r--   0        0        0       92 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_comparisons/ne/out.sql
--rw-r--r--   0        0        0      291 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_cte_factor_distinct_but_equal/out.sql
--rw-r--r--   0        0        0       87 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_distinct/count_distinct/out.sql
--rw-r--r--   0        0        0      121 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_distinct/group_by_count_distinct/out.sql
--rw-r--r--   0        0        0      124 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_distinct/projection_distinct/out.sql
--rw-r--r--   0        0        0      104 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_distinct/single_column_projection_distinct/out.sql
--rw-r--r--   0        0        0       54 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_distinct/table_distinct/out.sql
--rw-r--r--   0        0        0       87 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_double_order_by/out.sql
--rw-r--r--   0        0        0      115 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_double_order_by_deferred/out.sql
--rw-r--r--   0        0        0      152 2024-04-28 00:02:48.601096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_double_order_by_different_expression/out.sql
--rw-r--r--   0        0        0       87 2024-04-28 00:02:48.605096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_double_order_by_not_fused/out.sql
--rw-r--r--   0        0        0      115 2024-04-28 00:02:48.605096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_double_order_by_same_column/out.sql
--rw-r--r--   0        0        0      179 2024-04-28 00:02:48.605096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_exists/e1.sql
--rw-r--r--   0        0        0      239 2024-04-28 00:02:48.605096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_exists/e2.sql
--rw-r--r--   0        0        0      217 2024-04-28 00:02:48.605096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_filter_group_by_agg_with_same_name/out.sql
--rw-r--r--   0        0        0      704 2024-04-28 00:02:48.605096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_gh_1045/out.sql
--rw-r--r--   0        0        0       78 2024-04-28 00:02:48.605096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_isnull_notnull/isnull/out.sql
--rw-r--r--   0        0        0       82 2024-04-28 00:02:48.605096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_isnull_notnull/notnull/out.sql
--rw-r--r--   0        0        0      490 2024-04-28 00:02:48.605096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_join_just_materialized/out.sql
--rw-r--r--   0        0        0      250 2024-04-28 00:02:48.605096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_joins/inner/out.sql
--rw-r--r--   0        0        0      191 2024-04-28 00:02:48.605096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_joins/inner_select/out.sql
--rw-r--r--   0        0        0      255 2024-04-28 00:02:48.605096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_joins/left/out.sql
--rw-r--r--   0        0        0      196 2024-04-28 00:02:48.605096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_joins/left_select/out.sql
--rw-r--r--   0        0        0      255 2024-04-28 00:02:48.605096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_joins/outer/out.sql
--rw-r--r--   0        0        0      196 2024-04-28 00:02:48.605096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_joins/outer_select/out.sql
--rw-r--r--   0        0        0       40 2024-04-28 00:02:48.605096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_limit/expr_fn0/out.sql
--rw-r--r--   0        0        0       49 2024-04-28 00:02:48.605096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_limit/expr_fn1/out.sql
--rw-r--r--   0        0        0      131 2024-04-28 00:02:48.605096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_limit_filter/out.sql
--rw-r--r--   0        0        0      167 2024-04-28 00:02:48.605096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_limit_subquery/out.sql
--rw-r--r--   0        0        0      512 2024-04-28 00:02:48.605096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_lower_projection_sort_key/decompiled.py
--rw-r--r--   0        0        0      405 2024-04-28 00:02:48.605096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_lower_projection_sort_key/out.sql
--rw-r--r--   0        0        0      330 2024-04-28 00:02:48.605096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_multi_join/out.sql
--rw-r--r--   0        0        0      100 2024-04-28 00:02:48.605096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_mutate_filter_join_no_cross_join/out.sql
--rw-r--r--   0        0        0       91 2024-04-28 00:02:48.605096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_named_expr/out.sql
--rw-r--r--   0        0        0      105 2024-04-28 00:02:48.605096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_negate/out.sql
--rw-r--r--   0        0        0      986 2024-04-28 00:02:48.605096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_no_cart_join/out.sql
--rw-r--r--   0        0        0     1517 2024-04-28 00:02:48.605096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_no_cartesian_join/out.sql
--rw-r--r--   0        0        0      319 2024-04-28 00:02:48.605096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_no_cross_join/out.sql
--rw-r--r--   0        0        0      205 2024-04-28 00:02:48.605096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_not_exists/out.sql
--rw-r--r--   0        0        0      108 2024-04-28 00:02:48.605096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_order_by/column/out.sql
--rw-r--r--   0        0        0      108 2024-04-28 00:02:48.605096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_order_by/random/out.sql
--rw-r--r--   0        0        0      115 2024-04-28 00:02:48.605096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_order_by_expr/out.sql
--rw-r--r--   0        0        0      238 2024-04-28 00:02:48.605096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_searched_case/out.sql
--rw-r--r--   0        0        0      457 2024-04-28 00:02:48.605096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_self_reference_in_not_exists/anti.sql
--rw-r--r--   0        0        0      431 2024-04-28 00:02:48.605096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_self_reference_in_not_exists/semi.sql
--rw-r--r--   0        0        0      146 2024-04-28 00:02:48.605096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_self_reference_join/out.sql
--rw-r--r--   0        0        0      118 2024-04-28 00:02:48.605096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_simple_case/out.sql
--rw-r--r--   0        0        0      206 2024-04-28 00:02:48.605096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_sort_aggregation_translation_failure/out.sql
--rw-r--r--   0        0        0      231 2024-04-28 00:02:48.605096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_subquery_aliased/out.sql
--rw-r--r--   0        0        0      349 2024-04-28 00:02:48.605096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_where_correlated_subquery/out.sql
--rw-r--r--   0        0        0      699 2024-04-28 00:02:48.605096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_where_correlated_subquery_with_join/out.sql
--rw-r--r--   0        0        0      198 2024-04-28 00:02:48.605096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_where_simple_comparisons/decompiled.py
--rw-r--r--   0        0        0      177 2024-04-28 00:02:48.605096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_where_simple_comparisons/out.sql
--rw-r--r--   0        0        0      162 2024-04-28 00:02:48.605096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/snapshots/test_sql/test_where_uncorrelated_subquery/out.sql
--rw-r--r--   0        0        0     7554 2024-04-28 00:02:48.605096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/test_compiler.py
--rw-r--r--   0        0        0    26432 2024-04-28 00:02:48.605096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/test_select_sql.py
--rw-r--r--   0        0        0    18149 2024-04-28 00:02:48.605096 ibis_framework-9.0.0.dev686/ibis/backends/tests/sql/test_sql.py
--rw-r--r--   0        0        0    51522 2024-04-28 00:02:48.605096 ibis_framework-9.0.0.dev686/ibis/backends/tests/test_aggregation.py
--rw-r--r--   0        0        0     4167 2024-04-28 00:02:48.605096 ibis_framework-9.0.0.dev686/ibis/backends/tests/test_api.py
--rw-r--r--   0        0        0    42382 2024-04-28 00:02:48.605096 ibis_framework-9.0.0.dev686/ibis/backends/tests/test_array.py
--rw-r--r--   0        0        0     4432 2024-04-28 00:02:48.605096 ibis_framework-9.0.0.dev686/ibis/backends/tests/test_asof_join.py
--rw-r--r--   0        0        0      982 2024-04-28 00:02:48.605096 ibis_framework-9.0.0.dev686/ibis/backends/tests/test_binary.py
--rw-r--r--   0        0        0    51142 2024-04-28 00:02:48.605096 ibis_framework-9.0.0.dev686/ibis/backends/tests/test_client.py
--rw-r--r--   0        0        0     1126 2024-04-28 00:02:48.605096 ibis_framework-9.0.0.dev686/ibis/backends/tests/test_column.py
--rw-r--r--   0        0        0     3916 2024-04-28 00:02:48.609096 ibis_framework-9.0.0.dev686/ibis/backends/tests/test_dataframe_interchange.py
--rw-r--r--   0        0        0    10640 2024-04-28 00:02:48.609096 ibis_framework-9.0.0.dev686/ibis/backends/tests/test_dot_sql.py
--rw-r--r--   0        0        0     1439 2024-04-28 00:02:48.609096 ibis_framework-9.0.0.dev686/ibis/backends/tests/test_examples.py
--rw-r--r--   0        0        0    16767 2024-04-28 00:02:48.609096 ibis_framework-9.0.0.dev686/ibis/backends/tests/test_export.py
--rw-r--r--   0        0        0    65814 2024-04-28 00:02:48.609096 ibis_framework-9.0.0.dev686/ibis/backends/tests/test_generic.py
--rw-r--r--   0        0        0     2643 2024-04-28 00:02:48.609096 ibis_framework-9.0.0.dev686/ibis/backends/tests/test_interactive.py
--rw-r--r--   0        0        0    12415 2024-04-28 00:02:48.609096 ibis_framework-9.0.0.dev686/ibis/backends/tests/test_join.py
--rw-r--r--   0        0        0     4345 2024-04-28 00:02:48.609096 ibis_framework-9.0.0.dev686/ibis/backends/tests/test_json.py
--rw-r--r--   0        0        0    20456 2024-04-28 00:02:48.609096 ibis_framework-9.0.0.dev686/ibis/backends/tests/test_map.py
--rw-r--r--   0        0        0     1541 2024-04-28 00:02:48.609096 ibis_framework-9.0.0.dev686/ibis/backends/tests/test_markers.py
--rw-r--r--   0        0        0     4046 2024-04-28 00:02:48.609096 ibis_framework-9.0.0.dev686/ibis/backends/tests/test_network.py
--rw-r--r--   0        0        0    52401 2024-04-28 00:02:48.609096 ibis_framework-9.0.0.dev686/ibis/backends/tests/test_numeric.py
--rw-r--r--   0        0        0     6441 2024-04-28 00:02:48.609096 ibis_framework-9.0.0.dev686/ibis/backends/tests/test_param.py
--rw-r--r--   0        0        0    14141 2024-04-28 00:02:48.609096 ibis_framework-9.0.0.dev686/ibis/backends/tests/test_register.py
--rw-r--r--   0        0        0     8795 2024-04-28 00:02:48.609096 ibis_framework-9.0.0.dev686/ibis/backends/tests/test_set_ops.py
--rw-r--r--   0        0        0     6319 2024-04-28 00:02:48.609096 ibis_framework-9.0.0.dev686/ibis/backends/tests/test_sql.py
--rw-r--r--   0        0        0    33609 2024-04-28 00:02:48.609096 ibis_framework-9.0.0.dev686/ibis/backends/tests/test_string.py
--rw-r--r--   0        0        0     8530 2024-04-28 00:02:48.609096 ibis_framework-9.0.0.dev686/ibis/backends/tests/test_struct.py
--rw-r--r--   0        0        0    83646 2024-04-28 00:02:48.609096 ibis_framework-9.0.0.dev686/ibis/backends/tests/test_temporal.py
--rw-r--r--   0        0        0     5605 2024-04-28 00:02:48.609096 ibis_framework-9.0.0.dev686/ibis/backends/tests/test_udf.py
--rw-r--r--   0        0        0     1718 2024-04-28 00:02:48.609096 ibis_framework-9.0.0.dev686/ibis/backends/tests/test_uuid.py
--rw-r--r--   0        0        0    26307 2024-04-28 00:02:48.609096 ibis_framework-9.0.0.dev686/ibis/backends/tests/test_vectorized_udf.py
--rw-r--r--   0        0        0    40344 2024-04-28 00:02:48.609096 ibis_framework-9.0.0.dev686/ibis/backends/tests/test_window.py
--rw-r--r--   0        0        0        0 2024-04-28 00:02:48.609096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/__init__.py
--rw-r--r--   0        0        0     4113 2024-04-28 00:02:48.609096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/conftest.py
--rw-r--r--   0        0        0      545 2024-04-28 00:02:48.609096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/queries/duckdb/h01.sql
--rw-r--r--   0        0        0      841 2024-04-28 00:02:48.609096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/queries/duckdb/h02.sql
--rw-r--r--   0        0        0      468 2024-04-28 00:02:48.609096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/queries/duckdb/h03.sql
--rw-r--r--   0        0        0      408 2024-04-28 00:02:48.609096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/queries/duckdb/h04.sql
--rw-r--r--   0        0        0      522 2024-04-28 00:02:48.609096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/queries/duckdb/h05.sql
--rw-r--r--   0        0        0      243 2024-04-28 00:02:48.609096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/queries/duckdb/h06.sql
--rw-r--r--   0        0        0      954 2024-04-28 00:02:48.609096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/queries/duckdb/h07.sql
--rw-r--r--   0        0        0      950 2024-04-28 00:02:48.609096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/queries/duckdb/h08.sql
--rw-r--r--   0        0        0      678 2024-04-28 00:02:48.609096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/queries/duckdb/h09.sql
--rw-r--r--   0        0        0      580 2024-04-28 00:02:48.609096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/queries/duckdb/h10.sql
--rw-r--r--   0        0        0      582 2024-04-28 00:02:48.609096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/queries/duckdb/h11.sql
--rw-r--r--   0        0        0      704 2024-04-28 00:02:48.609096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/queries/duckdb/h12.sql
--rw-r--r--   0        0        0      366 2024-04-28 00:02:48.609096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/queries/duckdb/h13.sql
--rw-r--r--   0        0        0      364 2024-04-28 00:02:48.609096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/queries/duckdb/h14.sql
--rw-r--r--   0        0        0      951 2024-04-28 00:02:48.609096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/queries/duckdb/h15.sql
--rw-r--r--   0        0        0      553 2024-04-28 00:02:48.609096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/queries/duckdb/h16.sql
--rw-r--r--   0        0        0      329 2024-04-28 00:02:48.609096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/queries/duckdb/h17.sql
--rw-r--r--   0        0        0      555 2024-04-28 00:02:48.609096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/queries/duckdb/h18.sql
--rw-r--r--   0        0        0     1043 2024-04-28 00:02:48.609096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/queries/duckdb/h19.sql
--rw-r--r--   0        0        0      909 2024-04-28 00:02:48.609096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/queries/duckdb/h20.sql
--rw-r--r--   0        0        0      803 2024-04-28 00:02:48.609096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/queries/duckdb/h21.sql
--rw-r--r--   0        0        0      807 2024-04-28 00:02:48.609096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/queries/duckdb/h22.sql
--rw-r--r--   0        0        0     1470 2024-04-28 00:02:48.609096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/snapshots/test_h01/test_tpc_h01/duckdb/h01.sql
--rw-r--r--   0        0        0     1728 2024-04-28 00:02:48.613096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/snapshots/test_h01/test_tpc_h01/snowflake/h01.sql
--rw-r--r--   0        0        0     1607 2024-04-28 00:02:48.613096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/snapshots/test_h01/test_tpc_h01/trino/h01.sql
--rw-r--r--   0        0        0     3009 2024-04-28 00:02:48.613096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/snapshots/test_h02/test_tpc_h02/duckdb/h02.sql
--rw-r--r--   0        0        0     4455 2024-04-28 00:02:48.613096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/snapshots/test_h02/test_tpc_h02/snowflake/h02.sql
--rw-r--r--   0        0        0     4066 2024-04-28 00:02:48.613096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/snapshots/test_h02/test_tpc_h02/trino/h02.sql
--rw-r--r--   0        0        0     2552 2024-04-28 00:02:48.613096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/snapshots/test_h03/test_tpc_h03/duckdb/h03.sql
--rw-r--r--   0        0        0     4324 2024-04-28 00:02:48.613096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/snapshots/test_h03/test_tpc_h03/snowflake/h03.sql
--rw-r--r--   0        0        0     3967 2024-04-28 00:02:48.613096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/snapshots/test_h03/test_tpc_h03/trino/h03.sql
--rw-r--r--   0        0        0      854 2024-04-28 00:02:48.613096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/snapshots/test_h04/test_tpc_h04/duckdb/h04.sql
--rw-r--r--   0        0        0     1382 2024-04-28 00:02:48.613096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/snapshots/test_h04/test_tpc_h04/snowflake/h04.sql
--rw-r--r--   0        0        0     1243 2024-04-28 00:02:48.613096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/snapshots/test_h04/test_tpc_h04/trino/h04.sql
--rw-r--r--   0        0        0     3457 2024-04-28 00:02:48.613096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/snapshots/test_h05/test_tpc_h05/duckdb/h05.sql
--rw-r--r--   0        0        0     6105 2024-04-28 00:02:48.613096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/snapshots/test_h05/test_tpc_h05/snowflake/h05.sql
--rw-r--r--   0        0        0     5531 2024-04-28 00:02:48.613096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/snapshots/test_h05/test_tpc_h05/trino/h05.sql
--rw-r--r--   0        0        0      733 2024-04-28 00:02:48.613096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/snapshots/test_h06/test_tpc_h06/duckdb/h06.sql
--rw-r--r--   0        0        0      999 2024-04-28 00:02:48.613096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/snapshots/test_h06/test_tpc_h06/snowflake/h06.sql
--rw-r--r--   0        0        0      962 2024-04-28 00:02:48.613096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/snapshots/test_h06/test_tpc_h06/trino/h06.sql
--rw-r--r--   0        0        0     1785 2024-04-28 00:02:48.613096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/snapshots/test_h07/test_tpc_h07/duckdb/h07.sql
--rw-r--r--   0        0        0     4153 2024-04-28 00:02:48.613096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/snapshots/test_h07/test_tpc_h07/snowflake/h07.sql
--rw-r--r--   0        0        0     3651 2024-04-28 00:02:48.613096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/snapshots/test_h07/test_tpc_h07/trino/h07.sql
--rw-r--r--   0        0        0     1611 2024-04-28 00:02:48.613096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/snapshots/test_h08/test_tpc_h08/duckdb/h08.sql
--rw-r--r--   0        0        0     4614 2024-04-28 00:02:48.613096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/snapshots/test_h08/test_tpc_h08/snowflake/h08.sql
--rw-r--r--   0        0        0     3963 2024-04-28 00:02:48.613096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/snapshots/test_h08/test_tpc_h08/trino/h08.sql
--rw-r--r--   0        0        0     1257 2024-04-28 00:02:48.613096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/snapshots/test_h09/test_tpc_h09/duckdb/h09.sql
--rw-r--r--   0        0        0     3990 2024-04-28 00:02:48.613096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/snapshots/test_h09/test_tpc_h09/snowflake/h09.sql
--rw-r--r--   0        0        0     3410 2024-04-28 00:02:48.613096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/snapshots/test_h09/test_tpc_h09/trino/h09.sql
--rw-r--r--   0        0        0     2981 2024-04-28 00:02:48.613096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/snapshots/test_h10/test_tpc_h10/duckdb/h10.sql
--rw-r--r--   0        0        0     5069 2024-04-28 00:02:48.613096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/snapshots/test_h10/test_tpc_h10/snowflake/h10.sql
--rw-r--r--   0        0        0     4641 2024-04-28 00:02:48.613096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/snapshots/test_h10/test_tpc_h10/trino/h10.sql
--rw-r--r--   0        0        0     1457 2024-04-28 00:02:48.613096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/snapshots/test_h11/test_tpc_h11/duckdb/h11.sql
--rw-r--r--   0        0        0     2359 2024-04-28 00:02:48.613096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/snapshots/test_h11/test_tpc_h11/snowflake/h11.sql
--rw-r--r--   0        0        0     2148 2024-04-28 00:02:48.613096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/snapshots/test_h11/test_tpc_h11/trino/h11.sql
--rw-r--r--   0        0        0     2413 2024-04-28 00:02:48.613096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/snapshots/test_h12/test_tpc_h12/duckdb/h12.sql
--rw-r--r--   0        0        0     3560 2024-04-28 00:02:48.613096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/snapshots/test_h12/test_tpc_h12/snowflake/h12.sql
--rw-r--r--   0        0        0     3314 2024-04-28 00:02:48.613096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/snapshots/test_h12/test_tpc_h12/trino/h12.sql
--rw-r--r--   0        0        0     1003 2024-04-28 00:02:48.613096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/snapshots/test_h13/test_tpc_h13/duckdb/h13.sql
--rw-r--r--   0        0        0     1954 2024-04-28 00:02:48.613096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/snapshots/test_h13/test_tpc_h13/snowflake/h13.sql
--rw-r--r--   0        0        0     1705 2024-04-28 00:02:48.613096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/snapshots/test_h13/test_tpc_h13/trino/h13.sql
--rw-r--r--   0        0        0     1800 2024-04-28 00:02:48.613096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/snapshots/test_h14/test_tpc_h14/duckdb/h14.sql
--rw-r--r--   0        0        0     2906 2024-04-28 00:02:48.617096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/snapshots/test_h14/test_tpc_h14/snowflake/h14.sql
--rw-r--r--   0        0        0     2702 2024-04-28 00:02:48.617096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/snapshots/test_h14/test_tpc_h14/trino/h14.sql
--rw-r--r--   0        0        0     1396 2024-04-28 00:02:48.617096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/snapshots/test_h15/test_tpc_h15/duckdb/h15.sql
--rw-r--r--   0        0        0     2029 2024-04-28 00:02:48.617096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/snapshots/test_h15/test_tpc_h15/snowflake/h15.sql
--rw-r--r--   0        0        0     1832 2024-04-28 00:02:48.617096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/snapshots/test_h15/test_tpc_h15/trino/h15.sql
--rw-r--r--   0        0        0     1759 2024-04-28 00:02:48.617096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/snapshots/test_h16/test_tpc_h16/duckdb/h16.sql
--rw-r--r--   0        0        0     2457 2024-04-28 00:02:48.617096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/snapshots/test_h16/test_tpc_h16/snowflake/h16.sql
--rw-r--r--   0        0        0     2255 2024-04-28 00:02:48.617096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/snapshots/test_h16/test_tpc_h16/trino/h16.sql
--rw-r--r--   0        0        0     2301 2024-04-28 00:02:48.617096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/snapshots/test_h17/test_tpc_h17/duckdb/h17.sql
--rw-r--r--   0        0        0     3785 2024-04-28 00:02:48.617096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/snapshots/test_h17/test_tpc_h17/snowflake/h17.sql
--rw-r--r--   0        0        0     3466 2024-04-28 00:02:48.617096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/snapshots/test_h17/test_tpc_h17/trino/h17.sql
--rw-r--r--   0        0        0     2788 2024-04-28 00:02:48.617096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/snapshots/test_h18/test_tpc_h18/duckdb/h18.sql
--rw-r--r--   0        0        0     4477 2024-04-28 00:02:48.617096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/snapshots/test_h18/test_tpc_h18/snowflake/h18.sql
--rw-r--r--   0        0        0     4114 2024-04-28 00:02:48.617096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/snapshots/test_h18/test_tpc_h18/trino/h18.sql
--rw-r--r--   0        0        0     3630 2024-04-28 00:02:48.617096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/snapshots/test_h19/test_tpc_h19/duckdb/h19.sql
--rw-r--r--   0        0        0     4622 2024-04-28 00:02:48.617096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/snapshots/test_h19/test_tpc_h19/snowflake/h19.sql
--rw-r--r--   0        0        0     4396 2024-04-28 00:02:48.617096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/snapshots/test_h19/test_tpc_h19/trino/h19.sql
--rw-r--r--   0        0        0     2438 2024-04-28 00:02:48.617096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/snapshots/test_h20/test_tpc_h20/duckdb/h20.sql
--rw-r--r--   0        0        0     3727 2024-04-28 00:02:48.617096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/snapshots/test_h20/test_tpc_h20/snowflake/h20.sql
--rw-r--r--   0        0        0     3387 2024-04-28 00:02:48.617096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/snapshots/test_h20/test_tpc_h20/trino/h20.sql
--rw-r--r--   0        0        0     1773 2024-04-28 00:02:48.617096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/snapshots/test_h21/test_tpc_h21/duckdb/h21.sql
--rw-r--r--   0        0        0     3652 2024-04-28 00:02:48.617096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/snapshots/test_h21/test_tpc_h21/snowflake/h21.sql
--rw-r--r--   0        0        0     3229 2024-04-28 00:02:48.617096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/snapshots/test_h21/test_tpc_h21/trino/h21.sql
--rw-r--r--   0        0        0     2338 2024-04-28 00:02:48.617096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/snapshots/test_h22/test_tpc_h22/duckdb/h22.sql
--rw-r--r--   0        0        0     2367 2024-04-28 00:02:48.617096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/snapshots/test_h22/test_tpc_h22/snowflake/h22.sql
--rw-r--r--   0        0        0     2191 2024-04-28 00:02:48.617096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/snapshots/test_h22/test_tpc_h22/trino/h22.sql
--rw-r--r--   0        0        0     1445 2024-04-28 00:02:48.617096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/test_h01.py
--rw-r--r--   0        0        0     1417 2024-04-28 00:02:48.617096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/test_h02.py
--rw-r--r--   0        0        0      778 2024-04-28 00:02:48.617096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/test_h03.py
--rw-r--r--   0        0        0      663 2024-04-28 00:02:48.617096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/test_h04.py
--rw-r--r--   0        0        0     1046 2024-04-28 00:02:48.617096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/test_h05.py
--rw-r--r--   0        0        0      666 2024-04-28 00:02:48.617096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/test_h06.py
--rw-r--r--   0        0        0     1413 2024-04-28 00:02:48.617096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/test_h07.py
--rw-r--r--   0        0        0     1516 2024-04-28 00:02:48.617096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/test_h08.py
--rw-r--r--   0        0        0     1100 2024-04-28 00:02:48.617096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/test_h09.py
--rw-r--r--   0        0        0     1019 2024-04-28 00:02:48.617096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/test_h10.py
--rw-r--r--   0        0        0      908 2024-04-28 00:02:48.617096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/test_h11.py
--rw-r--r--   0        0        0     1337 2024-04-28 00:02:48.617096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/test_h12.py
--rw-r--r--   0        0        0      775 2024-04-28 00:02:48.617096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/test_h13.py
--rw-r--r--   0        0        0      689 2024-04-28 00:02:48.617096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/test_h14.py
--rw-r--r--   0        0        0      986 2024-04-28 00:02:48.617096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/test_h15.py
--rw-r--r--   0        0        0     1098 2024-04-28 00:02:48.617096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/test_h16.py
--rw-r--r--   0        0        0      850 2024-04-28 00:02:48.617096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/test_h17.py
--rw-r--r--   0        0        0     1025 2024-04-28 00:02:48.617096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/test_h18.py
--rw-r--r--   0        0        0     1758 2024-04-28 00:02:48.617096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/test_h19.py
--rw-r--r--   0        0        0     1161 2024-04-28 00:02:48.617096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/test_h20.py
--rw-r--r--   0        0        0     1451 2024-04-28 00:02:48.617096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/test_h21.py
--rw-r--r--   0        0        0     1085 2024-04-28 00:02:48.617096 ibis_framework-9.0.0.dev686/ibis/backends/tests/tpch/test_h22.py
--rw-r--r--   0        0        0    17111 2024-04-28 00:02:48.617096 ibis_framework-9.0.0.dev686/ibis/backends/trino/__init__.py
--rw-r--r--   0        0        0    17732 2024-04-28 00:02:48.617096 ibis_framework-9.0.0.dev686/ibis/backends/trino/compiler.py
--rw-r--r--   0        0        0      754 2024-04-28 00:02:48.617096 ibis_framework-9.0.0.dev686/ibis/backends/trino/converter.py
--rw-r--r--   0        0        0        0 2024-04-28 00:02:48.617096 ibis_framework-9.0.0.dev686/ibis/backends/trino/tests/__init__.py
--rw-r--r--   0        0        0     5864 2024-04-28 00:02:48.617096 ibis_framework-9.0.0.dev686/ibis/backends/trino/tests/conftest.py
--rw-r--r--   0        0        0      174 2024-04-28 00:02:48.617096 ibis_framework-9.0.0.dev686/ibis/backends/trino/tests/snapshots/test_client/test_builtin_scalar_udf/result.txt
--rw-r--r--   0        0        0     5236 2024-04-28 00:02:48.617096 ibis_framework-9.0.0.dev686/ibis/backends/trino/tests/test_client.py
--rw-r--r--   0        0        0     2239 2024-04-28 00:02:48.617096 ibis_framework-9.0.0.dev686/ibis/backends/trino/tests/test_datatypes.py
--rw-r--r--   0        0        0        0 2024-04-28 00:02:48.617096 ibis_framework-9.0.0.dev686/ibis/common/__init__.py
--rw-r--r--   0        0        0    19763 2024-04-28 00:02:48.621096 ibis_framework-9.0.0.dev686/ibis/common/annotations.py
--rw-r--r--   0        0        0     7743 2024-04-28 00:02:48.621096 ibis_framework-9.0.0.dev686/ibis/common/bases.py
--rw-r--r--   0        0        0     3159 2024-04-28 00:02:48.621096 ibis_framework-9.0.0.dev686/ibis/common/caching.py
--rw-r--r--   0        0        0    10291 2024-04-28 00:02:48.621096 ibis_framework-9.0.0.dev686/ibis/common/collections.py
--rw-r--r--   0        0        0    18211 2024-04-28 00:02:48.621096 ibis_framework-9.0.0.dev686/ibis/common/deferred.py
--rw-r--r--   0        0        0     7283 2024-04-28 00:02:48.621096 ibis_framework-9.0.0.dev686/ibis/common/dispatch.py
--rw-r--r--   0        0        0    24797 2024-04-28 00:02:48.621096 ibis_framework-9.0.0.dev686/ibis/common/egraph.py
--rw-r--r--   0        0        0     4225 2024-04-28 00:02:48.621096 ibis_framework-9.0.0.dev686/ibis/common/exceptions.py
--rw-r--r--   0        0        0    24157 2024-04-28 00:02:48.621096 ibis_framework-9.0.0.dev686/ibis/common/graph.py
--rw-r--r--   0        0        0     8160 2024-04-28 00:02:48.621096 ibis_framework-9.0.0.dev686/ibis/common/grounds.py
--rw-r--r--   0        0        0     1395 2024-04-28 00:02:48.621096 ibis_framework-9.0.0.dev686/ibis/common/numeric.py
--rw-r--r--   0        0        0    49056 2024-04-28 00:02:48.621096 ibis_framework-9.0.0.dev686/ibis/common/patterns.py
--rw-r--r--   0        0        0     6684 2024-04-28 00:02:48.621096 ibis_framework-9.0.0.dev686/ibis/common/temporal.py
--rw-r--r--   0        0        0        0 2024-04-28 00:02:48.621096 ibis_framework-9.0.0.dev686/ibis/common/tests/__init__.py
--rw-r--r--   0        0        0      161 2024-04-28 00:02:48.621096 ibis_framework-9.0.0.dev686/ibis/common/tests/conftest.py
--rw-r--r--   0        0        0       96 2024-04-28 00:02:48.621096 ibis_framework-9.0.0.dev686/ibis/common/tests/snapshots/test_annotations/test_annotated_function_without_decoration/error.txt
--rw-r--r--   0        0        0      114 2024-04-28 00:02:48.621096 ibis_framework-9.0.0.dev686/ibis/common/tests/snapshots/test_annotations/test_signature_from_callable_with_keyword_only_arguments/too_many_positional_arguments.txt
--rw-r--r--   0        0        0      131 2024-04-28 00:02:48.621096 ibis_framework-9.0.0.dev686/ibis/common/tests/snapshots/test_annotations/test_signature_from_callable_with_positional_only_arguments/parameter_is_positional_only.txt
--rw-r--r--   0        0        0      375 2024-04-28 00:02:48.621096 ibis_framework-9.0.0.dev686/ibis/common/tests/snapshots/test_grounds/test_error_message/error_message.txt
--rw-r--r--   0        0        0      385 2024-04-28 00:02:48.621096 ibis_framework-9.0.0.dev686/ibis/common/tests/snapshots/test_grounds/test_error_message/error_message_py311.txt
--rw-r--r--   0        0        0    13351 2024-04-28 00:02:48.621096 ibis_framework-9.0.0.dev686/ibis/common/tests/test_annotations.py
--rw-r--r--   0        0        0     8552 2024-04-28 00:02:48.621096 ibis_framework-9.0.0.dev686/ibis/common/tests/test_bases.py
--rw-r--r--   0        0        0    12527 2024-04-28 00:02:48.621096 ibis_framework-9.0.0.dev686/ibis/common/tests/test_collections.py
--rw-r--r--   0        0        0    15334 2024-04-28 00:02:48.621096 ibis_framework-9.0.0.dev686/ibis/common/tests/test_deferred.py
--rw-r--r--   0        0        0     4578 2024-04-28 00:02:48.621096 ibis_framework-9.0.0.dev686/ibis/common/tests/test_dispatch.py
--rw-r--r--   0        0        0    12986 2024-04-28 00:02:48.621096 ibis_framework-9.0.0.dev686/ibis/common/tests/test_egraph.py
--rw-r--r--   0        0        0    11959 2024-04-28 00:02:48.621096 ibis_framework-9.0.0.dev686/ibis/common/tests/test_graph.py
--rw-r--r--   0        0        0     1866 2024-04-28 00:02:48.621096 ibis_framework-9.0.0.dev686/ibis/common/tests/test_graph_benchmarks.py
--rw-r--r--   0        0        0    26709 2024-04-28 00:02:48.621096 ibis_framework-9.0.0.dev686/ibis/common/tests/test_grounds.py
--rw-r--r--   0        0        0      937 2024-04-28 00:02:48.621096 ibis_framework-9.0.0.dev686/ibis/common/tests/test_grounds_benchmarks.py
--rw-r--r--   0        0        0     1011 2024-04-28 00:02:48.621096 ibis_framework-9.0.0.dev686/ibis/common/tests/test_grounds_py310.py
--rw-r--r--   0        0        0     2314 2024-04-28 00:02:48.621096 ibis_framework-9.0.0.dev686/ibis/common/tests/test_numeric.py
--rw-r--r--   0        0        0    39898 2024-04-28 00:02:48.621096 ibis_framework-9.0.0.dev686/ibis/common/tests/test_patterns.py
--rw-r--r--   0        0        0     8384 2024-04-28 00:02:48.621096 ibis_framework-9.0.0.dev686/ibis/common/tests/test_temporal.py
--rw-r--r--   0        0        0     3698 2024-04-28 00:02:48.621096 ibis_framework-9.0.0.dev686/ibis/common/tests/test_typing.py
--rw-r--r--   0        0        0     7609 2024-04-28 00:02:48.621096 ibis_framework-9.0.0.dev686/ibis/common/typing.py
--rw-r--r--   0        0        0     5776 2024-04-28 00:02:48.621096 ibis_framework-9.0.0.dev686/ibis/config.py
--rw-r--r--   0        0        0     1356 2024-04-28 00:02:48.621096 ibis_framework-9.0.0.dev686/ibis/conftest.py
--rw-r--r--   0        0        0      758 2024-04-28 00:02:48.621096 ibis_framework-9.0.0.dev686/ibis/examples/CITATIONS.md
--rw-r--r--   0        0        0      983 2024-04-28 00:02:48.621096 ibis_framework-9.0.0.dev686/ibis/examples/README.md
--rw-r--r--   0        0        0     4689 2024-04-28 00:02:48.621096 ibis_framework-9.0.0.dev686/ibis/examples/__init__.py
--rw-r--r--   0        0        0    39401 2024-04-28 00:02:48.621096 ibis_framework-9.0.0.dev686/ibis/examples/metadata.json
--rw-r--r--   0        0        0   210524 2024-04-28 00:02:48.625096 ibis_framework-9.0.0.dev686/ibis/examples/pixi.lock
--rw-r--r--   0        0        0      499 2024-04-28 00:02:48.625096 ibis_framework-9.0.0.dev686/ibis/examples/pixi.toml
--rw-r--r--   0        0        0        0 2024-04-28 00:02:48.625096 ibis_framework-9.0.0.dev686/ibis/examples/tests/__init__.py
--rw-r--r--   0        0        0     2386 2024-04-28 00:02:48.625096 ibis_framework-9.0.0.dev686/ibis/examples/tests/test_examples.py
--rw-r--r--   0        0        0        0 2024-04-28 00:02:48.625096 ibis_framework-9.0.0.dev686/ibis/expr/__init__.py
--rw-r--r--   0        0        0      896 2024-04-28 00:02:48.625096 ibis_framework-9.0.0.dev686/ibis/expr/analysis.py
--rw-r--r--   0        0        0    66984 2024-04-28 00:02:48.625096 ibis_framework-9.0.0.dev686/ibis/expr/api.py
--rw-r--r--   0        0        0    10969 2024-04-28 00:02:48.625096 ibis_framework-9.0.0.dev686/ibis/expr/builders.py
--rw-r--r--   0        0        0     1327 2024-04-28 00:02:48.625096 ibis_framework-9.0.0.dev686/ibis/expr/datashape.py
--rw-r--r--   0        0        0      523 2024-04-28 00:02:48.625096 ibis_framework-9.0.0.dev686/ibis/expr/datatypes/__init__.py
--rw-r--r--   0        0        0     5036 2024-04-28 00:02:48.625096 ibis_framework-9.0.0.dev686/ibis/expr/datatypes/cast.py
--rw-r--r--   0        0        0    29035 2024-04-28 00:02:48.625096 ibis_framework-9.0.0.dev686/ibis/expr/datatypes/core.py
--rw-r--r--   0        0        0     5992 2024-04-28 00:02:48.625096 ibis_framework-9.0.0.dev686/ibis/expr/datatypes/parse.py
--rw-r--r--   0        0        0        0 2024-04-28 00:02:48.625096 ibis_framework-9.0.0.dev686/ibis/expr/datatypes/tests/__init__.py
--rw-r--r--   0        0        0     3110 2024-04-28 00:02:48.625096 ibis_framework-9.0.0.dev686/ibis/expr/datatypes/tests/test_cast.py
--rw-r--r--   0        0        0    17967 2024-04-28 00:02:48.625096 ibis_framework-9.0.0.dev686/ibis/expr/datatypes/tests/test_core.py
--rw-r--r--   0        0        0     8028 2024-04-28 00:02:48.625096 ibis_framework-9.0.0.dev686/ibis/expr/datatypes/tests/test_parse.py
--rw-r--r--   0        0        0    13969 2024-04-28 00:02:48.625096 ibis_framework-9.0.0.dev686/ibis/expr/datatypes/tests/test_value.py
--rw-r--r--   0        0        0    11526 2024-04-28 00:02:48.625096 ibis_framework-9.0.0.dev686/ibis/expr/datatypes/value.py
--rw-r--r--   0        0        0    12338 2024-04-28 00:02:48.625096 ibis_framework-9.0.0.dev686/ibis/expr/decompile.py
--rw-r--r--   0        0        0    11638 2024-04-28 00:02:48.625096 ibis_framework-9.0.0.dev686/ibis/expr/format.py
--rw-r--r--   0        0        0     1247 2024-04-28 00:02:48.625096 ibis_framework-9.0.0.dev686/ibis/expr/operations/__init__.py
--rw-r--r--   0        0        0     1610 2024-04-28 00:02:48.625096 ibis_framework-9.0.0.dev686/ibis/expr/operations/analytic.py
--rw-r--r--   0        0        0     4265 2024-04-28 00:02:48.625096 ibis_framework-9.0.0.dev686/ibis/expr/operations/arrays.py
--rw-r--r--   0        0        0     4662 2024-04-28 00:02:48.625096 ibis_framework-9.0.0.dev686/ibis/expr/operations/core.py
--rw-r--r--   0        0        0     6164 2024-04-28 00:02:48.625096 ibis_framework-9.0.0.dev686/ibis/expr/operations/generic.py
--rw-r--r--   0        0        0    10593 2024-04-28 00:02:48.625096 ibis_framework-9.0.0.dev686/ibis/expr/operations/geospatial.py
--rw-r--r--   0        0        0     1427 2024-04-28 00:02:48.625096 ibis_framework-9.0.0.dev686/ibis/expr/operations/histograms.py
--rw-r--r--   0        0        0     1069 2024-04-28 00:02:48.625096 ibis_framework-9.0.0.dev686/ibis/expr/operations/json.py
--rw-r--r--   0        0        0     3220 2024-04-28 00:02:48.625096 ibis_framework-9.0.0.dev686/ibis/expr/operations/logical.py
--rw-r--r--   0        0        0     1475 2024-04-28 00:02:48.625096 ibis_framework-9.0.0.dev686/ibis/expr/operations/maps.py
--rw-r--r--   0        0        0     5933 2024-04-28 00:02:48.625096 ibis_framework-9.0.0.dev686/ibis/expr/operations/numeric.py
--rw-r--r--   0        0        0     7237 2024-04-28 00:02:48.625096 ibis_framework-9.0.0.dev686/ibis/expr/operations/reductions.py
--rw-r--r--   0        0        0    10512 2024-04-28 00:02:48.625096 ibis_framework-9.0.0.dev686/ibis/expr/operations/relations.py
--rw-r--r--   0        0        0     1270 2024-04-28 00:02:48.625096 ibis_framework-9.0.0.dev686/ibis/expr/operations/sortkeys.py
--rw-r--r--   0        0        0     5561 2024-04-28 00:02:48.625096 ibis_framework-9.0.0.dev686/ibis/expr/operations/strings.py
--rw-r--r--   0        0        0     1229 2024-04-28 00:02:48.625096 ibis_framework-9.0.0.dev686/ibis/expr/operations/structs.py
--rw-r--r--   0        0        0     1832 2024-04-28 00:02:48.625096 ibis_framework-9.0.0.dev686/ibis/expr/operations/subqueries.py
--rw-r--r--   0        0        0     6809 2024-04-28 00:02:48.625096 ibis_framework-9.0.0.dev686/ibis/expr/operations/temporal.py
--rw-r--r--   0        0        0     1997 2024-04-28 00:02:48.625096 ibis_framework-9.0.0.dev686/ibis/expr/operations/temporal_windows.py
--rw-r--r--   0        0        0        0 2024-04-28 00:02:48.625096 ibis_framework-9.0.0.dev686/ibis/expr/operations/tests/__init__.py
--rw-r--r--   0        0        0      158 2024-04-28 00:02:48.625096 ibis_framework-9.0.0.dev686/ibis/expr/operations/tests/conftest.py
--rw-r--r--   0        0        0      166 2024-04-28 00:02:48.629096 ibis_framework-9.0.0.dev686/ibis/expr/operations/tests/snapshots/test_generic/test_error_message_when_constructing_literal/call0-missing_a_required_argument/missing_a_required_argument.txt
--rw-r--r--   0        0        0      187 2024-04-28 00:02:48.629096 ibis_framework-9.0.0.dev686/ibis/expr/operations/tests/snapshots/test_generic/test_error_message_when_constructing_literal/call1-too_many_positional_arguments/too_many_positional_arguments.txt
--rw-r--r--   0        0        0      204 2024-04-28 00:02:48.629096 ibis_framework-9.0.0.dev686/ibis/expr/operations/tests/snapshots/test_generic/test_error_message_when_constructing_literal/call2-got_an_unexpected_keyword/got_an_unexpected_keyword.txt
--rw-r--r--   0        0        0      215 2024-04-28 00:02:48.629096 ibis_framework-9.0.0.dev686/ibis/expr/operations/tests/snapshots/test_generic/test_error_message_when_constructing_literal/call3-multiple_values_for_argument/multiple_values_for_argument.txt
--rw-r--r--   0        0        0      216 2024-04-28 00:02:48.629096 ibis_framework-9.0.0.dev686/ibis/expr/operations/tests/snapshots/test_generic/test_error_message_when_constructing_literal/call4-invalid_dtype/invalid_dtype.txt
--rw-r--r--   0        0        0       46 2024-04-28 00:02:48.629096 ibis_framework-9.0.0.dev686/ibis/expr/operations/tests/snapshots/test_generic/test_error_message_when_constructing_literal/call5-unable_to_normalize/unable_to_normalize.txt
--rw-r--r--   0        0        0     5645 2024-04-28 00:02:48.629096 ibis_framework-9.0.0.dev686/ibis/expr/operations/tests/test_core.py
--rw-r--r--   0        0        0      789 2024-04-28 00:02:48.629096 ibis_framework-9.0.0.dev686/ibis/expr/operations/tests/test_core_py310.py
--rw-r--r--   0        0        0     5221 2024-04-28 00:02:48.629096 ibis_framework-9.0.0.dev686/ibis/expr/operations/tests/test_generic.py
--rw-r--r--   0        0        0     3058 2024-04-28 00:02:48.629096 ibis_framework-9.0.0.dev686/ibis/expr/operations/tests/test_rewrites.py
--rw-r--r--   0        0        0      432 2024-04-28 00:02:48.629096 ibis_framework-9.0.0.dev686/ibis/expr/operations/tests/test_sortkeys.py
--rw-r--r--   0        0        0      909 2024-04-28 00:02:48.629096 ibis_framework-9.0.0.dev686/ibis/expr/operations/tests/test_structs.py
--rw-r--r--   0        0        0    16317 2024-04-28 00:02:48.629096 ibis_framework-9.0.0.dev686/ibis/expr/operations/udf.py
--rw-r--r--   0        0        0     1138 2024-04-28 00:02:48.629096 ibis_framework-9.0.0.dev686/ibis/expr/operations/vectorized.py
--rw-r--r--   0        0        0     3551 2024-04-28 00:02:48.629096 ibis_framework-9.0.0.dev686/ibis/expr/operations/window.py
--rw-r--r--   0        0        0    14715 2024-04-28 00:02:48.629096 ibis_framework-9.0.0.dev686/ibis/expr/rewrites.py
--rw-r--r--   0        0        0     4393 2024-04-28 00:02:48.629096 ibis_framework-9.0.0.dev686/ibis/expr/rules.py
--rw-r--r--   0        0        0     7677 2024-04-28 00:02:48.629096 ibis_framework-9.0.0.dev686/ibis/expr/schema.py
--rw-r--r--   0        0        0    10442 2024-04-28 00:02:48.629096 ibis_framework-9.0.0.dev686/ibis/expr/sql.py
--rw-r--r--   0        0        0        0 2024-04-28 00:02:48.629096 ibis_framework-9.0.0.dev686/ibis/expr/tests/__init__.py
--rw-r--r--   0        0        0    11451 2024-04-28 00:02:48.629096 ibis_framework-9.0.0.dev686/ibis/expr/tests/conftest.py
--rw-r--r--   0        0        0      254 2024-04-28 00:02:48.629096 ibis_framework-9.0.0.dev686/ibis/expr/tests/snapshots/test_format/test_aggregate_arg_names/repr.txt
--rw-r--r--   0        0        0       89 2024-04-28 00:02:48.629096 ibis_framework-9.0.0.dev686/ibis/expr/tests/snapshots/test_format/test_arbitrary_traversables_are_supported/repr.txt
--rw-r--r--   0        0        0      116 2024-04-28 00:02:48.629096 ibis_framework-9.0.0.dev686/ibis/expr/tests/snapshots/test_format/test_argument_repr_shows_name/repr.txt
--rw-r--r--   0        0        0      421 2024-04-28 00:02:48.629096 ibis_framework-9.0.0.dev686/ibis/expr/tests/snapshots/test_format/test_asof_join/repr.txt
--rw-r--r--   0        0        0      206 2024-04-28 00:02:48.629096 ibis_framework-9.0.0.dev686/ibis/expr/tests/snapshots/test_format/test_complex_repr/repr.txt
--rw-r--r--   0        0        0       87 2024-04-28 00:02:48.629096 ibis_framework-9.0.0.dev686/ibis/expr/tests/snapshots/test_format/test_default_format_implementation/repr.txt
--rw-r--r--   0        0        0      394 2024-04-28 00:02:48.629096 ibis_framework-9.0.0.dev686/ibis/expr/tests/snapshots/test_format/test_destruct_selection/repr.txt
--rw-r--r--   0        0        0       79 2024-04-28 00:02:48.629096 ibis_framework-9.0.0.dev686/ibis/expr/tests/snapshots/test_format/test_fillna/fillna_dict_repr.txt
--rw-r--r--   0        0        0      105 2024-04-28 00:02:48.629096 ibis_framework-9.0.0.dev686/ibis/expr/tests/snapshots/test_format/test_fillna/fillna_int_repr.txt
--rw-r--r--   0        0        0      109 2024-04-28 00:02:48.629096 ibis_framework-9.0.0.dev686/ibis/expr/tests/snapshots/test_format/test_fillna/fillna_str_repr.txt
--rw-r--r--   0        0        0       28 2024-04-28 00:02:48.629096 ibis_framework-9.0.0.dev686/ibis/expr/tests/snapshots/test_format/test_format_dummy_table/repr.txt
--rw-r--r--   0        0        0      153 2024-04-28 00:02:48.629096 ibis_framework-9.0.0.dev686/ibis/expr/tests/snapshots/test_format/test_format_in_memory_table/repr.txt
--rw-r--r--   0        0        0      478 2024-04-28 00:02:48.629096 ibis_framework-9.0.0.dev686/ibis/expr/tests/snapshots/test_format/test_format_multiple_join_with_projection/repr.txt
--rw-r--r--   0        0        0      479 2024-04-28 00:02:48.629096 ibis_framework-9.0.0.dev686/ibis/expr/tests/snapshots/test_format/test_format_new_relational_operation/repr.txt
--rw-r--r--   0        0        0      204 2024-04-28 00:02:48.629096 ibis_framework-9.0.0.dev686/ibis/expr/tests/snapshots/test_format/test_format_projection/repr.txt
--rw-r--r--   0        0        0      435 2024-04-28 00:02:48.629096 ibis_framework-9.0.0.dev686/ibis/expr/tests/snapshots/test_format/test_format_show_variables/repr.txt
--rw-r--r--   0        0        0      155 2024-04-28 00:02:48.629096 ibis_framework-9.0.0.dev686/ibis/expr/tests/snapshots/test_format/test_format_table_column/repr.txt
--rw-r--r--   0        0        0       32 2024-04-28 00:02:48.629096 ibis_framework-9.0.0.dev686/ibis/expr/tests/snapshots/test_format/test_format_table_with_empty_schema/repr.txt
--rw-r--r--   0        0        0       38 2024-04-28 00:02:48.629096 ibis_framework-9.0.0.dev686/ibis/expr/tests/snapshots/test_format/test_format_unbound_table_namespace/repr.txt
--rw-r--r--   0        0        0       47 2024-04-28 00:02:48.629096 ibis_framework-9.0.0.dev686/ibis/expr/tests/snapshots/test_format/test_format_unbound_table_namespace/reprcatdb.txt
--rw-r--r--   0        0        0       43 2024-04-28 00:02:48.629096 ibis_framework-9.0.0.dev686/ibis/expr/tests/snapshots/test_format/test_format_unbound_table_namespace/reprdb.txt
--rw-r--r--   0        0        0      315 2024-04-28 00:02:48.629096 ibis_framework-9.0.0.dev686/ibis/expr/tests/snapshots/test_format/test_memoize_filtered_table/repr.txt
--rw-r--r--   0        0        0      455 2024-04-28 00:02:48.629096 ibis_framework-9.0.0.dev686/ibis/expr/tests/snapshots/test_format/test_memoize_filtered_tables_in_join/repr.txt
--rw-r--r--   0        0        0      172 2024-04-28 00:02:48.629096 ibis_framework-9.0.0.dev686/ibis/expr/tests/snapshots/test_format/test_named_value_expr_show_name/repr.txt
--rw-r--r--   0        0        0      161 2024-04-28 00:02:48.629096 ibis_framework-9.0.0.dev686/ibis/expr/tests/snapshots/test_format/test_named_value_expr_show_name/repr2.txt
--rw-r--r--   0        0        0      153 2024-04-28 00:02:48.629096 ibis_framework-9.0.0.dev686/ibis/expr/tests/snapshots/test_format/test_repr_exact/repr.txt
--rw-r--r--   0        0        0       88 2024-04-28 00:02:48.629096 ibis_framework-9.0.0.dev686/ibis/expr/tests/snapshots/test_format/test_same_column_multiple_aliases/repr.txt
--rw-r--r--   0        0        0       26 2024-04-28 00:02:48.629096 ibis_framework-9.0.0.dev686/ibis/expr/tests/snapshots/test_format/test_schema_truncation/repr1.txt
--rw-r--r--   0        0        0      116 2024-04-28 00:02:48.629096 ibis_framework-9.0.0.dev686/ibis/expr/tests/snapshots/test_format/test_schema_truncation/repr8.txt
--rw-r--r--   0        0        0      235 2024-04-28 00:02:48.629096 ibis_framework-9.0.0.dev686/ibis/expr/tests/snapshots/test_format/test_schema_truncation/repr_all.txt
--rw-r--r--   0        0        0       74 2024-04-28 00:02:48.629096 ibis_framework-9.0.0.dev686/ibis/expr/tests/snapshots/test_format/test_table_count_expr/cnt_repr.txt
--rw-r--r--   0        0        0      242 2024-04-28 00:02:48.629096 ibis_framework-9.0.0.dev686/ibis/expr/tests/snapshots/test_format/test_table_count_expr/join_repr.txt
--rw-r--r--   0        0        0      194 2024-04-28 00:02:48.629096 ibis_framework-9.0.0.dev686/ibis/expr/tests/snapshots/test_format/test_table_count_expr/union_repr.txt
--rw-r--r--   0        0        0      168 2024-04-28 00:02:48.629096 ibis_framework-9.0.0.dev686/ibis/expr/tests/snapshots/test_format/test_table_type_output/repr.txt
--rw-r--r--   0        0        0      514 2024-04-28 00:02:48.629096 ibis_framework-9.0.0.dev686/ibis/expr/tests/snapshots/test_format/test_two_inner_joins/repr.txt
--rw-r--r--   0        0        0      113 2024-04-28 00:02:48.629096 ibis_framework-9.0.0.dev686/ibis/expr/tests/snapshots/test_format/test_window_group_by/repr.txt
--rw-r--r--   0        0        0      143 2024-04-28 00:02:48.629096 ibis_framework-9.0.0.dev686/ibis/expr/tests/snapshots/test_format/test_window_no_group_by/repr.txt
--rw-r--r--   0        0        0     1041 2024-04-28 00:02:48.629096 ibis_framework-9.0.0.dev686/ibis/expr/tests/snapshots/test_sql/test_parse_sql_aggregation_with_multiple_joins/decompiled.py
--rw-r--r--   0        0        0      341 2024-04-28 00:02:48.629096 ibis_framework-9.0.0.dev686/ibis/expr/tests/snapshots/test_sql/test_parse_sql_basic_aggregation/decompiled.py
--rw-r--r--   0        0        0      726 2024-04-28 00:02:48.629096 ibis_framework-9.0.0.dev686/ibis/expr/tests/snapshots/test_sql/test_parse_sql_basic_aggregation_with_join/decompiled.py
--rw-r--r--   0        0        0      885 2024-04-28 00:02:48.633096 ibis_framework-9.0.0.dev686/ibis/expr/tests/snapshots/test_sql/test_parse_sql_basic_join/inner/decompiled.py
--rw-r--r--   0        0        0      884 2024-04-28 00:02:48.633096 ibis_framework-9.0.0.dev686/ibis/expr/tests/snapshots/test_sql/test_parse_sql_basic_join/left/decompiled.py
--rw-r--r--   0        0        0      885 2024-04-28 00:02:48.633096 ibis_framework-9.0.0.dev686/ibis/expr/tests/snapshots/test_sql/test_parse_sql_basic_join/right/decompiled.py
--rw-r--r--   0        0        0      280 2024-04-28 00:02:48.633096 ibis_framework-9.0.0.dev686/ibis/expr/tests/snapshots/test_sql/test_parse_sql_basic_projection/decompiled.py
--rw-r--r--   0        0        0      423 2024-04-28 00:02:48.633096 ibis_framework-9.0.0.dev686/ibis/expr/tests/snapshots/test_sql/test_parse_sql_in_clause/decompiled.py
--rw-r--r--   0        0        0      884 2024-04-28 00:02:48.633096 ibis_framework-9.0.0.dev686/ibis/expr/tests/snapshots/test_sql/test_parse_sql_join_with_filter/decompiled.py
--rw-r--r--   0        0        0      920 2024-04-28 00:02:48.633096 ibis_framework-9.0.0.dev686/ibis/expr/tests/snapshots/test_sql/test_parse_sql_multiple_joins/decompiled.py
--rw-r--r--   0        0        0      531 2024-04-28 00:02:48.633096 ibis_framework-9.0.0.dev686/ibis/expr/tests/snapshots/test_sql/test_parse_sql_scalar_subquery/decompiled.py
--rw-r--r--   0        0        0      309 2024-04-28 00:02:48.633096 ibis_framework-9.0.0.dev686/ibis/expr/tests/snapshots/test_sql/test_parse_sql_simple_reduction/decompiled.py
--rw-r--r--   0        0        0      210 2024-04-28 00:02:48.633096 ibis_framework-9.0.0.dev686/ibis/expr/tests/snapshots/test_sql/test_parse_sql_simple_select_count/decompiled.py
--rw-r--r--   0        0        0      133 2024-04-28 00:02:48.633096 ibis_framework-9.0.0.dev686/ibis/expr/tests/snapshots/test_sql/test_parse_sql_table_alias/decompiled.py
--rw-r--r--   0        0        0     5172 2024-04-28 00:02:48.633096 ibis_framework-9.0.0.dev686/ibis/expr/tests/test_api.py
--rw-r--r--   0        0        0     2006 2024-04-28 00:02:48.633096 ibis_framework-9.0.0.dev686/ibis/expr/tests/test_datashape.py
--rw-r--r--   0        0        0     2288 2024-04-28 00:02:48.633096 ibis_framework-9.0.0.dev686/ibis/expr/tests/test_decompile.py
--rw-r--r--   0        0        0     1026 2024-04-28 00:02:48.633096 ibis_framework-9.0.0.dev686/ibis/expr/tests/test_dereference.py
--rw-r--r--   0        0        0    13756 2024-04-28 00:02:48.633096 ibis_framework-9.0.0.dev686/ibis/expr/tests/test_format.py
--rw-r--r--   0        0        0    51752 2024-04-28 00:02:48.633096 ibis_framework-9.0.0.dev686/ibis/expr/tests/test_newrels.py
--rw-r--r--   0        0        0     3216 2024-04-28 00:02:48.633096 ibis_framework-9.0.0.dev686/ibis/expr/tests/test_reductions.py
--rw-r--r--   0        0        0     2721 2024-04-28 00:02:48.633096 ibis_framework-9.0.0.dev686/ibis/expr/tests/test_rewrites.py
--rw-r--r--   0        0        0    12444 2024-04-28 00:02:48.633096 ibis_framework-9.0.0.dev686/ibis/expr/tests/test_schema.py
--rw-r--r--   0        0        0     4095 2024-04-28 00:02:48.633096 ibis_framework-9.0.0.dev686/ibis/expr/tests/test_sql.py
--rw-r--r--   0        0        0     4626 2024-04-28 00:02:48.633096 ibis_framework-9.0.0.dev686/ibis/expr/tests/test_visualize.py
--rw-r--r--   0        0        0      931 2024-04-28 00:02:48.633096 ibis_framework-9.0.0.dev686/ibis/expr/types/__init__.py
--rw-r--r--   0        0        0    50318 2024-04-28 00:02:48.633096 ibis_framework-9.0.0.dev686/ibis/expr/types/arrays.py
--rw-r--r--   0        0        0      819 2024-04-28 00:02:48.633096 ibis_framework-9.0.0.dev686/ibis/expr/types/binary.py
--rw-r--r--   0        0        0      271 2024-04-28 00:02:48.633096 ibis_framework-9.0.0.dev686/ibis/expr/types/collections.py
--rw-r--r--   0        0        0    24086 2024-04-28 00:02:48.633096 ibis_framework-9.0.0.dev686/ibis/expr/types/core.py
--rw-r--r--   0        0        0     5402 2024-04-28 00:02:48.633096 ibis_framework-9.0.0.dev686/ibis/expr/types/dataframe_interchange.py
--rw-r--r--   0        0        0    79026 2024-04-28 00:02:48.633096 ibis_framework-9.0.0.dev686/ibis/expr/types/generic.py
--rw-r--r--   0        0        0    66172 2024-04-28 00:02:48.633096 ibis_framework-9.0.0.dev686/ibis/expr/types/geospatial.py
--rw-r--r--   0        0        0    11409 2024-04-28 00:02:48.633096 ibis_framework-9.0.0.dev686/ibis/expr/types/groupby.py
--rw-r--r--   0        0        0      446 2024-04-28 00:02:48.633096 ibis_framework-9.0.0.dev686/ibis/expr/types/inet.py
--rw-r--r--   0        0        0    16698 2024-04-28 00:02:48.633096 ibis_framework-9.0.0.dev686/ibis/expr/types/joins.py
--rw-r--r--   0        0        0    29342 2024-04-28 00:02:48.633096 ibis_framework-9.0.0.dev686/ibis/expr/types/json.py
--rw-r--r--   0        0        0    20797 2024-04-28 00:02:48.633096 ibis_framework-9.0.0.dev686/ibis/expr/types/logical.py
--rw-r--r--   0        0        0    17301 2024-04-28 00:02:48.637096 ibis_framework-9.0.0.dev686/ibis/expr/types/maps.py
--rw-r--r--   0        0        0    37238 2024-04-28 00:02:48.637096 ibis_framework-9.0.0.dev686/ibis/expr/types/numeric.py
--rw-r--r--   0        0        0    15300 2024-04-28 00:02:48.637096 ibis_framework-9.0.0.dev686/ibis/expr/types/pretty.py
--rw-r--r--   0        0        0   249387 2024-04-28 00:02:48.637096 ibis_framework-9.0.0.dev686/ibis/expr/types/relations.py
--rw-r--r--   0        0        0    58727 2024-04-28 00:02:48.637096 ibis_framework-9.0.0.dev686/ibis/expr/types/strings.py
--rw-r--r--   0        0        0    13759 2024-04-28 00:02:48.637096 ibis_framework-9.0.0.dev686/ibis/expr/types/structs.py
--rw-r--r--   0        0        0    30851 2024-04-28 00:02:48.637096 ibis_framework-9.0.0.dev686/ibis/expr/types/temporal.py
--rw-r--r--   0        0        0     4743 2024-04-28 00:02:48.637096 ibis_framework-9.0.0.dev686/ibis/expr/types/temporal_windows.py
--rw-r--r--   0        0        0      173 2024-04-28 00:02:48.637096 ibis_framework-9.0.0.dev686/ibis/expr/types/typing.py
--rw-r--r--   0        0        0      276 2024-04-28 00:02:48.637096 ibis_framework-9.0.0.dev686/ibis/expr/types/uuid.py
--rw-r--r--   0        0        0     7373 2024-04-28 00:02:48.637096 ibis_framework-9.0.0.dev686/ibis/expr/visualize.py
--rw-r--r--   0        0        0     6772 2024-04-28 00:02:48.637096 ibis_framework-9.0.0.dev686/ibis/formats/__init__.py
--rw-r--r--   0        0        0     3551 2024-04-28 00:02:48.637096 ibis_framework-9.0.0.dev686/ibis/formats/numpy.py
--rw-r--r--   0        0        0    13702 2024-04-28 00:02:48.637096 ibis_framework-9.0.0.dev686/ibis/formats/pandas.py
--rw-r--r--   0        0        0     5896 2024-04-28 00:02:48.637096 ibis_framework-9.0.0.dev686/ibis/formats/polars.py
--rw-r--r--   0        0        0     9661 2024-04-28 00:02:48.637096 ibis_framework-9.0.0.dev686/ibis/formats/pyarrow.py
--rw-r--r--   0        0        0        0 2024-04-28 00:02:48.637096 ibis_framework-9.0.0.dev686/ibis/formats/tests/__init__.py
--rw-r--r--   0        0        0     4370 2024-04-28 00:02:48.637096 ibis_framework-9.0.0.dev686/ibis/formats/tests/test_numpy.py
--rw-r--r--   0        0        0    14006 2024-04-28 00:02:48.637096 ibis_framework-9.0.0.dev686/ibis/formats/tests/test_pandas.py
--rw-r--r--   0        0        0     4787 2024-04-28 00:02:48.637096 ibis_framework-9.0.0.dev686/ibis/formats/tests/test_polars.py
--rw-r--r--   0        0        0     6829 2024-04-28 00:02:48.637096 ibis_framework-9.0.0.dev686/ibis/formats/tests/test_pyarrow.py
--rw-r--r--   0        0        0      235 2024-04-28 00:02:48.637096 ibis_framework-9.0.0.dev686/ibis/interactive.py
--rw-r--r--   0        0        0        0 2024-04-28 00:02:48.637096 ibis_framework-9.0.0.dev686/ibis/legacy/__init__.py
--rw-r--r--   0        0        0        0 2024-04-28 00:02:48.637096 ibis_framework-9.0.0.dev686/ibis/legacy/udf/__init__.py
--rw-r--r--   0        0        0     2149 2024-04-28 00:02:48.637096 ibis_framework-9.0.0.dev686/ibis/legacy/udf/validate.py
--rw-r--r--   0        0        0     9457 2024-04-28 00:02:48.637096 ibis_framework-9.0.0.dev686/ibis/legacy/udf/vectorized.py
--rw-r--r--   0        0        0    24142 2024-04-28 00:02:48.637096 ibis_framework-9.0.0.dev686/ibis/selectors.py
--rw-r--r--   0        0        0     2236 2024-04-28 00:02:48.637096 ibis_framework-9.0.0.dev686/ibis/streamlit/__init__.py
--rw-r--r--   0        0        0      609 2024-04-28 00:02:48.637096 ibis_framework-9.0.0.dev686/ibis/tests/__init__.py
--rw-r--r--   0        0        0        0 2024-04-28 00:02:48.637096 ibis_framework-9.0.0.dev686/ibis/tests/benchmarks/__init__.py
--rw-r--r--   0        0        0    14944 2024-04-28 00:02:48.637096 ibis_framework-9.0.0.dev686/ibis/tests/benchmarks/benchfuncs.py
--rw-r--r--   0        0        0    21459 2024-04-28 00:02:48.637096 ibis_framework-9.0.0.dev686/ibis/tests/benchmarks/test_benchmarks.py
--rw-r--r--   0        0        0        0 2024-04-28 00:02:48.637096 ibis_framework-9.0.0.dev686/ibis/tests/conftest.py
--rw-r--r--   0        0        0        0 2024-04-28 00:02:48.637096 ibis_framework-9.0.0.dev686/ibis/tests/expr/__init__.py
--rw-r--r--   0        0        0     1729 2024-04-28 00:02:48.641096 ibis_framework-9.0.0.dev686/ibis/tests/expr/conftest.py
--rw-r--r--   0        0        0     2960 2024-04-28 00:02:48.641096 ibis_framework-9.0.0.dev686/ibis/tests/expr/mocks.py
--rw-r--r--   0        0        0     1984 2024-04-28 00:02:48.641096 ibis_framework-9.0.0.dev686/ibis/tests/expr/snapshots/test_format_sql_operations/test_format_sql_query_result/repr.txt
--rw-r--r--   0        0        0      421 2024-04-28 00:02:48.641096 ibis_framework-9.0.0.dev686/ibis/tests/expr/snapshots/test_format_sql_operations/test_memoize_database_table/repr.txt
--rw-r--r--   0        0        0     1234 2024-04-28 00:02:48.641096 ibis_framework-9.0.0.dev686/ibis/tests/expr/snapshots/test_format_sql_operations/test_memoize_insert_sort_key/repr.txt
--rw-r--r--   0        0        0       66 2024-04-28 00:02:48.641096 ibis_framework-9.0.0.dev686/ibis/tests/expr/snapshots/test_interactive/test_default_limit/out.sql
--rw-r--r--   0        0        0       66 2024-04-28 00:02:48.641096 ibis_framework-9.0.0.dev686/ibis/tests/expr/snapshots/test_interactive/test_disable_query_limit/out.sql
--rw-r--r--   0        0        0      105 2024-04-28 00:02:48.641096 ibis_framework-9.0.0.dev686/ibis/tests/expr/snapshots/test_interactive/test_respect_set_limit/out.sql
--rw-r--r--   0        0        0     8872 2024-04-28 00:02:48.641096 ibis_framework-9.0.0.dev686/ibis/tests/expr/test_analysis.py
--rw-r--r--   0        0        0     2844 2024-04-28 00:02:48.641096 ibis_framework-9.0.0.dev686/ibis/tests/expr/test_analytics.py
--rw-r--r--   0        0        0     4470 2024-04-28 00:02:48.641096 ibis_framework-9.0.0.dev686/ibis/tests/expr/test_case.py
--rw-r--r--   0        0        0     4125 2024-04-28 00:02:48.641096 ibis_framework-9.0.0.dev686/ibis/tests/expr/test_decimal.py
--rw-r--r--   0        0        0     1643 2024-04-28 00:02:48.641096 ibis_framework-9.0.0.dev686/ibis/tests/expr/test_format_sql_operations.py
--rw-r--r--   0        0        0      769 2024-04-28 00:02:48.641096 ibis_framework-9.0.0.dev686/ibis/tests/expr/test_geospatial.py
--rw-r--r--   0        0        0     5008 2024-04-28 00:02:48.641096 ibis_framework-9.0.0.dev686/ibis/tests/expr/test_literal.py
--rw-r--r--   0        0        0     1599 2024-04-28 00:02:48.641096 ibis_framework-9.0.0.dev686/ibis/tests/expr/test_pipe.py
--rw-r--r--   0        0        0     6127 2024-04-28 00:02:48.641096 ibis_framework-9.0.0.dev686/ibis/tests/expr/test_pretty_repr.py
--rw-r--r--   0        0        0     3250 2024-04-28 00:02:48.641096 ibis_framework-9.0.0.dev686/ibis/tests/expr/test_relocate.py
--rw-r--r--   0        0        0    14256 2024-04-28 00:02:48.641096 ibis_framework-9.0.0.dev686/ibis/tests/expr/test_selectors.py
--rw-r--r--   0        0        0     1243 2024-04-28 00:02:48.641096 ibis_framework-9.0.0.dev686/ibis/tests/expr/test_set_operations.py
--rw-r--r--   0        0        0     6326 2024-04-28 00:02:48.641096 ibis_framework-9.0.0.dev686/ibis/tests/expr/test_sql_builtins.py
--rw-r--r--   0        0        0     3191 2024-04-28 00:02:48.641096 ibis_framework-9.0.0.dev686/ibis/tests/expr/test_string.py
--rw-r--r--   0        0        0     2640 2024-04-28 00:02:48.641096 ibis_framework-9.0.0.dev686/ibis/tests/expr/test_struct.py
--rw-r--r--   0        0        0    60621 2024-04-28 00:02:48.641096 ibis_framework-9.0.0.dev686/ibis/tests/expr/test_table.py
--rw-r--r--   0        0        0    26925 2024-04-28 00:02:48.641096 ibis_framework-9.0.0.dev686/ibis/tests/expr/test_temporal.py
--rw-r--r--   0        0        0     2703 2024-04-28 00:02:48.641096 ibis_framework-9.0.0.dev686/ibis/tests/expr/test_temporal_windows.py
--rw-r--r--   0        0        0     5714 2024-04-28 00:02:48.641096 ibis_framework-9.0.0.dev686/ibis/tests/expr/test_timestamp.py
--rw-r--r--   0        0        0     4482 2024-04-28 00:02:48.641096 ibis_framework-9.0.0.dev686/ibis/tests/expr/test_udf.py
--rw-r--r--   0        0        0      181 2024-04-28 00:02:48.641096 ibis_framework-9.0.0.dev686/ibis/tests/expr/test_uuid.py
--rw-r--r--   0        0        0    49993 2024-04-28 00:02:48.641096 ibis_framework-9.0.0.dev686/ibis/tests/expr/test_value_exprs.py
--rw-r--r--   0        0        0    17623 2024-04-28 00:02:48.641096 ibis_framework-9.0.0.dev686/ibis/tests/expr/test_window_frames.py
--rw-r--r--   0        0        0     2008 2024-04-28 00:02:48.641096 ibis_framework-9.0.0.dev686/ibis/tests/expr/test_window_functions.py
--rw-r--r--   0        0        0     7581 2024-04-28 00:02:48.641096 ibis_framework-9.0.0.dev686/ibis/tests/strategies.py
--rw-r--r--   0        0        0     2029 2024-04-28 00:02:48.641096 ibis_framework-9.0.0.dev686/ibis/tests/test_api.py
--rw-r--r--   0        0        0      401 2024-04-28 00:02:48.641096 ibis_framework-9.0.0.dev686/ibis/tests/test_config.py
--rw-r--r--   0        0        0     5025 2024-04-28 00:02:48.641096 ibis_framework-9.0.0.dev686/ibis/tests/test_strategies.py
--rw-r--r--   0        0        0     3741 2024-04-28 00:02:48.641096 ibis_framework-9.0.0.dev686/ibis/tests/test_util.py
--rw-r--r--   0        0        0      226 2024-04-28 00:02:48.641096 ibis_framework-9.0.0.dev686/ibis/tests/test_version.py
--rw-r--r--   0        0        0     1947 2024-04-28 00:02:48.641096 ibis_framework-9.0.0.dev686/ibis/tests/util.py
--rw-r--r--   0        0        0    16151 2024-04-28 00:02:48.641096 ibis_framework-9.0.0.dev686/ibis/util.py
--rw-r--r--   0        0        0    18738 2024-04-28 00:03:03.649022 ibis_framework-9.0.0.dev686/pyproject.toml
--rw-r--r--   0        0        0    15518 1970-01-01 00:00:00.000000 ibis_framework-9.0.0.dev686/PKG-INFO
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/docs/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      925 2015-07-24 17:45:25.000000 ibis-framework-v0.6.0/docs/build-notebooks.py
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/docs/sphinxext/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)        0 2015-07-24 17:45:25.000000 ibis-framework-v0.6.0/docs/sphinxext/__init__.py
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/docs/sphinxext/ipython_sphinxext/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)        0 2015-10-07 03:41:44.000000 ibis-framework-v0.6.0/docs/sphinxext/ipython_sphinxext/__init__.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      116 2015-10-07 03:41:44.000000 ibis-framework-v0.6.0/docs/sphinxext/ipython_sphinxext/LICENSE
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    37645 2015-10-07 03:41:44.000000 ibis-framework-v0.6.0/docs/sphinxext/ipython_sphinxext/ipython_directive.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     4181 2015-10-07 03:41:44.000000 ibis-framework-v0.6.0/docs/sphinxext/ipython_sphinxext/ipython_console_highlighting.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)       74 2015-06-10 22:32:58.000000 ibis-framework-v0.6.0/docs/README
+-rw-r--r--   0 wesm      (1000) wesm      (1000)       26 2015-06-10 22:32:58.000000 ibis-framework-v0.6.0/docs/requirements-docs.txt
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/docs/source/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     8595 2015-10-07 03:41:44.000000 ibis-framework-v0.6.0/docs/source/conf.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      236 2015-11-30 09:38:55.000000 ibis-framework-v0.6.0/docs/source/tutorial.rst
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    23073 2015-11-30 09:38:55.000000 ibis-framework-v0.6.0/docs/source/impala.rst
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     1722 2015-07-24 17:45:25.000000 ibis-framework-v0.6.0/docs/source/developer.rst
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     2578 2015-11-30 09:38:55.000000 ibis-framework-v0.6.0/docs/source/index.rst
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     3190 2015-09-02 07:50:13.000000 ibis-framework-v0.6.0/docs/source/configuration.rst
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    10447 2015-11-30 09:40:30.000000 ibis-framework-v0.6.0/docs/source/release.rst
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/docs/source/_templates/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      487 2015-07-24 17:45:25.000000 ibis-framework-v0.6.0/docs/source/_templates/layout.html
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    33258 2015-11-01 14:02:52.000000 ibis-framework-v0.6.0/docs/source/sql.rst
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    11507 2015-07-24 17:45:25.000000 ibis-framework-v0.6.0/docs/source/legal.rst
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      102 2015-07-24 17:45:25.000000 ibis-framework-v0.6.0/docs/source/type-system.rst
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     4044 2015-11-30 09:38:55.000000 ibis-framework-v0.6.0/docs/source/getting-started.rst
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     8770 2015-11-30 09:38:55.000000 ibis-framework-v0.6.0/docs/source/api.rst
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     6464 2015-06-10 22:32:58.000000 ibis-framework-v0.6.0/docs/make.bat
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     7377 2015-07-24 17:45:25.000000 ibis-framework-v0.6.0/docs/Makefile
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     1750 2015-11-05 21:11:43.000000 ibis-framework-v0.6.0/README.md
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/ibis_framework.egg-info/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)        5 2015-11-30 09:48:45.000000 ibis-framework-v0.6.0/ibis_framework.egg-info/top_level.txt
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      841 2015-11-30 09:48:45.000000 ibis-framework-v0.6.0/ibis_framework.egg-info/PKG-INFO
+-rw-r--r--   0 wesm      (1000) wesm      (1000)        1 2015-11-30 09:48:45.000000 ibis-framework-v0.6.0/ibis_framework.egg-info/dependency_links.txt
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     4409 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/ibis_framework.egg-info/SOURCES.txt
+-rw-r--r--   0 wesm      (1000) wesm      (1000)       46 2015-11-30 09:48:45.000000 ibis-framework-v0.6.0/ibis_framework.egg-info/pbr.json
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      119 2015-11-30 09:48:45.000000 ibis-framework-v0.6.0/ibis_framework.egg-info/requires.txt
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    62474 2015-10-08 04:18:58.000000 ibis-framework-v0.6.0/versioneer.py
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/ibis/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     1383 2015-08-25 05:04:44.000000 ibis-framework-v0.6.0/ibis/config_init.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     3771 2015-10-08 04:18:58.000000 ibis-framework-v0.6.0/ibis/__init__.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     2416 2015-05-11 04:48:02.000000 ibis-framework-v0.6.0/ibis/wire.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     7815 2015-08-25 05:04:44.000000 ibis-framework-v0.6.0/ibis/tasks.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    32671 2015-01-21 05:11:59.000000 ibis-framework-v0.6.0/ibis/comms.pyx
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    31387 2015-08-24 21:56:32.000000 ibis-framework-v0.6.0/ibis/cloudpickle.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    20779 2015-07-13 05:54:08.000000 ibis-framework-v0.6.0/ibis/config.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    13243 2015-11-01 14:02:52.000000 ibis-framework-v0.6.0/ibis/client.py
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/ibis/spark/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)        0 2015-08-14 15:53:00.000000 ibis-framework-v0.6.0/ibis/spark/__init__.py
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/ibis/spark/tests/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)        0 2015-08-14 15:53:00.000000 ibis-framework-v0.6.0/ibis/spark/tests/__init__.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     1619 2015-11-01 14:03:25.000000 ibis-framework-v0.6.0/ibis/compat.py
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/ibis/src/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     2239 2015-01-24 22:13:49.000000 ibis-framework-v0.6.0/ibis/src/ipc_support.c
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     1445 2015-01-21 05:11:59.000000 ibis-framework-v0.6.0/ibis/src/ipc_support.h
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/ibis/hive/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)        0 2015-08-14 15:53:00.000000 ibis-framework-v0.6.0/ibis/hive/__init__.py
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/ibis/hive/tests/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)        0 2015-08-14 15:53:00.000000 ibis-framework-v0.6.0/ibis/hive/tests/__init__.py
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/ibis/sql/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)        0 2015-05-11 04:48:04.000000 ibis-framework-v0.6.0/ibis/sql/__init__.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    23190 2015-11-01 14:02:52.000000 ibis-framework-v0.6.0/ibis/sql/alchemy.py
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/ibis/sql/redshift/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)        0 2015-08-14 15:53:00.000000 ibis-framework-v0.6.0/ibis/sql/redshift/__init__.py
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/ibis/sql/redshift/tests/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)        0 2015-08-14 15:53:00.000000 ibis-framework-v0.6.0/ibis/sql/redshift/tests/__init__.py
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/ibis/sql/sqlite/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)        0 2015-08-14 15:53:00.000000 ibis-framework-v0.6.0/ibis/sql/sqlite/__init__.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     2768 2015-10-08 04:27:31.000000 ibis-framework-v0.6.0/ibis/sql/sqlite/client.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     1400 2015-10-08 04:27:31.000000 ibis-framework-v0.6.0/ibis/sql/sqlite/api.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     4806 2015-10-08 03:41:51.000000 ibis-framework-v0.6.0/ibis/sql/sqlite/compiler.py
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/ibis/sql/sqlite/tests/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     9911 2015-11-30 09:38:55.000000 ibis-framework-v0.6.0/ibis/sql/sqlite/tests/test_functions.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)        0 2015-08-14 15:53:00.000000 ibis-framework-v0.6.0/ibis/sql/sqlite/tests/__init__.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      616 2015-09-01 04:13:53.000000 ibis-framework-v0.6.0/ibis/sql/sqlite/tests/conftest.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     1970 2015-09-02 00:57:42.000000 ibis-framework-v0.6.0/ibis/sql/sqlite/tests/common.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     3047 2015-10-08 04:27:31.000000 ibis-framework-v0.6.0/ibis/sql/sqlite/tests/test_client.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     3701 2015-11-01 14:02:52.000000 ibis-framework-v0.6.0/ibis/sql/transforms.py
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/ibis/sql/presto/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)        0 2015-08-14 15:53:00.000000 ibis-framework-v0.6.0/ibis/sql/presto/__init__.py
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/ibis/sql/presto/tests/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)        0 2015-08-14 15:53:00.000000 ibis-framework-v0.6.0/ibis/sql/presto/tests/__init__.py
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/ibis/sql/postgres/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)        0 2015-09-01 04:13:53.000000 ibis-framework-v0.6.0/ibis/sql/postgres/__init__.py
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/ibis/sql/postgres/tests/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)        0 2015-09-01 04:13:53.000000 ibis-framework-v0.6.0/ibis/sql/postgres/tests/__init__.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      616 2015-09-01 04:13:53.000000 ibis-framework-v0.6.0/ibis/sql/postgres/tests/conftest.py
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/ibis/sql/vertica/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)        0 2015-08-14 15:53:00.000000 ibis-framework-v0.6.0/ibis/sql/vertica/__init__.py
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/ibis/sql/vertica/tests/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)        0 2015-08-14 15:53:00.000000 ibis-framework-v0.6.0/ibis/sql/vertica/tests/__init__.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    43932 2015-11-01 14:02:52.000000 ibis-framework-v0.6.0/ibis/sql/compiler.py
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/ibis/sql/tests/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)        0 2015-05-11 04:48:05.000000 ibis-framework-v0.6.0/ibis/sql/tests/__init__.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      616 2015-09-01 04:13:53.000000 ibis-framework-v0.6.0/ibis/sql/tests/conftest.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    59525 2015-11-01 14:02:52.000000 ibis-framework-v0.6.0/ibis/sql/tests/test_compiler.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    18028 2015-09-10 15:51:22.000000 ibis-framework-v0.6.0/ibis/sql/tests/test_sqlalchemy.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     5330 2015-11-22 23:42:20.000000 ibis-framework-v0.6.0/ibis/util.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      472 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/ibis/_version.py
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/ibis/impala/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)        0 2015-08-14 15:53:00.000000 ibis-framework-v0.6.0/ibis/impala/__init__.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     9517 2015-11-01 14:02:52.000000 ibis-framework-v0.6.0/ibis/impala/udf.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     2672 2015-08-14 15:53:00.000000 ibis-framework-v0.6.0/ibis/impala/identifiers.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     8436 2015-11-22 23:42:20.000000 ibis-framework-v0.6.0/ibis/impala/metadata.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    62350 2015-11-30 09:40:30.000000 ibis-framework-v0.6.0/ibis/impala/client.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     3428 2015-08-20 06:26:34.000000 ibis-framework-v0.6.0/ibis/impala/madlib.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      728 2015-08-14 15:53:00.000000 ibis-framework-v0.6.0/ibis/impala/compat.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      649 2015-11-22 23:42:20.000000 ibis-framework-v0.6.0/ibis/impala/parquet.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     3531 2015-11-30 09:38:55.000000 ibis-framework-v0.6.0/ibis/impala/api.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    24037 2015-11-30 09:38:55.000000 ibis-framework-v0.6.0/ibis/impala/ddl.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    37724 2015-11-29 21:58:21.000000 ibis-framework-v0.6.0/ibis/impala/compiler.py
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/ibis/impala/tests/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)        0 2015-08-14 15:53:00.000000 ibis-framework-v0.6.0/ibis/impala/tests/__init__.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      616 2015-08-14 15:53:00.000000 ibis-framework-v0.6.0/ibis/impala/tests/conftest.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     8090 2015-09-03 06:45:25.000000 ibis-framework-v0.6.0/ibis/impala/tests/test_window.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    47622 2015-11-29 21:58:21.000000 ibis-framework-v0.6.0/ibis/impala/tests/test_exprs.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     1100 2015-09-01 04:13:53.000000 ibis-framework-v0.6.0/ibis/impala/tests/test_sql.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    36166 2015-11-30 09:40:30.000000 ibis-framework-v0.6.0/ibis/impala/tests/test_ddl.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     4536 2015-11-22 23:42:20.000000 ibis-framework-v0.6.0/ibis/impala/tests/test_metadata.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    18191 2015-08-27 14:46:57.000000 ibis-framework-v0.6.0/ibis/impala/tests/test_udf.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     1787 2015-09-09 04:43:12.000000 ibis-framework-v0.6.0/ibis/impala/tests/test_madlib.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     6556 2015-11-22 23:43:12.000000 ibis-framework-v0.6.0/ibis/impala/tests/common.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     9610 2015-11-22 23:42:20.000000 ibis-framework-v0.6.0/ibis/impala/tests/test_pandas_interop.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    10737 2015-11-30 09:38:55.000000 ibis-framework-v0.6.0/ibis/impala/tests/test_client.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     8670 2015-11-30 09:43:53.000000 ibis-framework-v0.6.0/ibis/impala/tests/test_partition.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     6233 2015-11-22 23:42:20.000000 ibis-framework-v0.6.0/ibis/impala/pandas_interop.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     9745 2015-08-24 21:56:32.000000 ibis-framework-v0.6.0/ibis/server.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     4129 2015-01-21 05:11:59.000000 ibis-framework-v0.6.0/ibis/comms.pxd
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/ibis/expr/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)        0 2015-05-11 04:48:06.000000 ibis-framework-v0.6.0/ibis/expr/__init__.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     8784 2015-09-16 18:48:19.000000 ibis-framework-v0.6.0/ibis/expr/format.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     7295 2015-06-29 06:28:49.000000 ibis-framework-v0.6.0/ibis/expr/temporal.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    26447 2015-09-16 18:48:19.000000 ibis-framework-v0.6.0/ibis/expr/analysis.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    11262 2015-11-05 19:34:31.000000 ibis-framework-v0.6.0/ibis/expr/datatypes.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    27030 2015-11-01 14:02:52.000000 ibis-framework-v0.6.0/ibis/expr/types.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    51662 2015-11-01 14:02:52.000000 ibis-framework-v0.6.0/ibis/expr/operations.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     7032 2015-09-09 04:43:12.000000 ibis-framework-v0.6.0/ibis/expr/groupby.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     7688 2015-09-03 06:45:25.000000 ibis-framework-v0.6.0/ibis/expr/window.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    49858 2015-11-30 09:38:55.000000 ibis-framework-v0.6.0/ibis/expr/api.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    19387 2015-08-24 21:56:32.000000 ibis-framework-v0.6.0/ibis/expr/rules.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     5516 2015-08-11 20:25:43.000000 ibis-framework-v0.6.0/ibis/expr/analytics.py
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/ibis/expr/tests/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)        0 2015-05-11 04:48:08.000000 ibis-framework-v0.6.0/ibis/expr/tests/__init__.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      616 2015-09-01 04:13:53.000000 ibis-framework-v0.6.0/ibis/expr/tests/conftest.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     2858 2015-09-02 23:21:36.000000 ibis-framework-v0.6.0/ibis/expr/tests/test_interactive.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     3386 2015-08-05 16:09:03.000000 ibis-framework-v0.6.0/ibis/expr/tests/test_case.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     5034 2015-09-03 06:45:25.000000 ibis-framework-v0.6.0/ibis/expr/tests/test_window_functions.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     6266 2015-09-16 18:48:19.000000 ibis-framework-v0.6.0/ibis/expr/tests/test_format.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     9216 2015-09-16 18:48:19.000000 ibis-framework-v0.6.0/ibis/expr/tests/test_analysis.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     3313 2015-06-29 06:28:49.000000 ibis-framework-v0.6.0/ibis/expr/tests/test_decimal.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     6270 2015-10-08 04:09:39.000000 ibis-framework-v0.6.0/ibis/expr/tests/test_sql_builtins.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    20407 2015-11-30 09:38:55.000000 ibis-framework-v0.6.0/ibis/expr/tests/test_value_exprs.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     5955 2015-08-24 21:56:32.000000 ibis-framework-v0.6.0/ibis/expr/tests/test_temporal.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    13673 2015-09-09 04:43:12.000000 ibis-framework-v0.6.0/ibis/expr/tests/mocks.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     3282 2015-10-07 04:53:40.000000 ibis-framework-v0.6.0/ibis/expr/tests/test_string.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     1850 2015-06-29 06:28:49.000000 ibis-framework-v0.6.0/ibis/expr/tests/test_pipe.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     3299 2015-08-11 20:30:38.000000 ibis-framework-v0.6.0/ibis/expr/tests/test_analytics.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     3624 2015-10-08 05:36:22.000000 ibis-framework-v0.6.0/ibis/expr/tests/test_timestamp.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    38376 2015-11-01 14:02:52.000000 ibis-framework-v0.6.0/ibis/expr/tests/test_table.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      997 2015-07-13 05:54:08.000000 ibis-framework-v0.6.0/ibis/common.py
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/ibis/tests/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      573 2015-06-29 06:28:49.000000 ibis-framework-v0.6.0/ibis/tests/__init__.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     2688 2015-11-01 14:02:52.000000 ibis-framework-v0.6.0/ibis/tests/conftest.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    11505 2015-06-29 06:28:49.000000 ibis-framework-v0.6.0/ibis/tests/test_comms.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    10953 2015-08-24 21:56:32.000000 ibis-framework-v0.6.0/ibis/tests/test_tasks.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     6059 2015-11-22 23:42:20.000000 ibis-framework-v0.6.0/ibis/tests/test_server.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      956 2015-08-25 05:04:44.000000 ibis-framework-v0.6.0/ibis/tests/util.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    16773 2015-09-02 07:50:13.000000 ibis-framework-v0.6.0/ibis/tests/test_filesystems.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     9550 2015-08-25 05:04:44.000000 ibis-framework-v0.6.0/ibis/filesystems.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      841 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/PKG-INFO
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/scripts/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      753 2015-01-21 05:11:59.000000 ibis-framework-v0.6.0/scripts/semaphore_perf.py
+-rwxr-xr-x   0 wesm      (1000) wesm      (1000)     4139 2015-10-08 03:41:51.000000 ibis-framework-v0.6.0/scripts/run_jenkins.sh
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     1268 2015-09-10 05:09:43.000000 ibis-framework-v0.6.0/scripts/airline.py
+-rwxr-xr-x   0 wesm      (1000) wesm      (1000)    17250 2015-09-10 05:09:43.000000 ibis-framework-v0.6.0/scripts/test_data_admin.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      212 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/setup.cfg
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/conda-recipes/
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/conda-recipes/hdfs/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      776 2015-09-11 17:09:40.000000 ibis-framework-v0.6.0/conda-recipes/hdfs/meta.yaml
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      219 2015-08-07 16:53:33.000000 ibis-framework-v0.6.0/conda-recipes/hdfs/build.sh
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      236 2015-08-07 16:53:33.000000 ibis-framework-v0.6.0/conda-recipes/hdfs/bld.bat
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/conda-recipes/impyla/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     1117 2015-11-01 14:03:25.000000 ibis-framework-v0.6.0/conda-recipes/impyla/meta.yaml
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      219 2015-08-07 16:53:33.000000 ibis-framework-v0.6.0/conda-recipes/impyla/build.sh
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      236 2015-08-07 16:53:33.000000 ibis-framework-v0.6.0/conda-recipes/impyla/bld.bat
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/conda-recipes/ibis-framework/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      971 2015-11-10 22:41:53.000000 ibis-framework-v0.6.0/conda-recipes/ibis-framework/meta.yaml
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      294 2015-11-01 14:03:25.000000 ibis-framework-v0.6.0/conda-recipes/ibis-framework/build.sh
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      314 2015-11-01 14:03:25.000000 ibis-framework-v0.6.0/conda-recipes/ibis-framework/bld.bat
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/conda-recipes/thrift/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      498 2015-08-07 16:53:33.000000 ibis-framework-v0.6.0/conda-recipes/thrift/meta.yaml
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      219 2015-08-07 16:53:33.000000 ibis-framework-v0.6.0/conda-recipes/thrift/build.sh
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      236 2015-08-07 16:53:33.000000 ibis-framework-v0.6.0/conda-recipes/thrift/bld.bat
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/conda-recipes/thrift_sasl/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      554 2015-09-11 17:09:40.000000 ibis-framework-v0.6.0/conda-recipes/thrift_sasl/meta.yaml
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      219 2015-09-11 17:09:40.000000 ibis-framework-v0.6.0/conda-recipes/thrift_sasl/build.sh
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      236 2015-09-11 17:09:40.000000 ibis-framework-v0.6.0/conda-recipes/thrift_sasl/bld.bat
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      561 2015-10-08 04:18:58.000000 ibis-framework-v0.6.0/MANIFEST.in
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     4200 2015-11-01 14:02:52.000000 ibis-framework-v0.6.0/setup.py
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/LICENSES/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     1061 2015-06-11 06:57:54.000000 ibis-framework-v0.6.0/LICENSES/hdfscli.txt
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     1838 2015-06-04 20:35:47.000000 ibis-framework-v0.6.0/LICENSES/pandas.txt
+-rw-r--r--   0 wesm      (1000) wesm      (1000)       84 2015-11-10 22:41:53.000000 ibis-framework-v0.6.0/requirements.txt
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    11358 2015-03-11 21:51:00.000000 ibis-framework-v0.6.0/LICENSE.txt
```

### filetype from file(1)

```diff
@@ -1 +1 @@
-POSIX tar archive
+POSIX tar archive (GNU)
```

### Comparing `ibis_framework-9.0.0.dev686/LICENSE.txt` & `ibis-framework-v0.6.0/LICENSE.txt`

 * *Files 0% similar despite different names*

```diff
@@ -183,15 +183,15 @@
       replaced with your own identifying information. (Don't include
       the brackets!)  The text should be enclosed in the appropriate
       comment syntax for the file format. We also recommend that a
       file or class name and description of purpose be included on the
       same "printed page" as the copyright notice for easier
       identification within third-party archives.
 
-   Copyright 2015 Ibis developers
+   Copyright [yyyy] [name of copyright owner]
 
    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at
 
        http://www.apache.org/licenses/LICENSE-2.0
```

### Comparing `ibis_framework-9.0.0.dev686/ibis/backends/__init__.py` & `ibis-framework-v0.6.0/ibis/expr/api.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,1478 +1,2150 @@
-from __future__ import annotations
-
-import abc
-import collections.abc
-import functools
-import importlib.metadata
-import keyword
-import re
-import sys
-import urllib.parse
-from pathlib import Path
-from typing import TYPE_CHECKING, Any, Callable, ClassVar
-from urllib.parse import parse_qs, urlparse
-
-import ibis
-import ibis.common.exceptions as exc
-import ibis.config
-import ibis.expr.operations as ops
+# Copyright 2015 Cloudera Inc.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+# http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import six
+
+from ibis.expr.datatypes import Schema  # noqa
+from ibis.expr.types import (Expr,  # noqa
+                             ValueExpr, ScalarExpr, ArrayExpr,
+                             TableExpr,
+                             NumericValue, NumericArray,
+                             IntegerValue,
+                             Int8Value, Int8Scalar, Int8Array,
+                             Int16Value, Int16Scalar, Int16Array,
+                             Int32Value, Int32Scalar, Int32Array,
+                             Int64Value, Int64Scalar, Int64Array,
+                             NullScalar,
+                             BooleanValue, BooleanScalar, BooleanArray,
+                             FloatValue, FloatScalar, FloatArray,
+                             DoubleValue, DoubleScalar, DoubleArray,
+                             StringValue, StringScalar, StringArray,
+                             DecimalValue, DecimalScalar, DecimalArray,
+                             TimestampValue, TimestampScalar, TimestampArray,
+                             CategoryValue, unnamed, as_value_expr, literal,
+                             null, sequence)
+
+# __all__ is defined
+from ibis.expr.temporal import *  # noqa
+
+import ibis.common as _com
+
+from ibis.compat import py_string
+from ibis.expr.analytics import bucket, histogram
+from ibis.expr.groupby import GroupedTableExpr  # noqa
+from ibis.expr.window import window, trailing_window, cumulative_window
+import ibis.expr.analytics as _analytics
+import ibis.expr.analysis as _L
 import ibis.expr.types as ir
-from ibis import util
-from ibis.common.caching import RefCountedCache
+import ibis.expr.operations as _ops
+import ibis.expr.temporal as _T
+import ibis.util as util
+
+
+__all__ = [
+    'schema', 'table', 'literal', 'expr_list', 'timestamp',
+    'case', 'where', 'sequence',
+    'now', 'desc', 'null', 'NA',
+    'cast', 'coalesce', 'greatest', 'least',
+    'cross_join', 'join',
+    'aggregate',
+    'row_number',
+    'negate', 'ifelse',
+    'Expr', 'Schema',
+    'window', 'trailing_window', 'cumulative_window'
+]
+__all__ += _T.__all__
+
+
+NA = null()
+
+
+_data_type_docs = """\
+Ibis uses its own type aliases that map onto database types. See, for
+example, the correspondence between Ibis type names and Impala type names:
+
+Ibis type      Impala Type
+~~~~~~~~~      ~~~~~~~~~~~
+int8           TINYINT
+int16          SMALLINT
+int32          INT
+int64          BIGINT
+float          FLOAT
+double         DOUBLE
+boolean        BOOLEAN
+string         STRING
+timestamp      TIMESTAMP
+decimal(p, s)  DECIMAL(p,s)"""
+
+
+def schema(pairs=None, names=None, types=None):
+    if pairs is not None:
+        return Schema.from_tuples(pairs)
+    else:
+        return Schema(names, types)
 
-if TYPE_CHECKING:
-    from collections.abc import Iterable, Iterator, Mapping, MutableMapping
 
-    import pandas as pd
-    import polars as pl
-    import pyarrow as pa
-    import sqlglot as sg
-    import torch
+def table(schema, name=None):
+    """
+    Create an unbound Ibis table for creating expressions. Cannot be executed
+    without being bound to some physical table.
 
-__all__ = ("BaseBackend", "connect")
+    Useful for testing
 
+    Parameters
+    ----------
+    schema : ibis Schema
+    name : string, default None
+      Name for table
+
+    Returns
+    -------
+    table : TableExpr
+    """
+    if not isinstance(schema, Schema):
+        if isinstance(schema, dict):
+            schema = Schema.from_dict(schema)
+        else:
+            schema = Schema.from_tuples(schema)
+
+    node = _ops.UnboundTable(schema, name=name)
+    return TableExpr(node)
 
-class TablesAccessor(collections.abc.Mapping):
-    """A mapping-like object for accessing tables off a backend.
 
-    Tables may be accessed by name using either index or attribute access:
+def desc(expr):
+    """
+    Create a sort key (when used in sort_by) by the passed array expression or
+    column name.
+
+    Parameters
+    ----------
+    expr : array expression or string
+      Can be a column name in the table being sorted
 
     Examples
     --------
-    >>> con = ibis.sqlite.connect("example.db")
-    >>> people = con.tables["people"]  # access via index
-    >>> people = con.tables.people  # access via attribute
+    result = (self.table.group_by('g')
+              .size('count')
+              .sort_by(ibis.desc('count')))
+    """
+    if not isinstance(expr, Expr):
+        return _ops.DeferredSortKey(expr, ascending=False)
+    else:
+        return _ops.SortKey(expr, ascending=False).to_expr()
 
+
+def timestamp(value):
+    """
+    Returns a timestamp literal if value is likely coercible to a timestamp
     """
+    if isinstance(value, py_string):
+        from pandas import Timestamp
+        value = Timestamp(value)
+    op = ir.Literal(value)
+    return ir.TimestampScalar(op)
+
+
+schema.__doc__ = """\
+Validate and return an Ibis Schema object
+
+{0}
+
+Parameters
+----------
+pairs : list of (name, type) tuples
+  Mutually exclusive with names/types
+names : list of string
+  Field names
+types : list of string
+  Field types
+
+Examples
+--------
+sc = schema([('foo', 'string'),
+             ('bar', 'int64'),
+             ('baz', 'boolean')])
+
+sc2 = schema(names=['foo', 'bar', 'baz'],
+             types=['string', 'int64', 'boolean'])
+
+Returns
+-------
+schema : Schema
+""".format(_data_type_docs)
 
-    def __init__(self, backend: BaseBackend):
-        self._backend = backend
 
-    def __getitem__(self, name) -> ir.Table:
-        try:
-            return self._backend.table(name)
-        except Exception as exc:  # noqa: BLE001
-            raise KeyError(name) from exc
-
-    def __getattr__(self, name) -> ir.Table:
-        if name.startswith("_"):
-            raise AttributeError(name)
-        try:
-            return self._backend.table(name)
-        except Exception as exc:  # noqa: BLE001
-            raise AttributeError(name) from exc
-
-    def __iter__(self) -> Iterator[str]:
-        return iter(sorted(self._backend.list_tables()))
-
-    def __len__(self) -> int:
-        return len(self._backend.list_tables())
-
-    def __dir__(self) -> list[str]:
-        o = set()
-        o.update(dir(type(self)))
-        o.update(
-            name
-            for name in self._backend.list_tables()
-            if name.isidentifier() and not keyword.iskeyword(name)
-        )
-        return list(o)
-
-    def __repr__(self) -> str:
-        tables = self._backend.list_tables()
-        rows = ["Tables", "------"]
-        rows.extend(f"- {name}" for name in sorted(tables))
-        return "\n".join(rows)
-
-    def _ipython_key_completions_(self) -> list[str]:
-        return self._backend.list_tables()
-
-
-class _FileIOHandler:
-    @staticmethod
-    def _import_pyarrow():
-        try:
-            import pyarrow  # noqa: ICN001
-        except ImportError:
-            raise ModuleNotFoundError(
-                "Exporting to arrow formats requires `pyarrow` but it is not installed"
-            )
-        else:
-            import pyarrow_hotfix  # noqa: F401
+def case():
+    """
+    Similar to the .case method on array expressions, create a case builder
+    that accepts self-contained boolean expressions (as opposed to expressions
+    which are to be equality-compared with a fixed value expression)
 
-            return pyarrow
+    Use the .when method on the resulting object followed by .end to create a
+    complete case.
 
-    def to_pandas(
-        self,
-        expr: ir.Expr,
-        *,
-        params: Mapping[ir.Scalar, Any] | None = None,
-        limit: int | str | None = None,
-        **kwargs: Any,
-    ) -> pd.DataFrame | pd.Series | Any:
-        """Execute an Ibis expression and return a pandas `DataFrame`, `Series`, or scalar.
-
-        ::: {.callout-note}
-        This method is a wrapper around `execute`.
-        :::
-
-        Parameters
-        ----------
-        expr
-            Ibis expression to execute.
-        params
-            Mapping of scalar parameter expressions to value.
-        limit
-            An integer to effect a specific row limit. A value of `None` means
-            "no limit". The default is in `ibis/config.py`.
-        kwargs
-            Keyword arguments
-
-        """
-        return self.execute(expr, params=params, limit=limit, **kwargs)
-
-    def to_pandas_batches(
-        self,
-        expr: ir.Expr,
-        *,
-        params: Mapping[ir.Scalar, Any] | None = None,
-        limit: int | str | None = None,
-        chunk_size: int = 1_000_000,
-        **kwargs: Any,
-    ) -> Iterator[pd.DataFrame | pd.Series | Any]:
-        """Execute an Ibis expression and return an iterator of pandas `DataFrame`s.
-
-        Parameters
-        ----------
-        expr
-            Ibis expression to execute.
-        params
-            Mapping of scalar parameter expressions to value.
-        limit
-            An integer to effect a specific row limit. A value of `None` means
-            "no limit". The default is in `ibis/config.py`.
-        chunk_size
-            Maximum number of rows in each returned `DataFrame` batch. This may have
-            no effect depending on the backend.
-        kwargs
-            Keyword arguments
-
-        Returns
-        -------
-        Iterator[pd.DataFrame]
-            An iterator of pandas `DataFrame`s.
-
-        """
-        from ibis.formats.pandas import PandasData
-
-        orig_expr = expr
-        expr = expr.as_table()
-        schema = expr.schema()
-        yield from (
-            orig_expr.__pandas_result__(
-                PandasData.convert_table(batch.to_pandas(), schema)
-            )
-            for batch in self.to_pyarrow_batches(
-                expr, params=params, limit=limit, chunk_size=chunk_size, **kwargs
-            )
-        )
-
-    @util.experimental
-    def to_pyarrow(
-        self,
-        expr: ir.Expr,
-        *,
-        params: Mapping[ir.Scalar, Any] | None = None,
-        limit: int | str | None = None,
-        **kwargs: Any,
-    ) -> pa.Table:
-        """Execute expression and return results in as a pyarrow table.
-
-        This method is eager and will execute the associated expression
-        immediately.
-
-        Parameters
-        ----------
-        expr
-            Ibis expression to export to pyarrow
-        params
-            Mapping of scalar parameter expressions to value.
-        limit
-            An integer to effect a specific row limit. A value of `None` means
-            "no limit". The default is in `ibis/config.py`.
-        kwargs
-            Keyword arguments
-
-        Returns
-        -------
-        Table
-            A pyarrow table holding the results of the executed expression.
-
-        """
-        pa = self._import_pyarrow()
-        self._run_pre_execute_hooks(expr)
-
-        table_expr = expr.as_table()
-        schema = table_expr.schema()
-        arrow_schema = schema.to_pyarrow()
-        with self.to_pyarrow_batches(
-            table_expr, params=params, limit=limit, **kwargs
-        ) as reader:
-            table = pa.Table.from_batches(reader, schema=arrow_schema)
-
-        return expr.__pyarrow_result__(
-            table.rename_columns(table_expr.columns).cast(arrow_schema)
-        )
-
-    @util.experimental
-    def to_polars(
-        self,
-        expr: ir.Expr,
-        *,
-        params: Mapping[ir.Scalar, Any] | None = None,
-        limit: int | str | None = None,
-        **kwargs: Any,
-    ) -> pl.DataFrame:
-        """Execute expression and return results in as a polars DataFrame.
-
-        This method is eager and will execute the associated expression
-        immediately.
-
-        Parameters
-        ----------
-        expr
-            Ibis expression to export to polars.
-        params
-            Mapping of scalar parameter expressions to value.
-        limit
-            An integer to effect a specific row limit. A value of `None` means
-            "no limit". The default is in `ibis/config.py`.
-        kwargs
-            Keyword arguments
-
-        Returns
-        -------
-        dataframe
-            A polars DataFrame holding the results of the executed expression.
-
-        """
-        import polars as pl
-
-        table = self.to_pyarrow(expr.as_table(), params=params, limit=limit, **kwargs)
-        return expr.__polars_result__(pl.from_arrow(table))
-
-    @util.experimental
-    def to_pyarrow_batches(
-        self,
-        expr: ir.Expr,
-        *,
-        params: Mapping[ir.Scalar, Any] | None = None,
-        limit: int | str | None = None,
-        chunk_size: int = 1_000_000,
-        **kwargs: Any,
-    ) -> pa.ipc.RecordBatchReader:
-        """Execute expression and return a RecordBatchReader.
-
-        This method is eager and will execute the associated expression
-        immediately.
-
-        Parameters
-        ----------
-        expr
-            Ibis expression to export to pyarrow
-        limit
-            An integer to effect a specific row limit. A value of `None` means
-            "no limit". The default is in `ibis/config.py`.
-        params
-            Mapping of scalar parameter expressions to value.
-        chunk_size
-            Maximum number of rows in each returned record batch.
-        kwargs
-            Keyword arguments
-
-        Returns
-        -------
-        results
-            RecordBatchReader
+    Examples
+    --------
+    expr = (ibis.case()
+            .when(cond1, result1)
+            .when(cond2, result2).end())
+
+    Returns
+    -------
+    case : CaseBuilder
+    """
+    return _ops.SearchedCaseBuilder()
 
-        """
-        raise NotImplementedError
 
-    @util.experimental
-    def to_torch(
-        self,
-        expr: ir.Expr,
-        *,
-        params: Mapping[ir.Scalar, Any] | None = None,
-        limit: int | str | None = None,
-        **kwargs: Any,
-    ) -> dict[str, torch.Tensor]:
-        """Execute an expression and return results as a dictionary of torch tensors.
-
-        Parameters
-        ----------
-        expr
-            Ibis expression to execute.
-        params
-            Parameters to substitute into the expression.
-        limit
-            An integer to effect a specific row limit. A value of `None` means no limit.
-        kwargs
-            Keyword arguments passed into the backend's `to_torch` implementation.
-
-        Returns
-        -------
-        dict[str, torch.Tensor]
-            A dictionary of torch tensors, keyed by column name.
-
-        """
-        import torch
-
-        t = self.to_pyarrow(expr, params=params, limit=limit, **kwargs)
-        # without .copy() the arrays are read-only and thus writing to them is
-        # undefined behavior; we can't ignore this warning from torch because
-        # we're going out of ibis and downstream code can do whatever it wants
-        # with the data
-        return {
-            name: torch.from_numpy(t[name].to_numpy().copy()) for name in t.schema.names
-        }
-
-    def read_parquet(
-        self, path: str | Path, table_name: str | None = None, **kwargs: Any
-    ) -> ir.Table:
-        """Register a parquet file as a table in the current backend.
-
-        Parameters
-        ----------
-        path
-            The data source.
-        table_name
-            An optional name to use for the created table. This defaults to
-            a sequentially generated name.
-        **kwargs
-            Additional keyword arguments passed to the backend loading function.
-
-        Returns
-        -------
-        ir.Table
-            The just-registered table
-
-        """
-        raise NotImplementedError(
-            f"{self.name} does not support direct registration of parquet data."
-        )
-
-    def read_csv(
-        self, path: str | Path, table_name: str | None = None, **kwargs: Any
-    ) -> ir.Table:
-        """Register a CSV file as a table in the current backend.
-
-        Parameters
-        ----------
-        path
-            The data source. A string or Path to the CSV file.
-        table_name
-            An optional name to use for the created table. This defaults to
-            a sequentially generated name.
-        **kwargs
-            Additional keyword arguments passed to the backend loading function.
-
-        Returns
-        -------
-        ir.Table
-            The just-registered table
-
-        """
-        raise NotImplementedError(
-            f"{self.name} does not support direct registration of CSV data."
-        )
-
-    def read_json(
-        self, path: str | Path, table_name: str | None = None, **kwargs: Any
-    ) -> ir.Table:
-        """Register a JSON file as a table in the current backend.
-
-        Parameters
-        ----------
-        path
-            The data source. A string or Path to the JSON file.
-        table_name
-            An optional name to use for the created table. This defaults to
-            a sequentially generated name.
-        **kwargs
-            Additional keyword arguments passed to the backend loading function.
-
-        Returns
-        -------
-        ir.Table
-            The just-registered table
-
-        """
-        raise NotImplementedError(
-            f"{self.name} does not support direct registration of JSON data."
-        )
-
-    def read_delta(
-        self, source: str | Path, table_name: str | None = None, **kwargs: Any
-    ):
-        """Register a Delta Lake table in the current database.
-
-        Parameters
-        ----------
-        source
-            The data source. Must be a directory
-            containing a Delta Lake table.
-        table_name
-            An optional name to use for the created table. This defaults to
-            a sequentially generated name.
-        **kwargs
-            Additional keyword arguments passed to the underlying backend or library.
-
-        Returns
-        -------
-        ir.Table
-            The just-registered table.
-
-        """
-        raise NotImplementedError(
-            f"{self.name} does not support direct registration of DeltaLake tables."
-        )
-
-    @util.experimental
-    def to_parquet(
-        self,
-        expr: ir.Table,
-        path: str | Path,
-        *,
-        params: Mapping[ir.Scalar, Any] | None = None,
-        **kwargs: Any,
-    ) -> None:
-        """Write the results of executing the given expression to a parquet file.
-
-        This method is eager and will execute the associated expression
-        immediately.
-
-        Parameters
-        ----------
-        expr
-            The ibis expression to execute and persist to parquet.
-        path
-            The data source. A string or Path to the parquet file.
-        params
-            Mapping of scalar parameter expressions to value.
-        **kwargs
-            Additional keyword arguments passed to pyarrow.parquet.ParquetWriter
-
-        https://arrow.apache.org/docs/python/generated/pyarrow.parquet.ParquetWriter.html
-
-        """
-        self._import_pyarrow()
-        import pyarrow.parquet as pq
-
-        with expr.to_pyarrow_batches(params=params) as batch_reader:
-            with pq.ParquetWriter(path, batch_reader.schema, **kwargs) as writer:
-                for batch in batch_reader:
-                    writer.write_batch(batch)
-
-    @util.experimental
-    def to_csv(
-        self,
-        expr: ir.Table,
-        path: str | Path,
-        *,
-        params: Mapping[ir.Scalar, Any] | None = None,
-        **kwargs: Any,
-    ) -> None:
-        """Write the results of executing the given expression to a CSV file.
-
-        This method is eager and will execute the associated expression
-        immediately.
-
-        Parameters
-        ----------
-        expr
-            The ibis expression to execute and persist to CSV.
-        path
-            The data source. A string or Path to the CSV file.
-        params
-            Mapping of scalar parameter expressions to value.
-        kwargs
-            Additional keyword arguments passed to pyarrow.csv.CSVWriter
-
-        https://arrow.apache.org/docs/python/generated/pyarrow.csv.CSVWriter.html
-
-        """
-        self._import_pyarrow()
-        import pyarrow.csv as pcsv
-
-        with expr.to_pyarrow_batches(params=params) as batch_reader:
-            with pcsv.CSVWriter(path, batch_reader.schema, **kwargs) as writer:
-                for batch in batch_reader:
-                    writer.write_batch(batch)
-
-    @util.experimental
-    def to_delta(
-        self,
-        expr: ir.Table,
-        path: str | Path,
-        *,
-        params: Mapping[ir.Scalar, Any] | None = None,
-        **kwargs: Any,
-    ) -> None:
-        """Write the results of executing the given expression to a Delta Lake table.
-
-        This method is eager and will execute the associated expression
-        immediately.
-
-        Parameters
-        ----------
-        expr
-            The ibis expression to execute and persist to Delta Lake table.
-        path
-            The data source. A string or Path to the Delta Lake table.
-        params
-            Mapping of scalar parameter expressions to value.
-        kwargs
-            Additional keyword arguments passed to deltalake.writer.write_deltalake method
+def now():
+    """
+    Compute the current timestamp
+
+    Returns
+    -------
+    now : Timestamp scalar
+    """
+    return _ops.TimestampNow().to_expr()
+
+
+def row_number():
+    """
+    Analytic function for the current row number, starting at 0
+
+    Returns
+    -------
+    row_number : IntArray
+    """
+    return _ops.RowNumber().to_expr()
+
+
+e = _ops.E().to_expr()
+
+
+def _add_methods(klass, method_table):
+    for k, v in method_table.items():
+        setattr(klass, k, v)
+
+
+def _unary_op(name, klass, doc=None):
+    def f(arg):
+        return klass(arg).to_expr()
+    f.__name__ = name
+    if doc is not None:
+        f.__doc__ = doc
+    else:
+        f.__doc__ = klass.__doc__
+    return f
+
+
+def negate(arg):
+    """
+    Negate a numeric expression
+
+    Parameters
+    ----------
+    arg : numeric value expression
+
+    Returns
+    -------
+    negated : type of caller
+    """
+    op = arg.op()
+    if hasattr(op, 'negate'):
+        result = op.negate()
+    else:
+        result = _ops.Negate(arg)
+
+    return result.to_expr()
+
+
+def count(expr, where=None):
+    """
+    Compute cardinality / sequence size of expression. For array expressions,
+    the count is excluding nulls. For tables, it's the size of the entire
+    table.
+
+    Returns
+    -------
+    counts : int64 type
+    """
+    op = expr.op()
+    if isinstance(op, _ops.DistinctArray):
+        if where is not None:
+            raise NotImplementedError
+        result = op.count().to_expr()
+    else:
+        result = _ops.Count(expr, where).to_expr()
+
+    return result.name('count')
+
+
+def group_concat(arg, sep=','):
+    """
+    Concatenate values using the indicated separator (comma by default) to
+    produce a string
+
+    Parameters
+    ----------
+    arg : array expression
+    sep : string, default ','
+
+    Returns
+    -------
+    concatenated : string scalar
+    """
+    return _ops.GroupConcat(arg, sep).to_expr()
+
 
-        """
+def _binop_expr(name, klass):
+    def f(self, other):
         try:
-            from deltalake.writer import write_deltalake
-        except ImportError:
-            raise ImportError(
-                "The deltalake extra is required to use the "
-                "to_delta method. You can install it using pip:\n\n"
-                "pip install 'ibis-framework[deltalake]'\n"
-            )
-
-        with expr.to_pyarrow_batches(params=params) as batch_reader:
-            write_deltalake(path, batch_reader, **kwargs)
-
-
-class CanListCatalog(abc.ABC):
-    @abc.abstractmethod
-    def list_catalogs(self, like: str | None = None) -> list[str]:
-        """List existing catalogs in the current connection.
-
-        ::: {.callout-note}
-        ## Ibis does not use the word `schema` to refer to database hierarchy.
-
-        A collection of `table` is referred to as a `database`.
-        A collection of `database` is referred to as a `catalog`.
-
-        These terms are mapped onto the corresponding features in each
-        backend (where available), regardless of whether the backend itself
-        uses the same terminology.
-        :::
-
-        Parameters
-        ----------
-        like
-            A pattern in Python's regex format to filter returned database
-            names.
-
-        Returns
-        -------
-        list[str]
-            The catalog names that exist in the current connection, that match
-            the `like` pattern if provided.
-
-        """
-
-    @property
-    @abc.abstractmethod
-    def current_catalog(self) -> str:
-        """The current catalog in use."""
-
-
-class CanCreateCatalog(CanListCatalog):
-    @abc.abstractmethod
-    def create_catalog(self, name: str, force: bool = False) -> None:
-        """Create a new catalog.
-
-        ::: {.callout-note}
-        ## Ibis does not use the word `schema` to refer to database hierarchy.
-
-        A collection of `table` is referred to as a `database`.
-        A collection of `database` is referred to as a `catalog`.
-
-        These terms are mapped onto the corresponding features in each
-        backend (where available), regardless of whether the backend itself
-        uses the same terminology.
-        :::
-
-        Parameters
-        ----------
-        name
-            Name of the new catalog.
-        force
-            If `False`, an exception is raised if the catalog already exists.
-
-        """
-
-    @abc.abstractmethod
-    def drop_catalog(self, name: str, force: bool = False) -> None:
-        """Drop a catalog with name `name`.
-
-        ::: {.callout-note}
-        ## Ibis does not use the word `schema` to refer to database hierarchy.
-
-        A collection of `table` is referred to as a `database`.
-        A collection of `database` is referred to as a `catalog`.
-
-        These terms are mapped onto the corresponding features in each
-        backend (where available), regardless of whether the backend itself
-        uses the same terminology.
-        :::
-
-        Parameters
-        ----------
-        name
-            Catalog to drop.
-        force
-            If `False`, an exception is raised if the catalog does not exist.
-
-        """
-
-
-class CanListDatabase(abc.ABC):
-    @abc.abstractmethod
-    def list_databases(
-        self, like: str | None = None, catalog: str | None = None
-    ) -> list[str]:
-        """List existing databases in the current connection.
-
-        ::: {.callout-note}
-        ## Ibis does not use the word `schema` to refer to database hierarchy.
-
-        A collection of `table` is referred to as a `database`.
-        A collection of `database` is referred to as a `catalog`.
-
-        These terms are mapped onto the corresponding features in each
-        backend (where available), regardless of whether the backend itself
-        uses the same terminology.
-        :::
-
-        Parameters
-        ----------
-        like
-            A pattern in Python's regex format to filter returned database
-            names.
-        catalog
-            The catalog to list databases from. If `None`, the current catalog
-            is searched.
-
-        Returns
-        -------
-        list[str]
-            The database names that exist in the current connection, that match
-            the `like` pattern if provided.
-
-        """
-
-    @property
-    @abc.abstractmethod
-    def current_database(self) -> str:
-        """The current database in use."""
-
-
-class CanCreateDatabase(CanListDatabase):
-    @abc.abstractmethod
-    def create_database(
-        self, name: str, catalog: str | None = None, force: bool = False
-    ) -> None:
-        """Create a database named `name` in `catalog`.
-
-        Parameters
-        ----------
-        name
-            Name of the database to create.
-        catalog
-            Name of the catalog in which to create the database. If `None`, the
-            current catalog is used.
-        force
-            If `False`, an exception is raised if the database exists.
-
-        """
-
-    @abc.abstractmethod
-    def drop_database(
-        self, name: str, catalog: str | None = None, force: bool = False
-    ) -> None:
-        """Drop the database with `name` in `catalog`.
-
-        Parameters
-        ----------
-        name
-            Name of the schema to drop.
-        catalog
-            Name of the catalog to drop the database from. If `None`, the
-            current catalog is used.
-        force
-            If `False`, an exception is raised if the database does not exist.
-
-        """
-
-
-# TODO: remove this for 10.0
-class CanListSchema:
-    @util.deprecated(
-        instead="Use `list_databases` instead`", as_of="9.0", removed_in="10.0"
-    )
-    def list_schemas(
-        self, like: str | None = None, database: str | None = None
-    ) -> list[str]:
-        return self.list_databases(like=like, catalog=database)
-
-    @property
-    @util.deprecated(
-        instead="Use `Backend.current_database` instead.",
-        as_of="9.0",
-        removed_in="10.0",
-    )
-    def current_schema(self) -> str:
-        return self.current_database
-
-
-class CanCreateSchema(CanListSchema):
-    @util.deprecated(
-        instead="Use `create_database` instead", as_of="9.0", removed_in="10.0"
-    )
-    def create_schema(
-        self, name: str, database: str | None = None, force: bool = False
-    ) -> None:
-        self.create_database(name=name, catalog=database, force=force)
-
-    @util.deprecated(
-        instead="Use `drop_database` instead", as_of="9.0", removed_in="10.0"
-    )
-    def drop_schema(
-        self, name: str, database: str | None = None, force: bool = False
-    ) -> None:
-        self.drop_database(name=name, catalog=database, force=force)
-
-
-class BaseBackend(abc.ABC, _FileIOHandler):
-    """Base backend class.
-
-    All Ibis backends must subclass this class and implement all the
-    required methods.
-    """
-
-    name: ClassVar[str]
-
-    supports_temporary_tables = False
-    supports_python_udfs = False
-    supports_in_memory_tables = True
-
-    def __init__(self, *args, **kwargs):
-        self._con_args: tuple[Any] = args
-        self._con_kwargs: dict[str, Any] = kwargs
-        # expression cache
-        self._query_cache = RefCountedCache(
-            populate=self._load_into_cache,
-            lookup=lambda name: self.table(name).op(),
-            finalize=self._clean_up_cached_table,
-            generate_name=functools.partial(util.gen_name, "cache"),
-            key=lambda expr: expr.op(),
-        )
-
-    @property
-    @abc.abstractmethod
-    def dialect(self) -> sg.Dialect | None:
-        """The sqlglot dialect for this backend, where applicable.
-
-        Returns None if the backend is not a SQL backend.
-        """
-
-    def __getstate__(self):
-        return dict(_con_args=self._con_args, _con_kwargs=self._con_kwargs)
-
-    def __rich_repr__(self):
-        yield "name", self.name
-
-    def __hash__(self):
-        return hash(self.db_identity)
-
-    def __eq__(self, other):
-        return self.db_identity == other.db_identity
-
-    @functools.cached_property
-    def db_identity(self) -> str:
-        """Return the identity of the database.
-
-        Multiple connections to the same
-        database will return the same value for `db_identity`.
-
-        The default implementation assumes connection parameters uniquely
-        specify the database.
-
-        Returns
-        -------
-        Hashable
-            Database identity
-
-        """
-        parts = [self.__class__]
-        parts.extend(self._con_args)
-        parts.extend(f"{k}={v}" for k, v in self._con_kwargs.items())
-        return "_".join(map(str, parts))
-
-    # TODO(kszucs): this should be a classmethod returning with a new backend
-    # instance which does instantiate the connection
-    def connect(self, *args, **kwargs) -> BaseBackend:
-        """Connect to the database.
-
-        Parameters
-        ----------
-        *args
-            Mandatory connection parameters, see the docstring of `do_connect`
-            for details.
-        **kwargs
-            Extra connection parameters, see the docstring of `do_connect` for
-            details.
-
-        Notes
-        -----
-        This creates a new backend instance with saved `args` and `kwargs`,
-        then calls `reconnect` and finally returns the newly created and
-        connected backend instance.
-
-        Returns
-        -------
-        BaseBackend
-            An instance of the backend
-
-        """
-        new_backend = self.__class__(*args, **kwargs)
-        new_backend.reconnect()
-        return new_backend
-
-    @abc.abstractmethod
-    def disconnect(self) -> None:
-        """Close the connection to the backend."""
-
-    @staticmethod
-    def _convert_kwargs(kwargs: MutableMapping) -> None:
-        """Manipulate keyword arguments to `.connect` method."""
-
-    # TODO(kszucs): should call self.connect(*self._con_args, **self._con_kwargs)
-    def reconnect(self) -> None:
-        """Reconnect to the database already configured with connect."""
-        self.do_connect(*self._con_args, **self._con_kwargs)
-
-    def do_connect(self, *args, **kwargs) -> None:
-        """Connect to database specified by `args` and `kwargs`."""
-
-    @staticmethod
-    def _filter_with_like(values: Iterable[str], like: str | None = None) -> list[str]:
-        """Filter names with a `like` pattern (regex).
-
-        The methods `list_databases` and `list_tables` accept a `like`
-        argument, which filters the returned tables with tables that match the
-        provided pattern.
-
-        We provide this method in the base backend, so backends can use it
-        instead of reinventing the wheel.
-
-        Parameters
-        ----------
-        values
-            Iterable of strings to filter
-        like
-            Pattern to use for filtering names
-
-        Returns
-        -------
-        list[str]
-            Names filtered by the `like` pattern.
-
-        """
-        if like is None:
-            return sorted(values)
-
-        pattern = re.compile(like)
-        return sorted(filter(pattern.findall, values))
-
-    @abc.abstractmethod
-    def list_tables(
-        self, like: str | None = None, database: tuple[str, str] | str | None = None
-    ) -> list[str]:
-        """Return the list of table names in the current database.
-
-        For some backends, the tables may be files in a directory,
-        or other equivalent entities in a SQL database.
-
-        Parameters
-        ----------
-        like
-            A pattern in Python's regex format.
-        database
-            The database from which to list tables.
-            If not provided, the current database is used.
-            For backends that support multi-level table hierarchies, you can
-            pass in a dotted string path like `"catalog.database"` or a tuple of
-            strings like `("catalog", "database")`.
-
-            ::: {.callout-note}
-            ## Ibis does not use the word `schema` to refer to database hierarchy.
-
-            A collection of tables is referred to as a `database`.
-            A collection of `database` is referred to as a `catalog`.
-
-            These terms are mapped onto the corresponding features in each
-            backend (where available), regardless of whether the backend itself
-            uses the same terminology.
-            :::
-
-        Returns
-        -------
-        list[str]
-            The list of the table names that match the pattern `like`.
-
-        """
-
-    @abc.abstractmethod
-    def table(
-        self, name: str, database: tuple[str, str] | str | None = None
-    ) -> ir.Table:
-        """Construct a table expression.
-
-        Parameters
-        ----------
-        name
-            Table name
-        database
-            Database name
-            If not provided, the current database is used.
-            For backends that support multi-level table hierarchies, you can
-            pass in a dotted string path like `"catalog.database"` or a tuple of
-            strings like `("catalog", "database")`.
-
-            ::: {.callout-note}
-            ## Ibis does not use the word `schema` to refer to database hierarchy.
-
-            A collection of tables is referred to as a `database`.
-            A collection of `database` is referred to as a `catalog`.
-
-            These terms are mapped onto the corresponding features in each
-            backend (where available), regardless of whether the backend itself
-            uses the same terminology.
-            :::
-
-        Returns
-        -------
-        Table
-            Table expression
-
-        """
-
-    @functools.cached_property
-    def tables(self):
-        """An accessor for tables in the database.
-
-        Tables may be accessed by name using either index or attribute access:
-
-        Examples
-        --------
-        >>> con = ibis.sqlite.connect("example.db")
-        >>> people = con.tables["people"]  # access via index
-        >>> people = con.tables.people  # access via attribute
-
-        """
-        return TablesAccessor(self)
-
-    @property
-    @abc.abstractmethod
-    def version(self) -> str:
-        """Return the version of the backend engine.
-
-        For database servers, return the server version.
-
-        For others such as SQLite and pandas return the version of the
-        underlying library or application.
-
-        Returns
-        -------
-        str
-            The backend version
-
-        """
-
-    @classmethod
-    def register_options(cls) -> None:
-        """Register custom backend options."""
-        options = ibis.config.options
-        backend_name = cls.name
+            other = as_value_expr(other)
+            op = klass(self, other)
+            return op.to_expr()
+        except _com.InputTypeError:
+            return NotImplemented
+
+    f.__name__ = name
+
+    return f
+
+
+def _rbinop_expr(name, klass):
+    # For reflexive binary _ops, like radd, etc.
+    def f(self, other):
+        other = as_value_expr(other)
+        op = klass(other, self)
+        return op.to_expr()
+
+    f.__name__ = name
+    return f
+
+
+def _boolean_binary_op(name, klass):
+    def f(self, other):
+        other = as_value_expr(other)
+
+        if not isinstance(other, BooleanValue):
+            raise TypeError(other)
+
+        op = klass(self, other)
+        return op.to_expr()
+
+    f.__name__ = name
+
+    return f
+
+
+def _boolean_binary_rop(name, klass):
+    def f(self, other):
+        other = as_value_expr(other)
+
+        if not isinstance(other, BooleanValue):
+            raise TypeError(other)
+
+        op = klass(other, self)
+        return op.to_expr()
+
+    f.__name__ = name
+    return f
+
+
+def _agg_function(name, klass, assign_default_name=True):
+    def f(self, where=None):
+        expr = klass(self, where).to_expr()
+        if assign_default_name:
+            expr = expr.name(name)
+        return expr
+    f.__name__ = name
+    return f
+
+
+def _extract_field(name, klass):
+    def f(self):
+        expr = klass(self).to_expr()
+        return expr.name(name)
+    f.__name__ = name
+    return f
+
+
+# ---------------------------------------------------------------------
+# Generic value API
+
+
+def cast(arg, target_type):
+    # validate
+    op = _ops.Cast(arg, target_type)
+
+    if op.args[1] == arg.type():
+        # noop case if passed type is the same
+        return arg
+    else:
+        result = op.to_expr()
         try:
-            backend_options = cls.Options()
-        except AttributeError:
+            expr_name = ('cast({0}, {1!s})'
+                         .format(arg.get_name(),
+                                 op.args[1]))
+            result = result.name(expr_name)
+        except:
             pass
-        else:
-            try:
-                setattr(options, backend_name, backend_options)
-            except ValueError as e:
-                raise exc.BackendConfigurationNotRegistered(backend_name) from e
-
-    def _register_udfs(self, expr: ir.Expr) -> None:
-        """Register UDFs contained in `expr` with the backend."""
-        if self.supports_python_udfs:
-            raise NotImplementedError(self.name)
-
-    def _register_in_memory_tables(self, expr: ir.Expr):
-        if self.supports_in_memory_tables:
-            raise NotImplementedError(self.name)
-
-    def _run_pre_execute_hooks(self, expr: ir.Expr) -> None:
-        """Backend-specific hooks to run before an expression is executed."""
-        self._define_udf_translation_rules(expr)
-        self._register_udfs(expr)
-        self._register_in_memory_tables(expr)
-
-    def _define_udf_translation_rules(self, expr: ir.Expr):
-        if self.supports_python_udfs:
-            raise NotImplementedError(self.name)
-
-    def compile(
-        self,
-        expr: ir.Expr,
-        params: Mapping[ir.Expr, Any] | None = None,
-    ) -> Any:
-        """Compile an expression."""
-        return self.compiler.to_sql(expr, params=params)
-
-    def _to_sqlglot(self, expr: ir.Expr, **kwargs) -> sg.exp.Expression:
-        """Convert an Ibis expression to a sqlglot expression.
-
-        Called by `ibis.to_sql`; gives the backend an opportunity to generate
-        nicer SQL for human consumption.
-        """
-        raise NotImplementedError(f"Backend '{self.name}' backend doesn't support SQL")
-
-    def execute(self, expr: ir.Expr) -> Any:
-        """Execute an expression."""
-
-    def add_operation(self, operation: ops.Node) -> Callable:
-        """Add a translation function to the backend for a specific operation.
-
-        Operations are defined in `ibis.expr.operations`, and a translation
-        function receives the translator object and an expression as
-        parameters, and returns a value depending on the backend.
-        """
-        if not hasattr(self, "compiler"):
-            raise RuntimeError("Only SQL-based backends support `add_operation`")
-
-        def decorator(translation_function: Callable) -> None:
-            self.compiler.translator_class.add_operation(
-                operation, translation_function
-            )
-
-        return decorator
-
-    @abc.abstractmethod
-    def create_table(
-        self,
-        name: str,
-        obj: pd.DataFrame | pa.Table | ir.Table | None = None,
-        *,
-        schema: ibis.Schema | None = None,
-        database: str | None = None,
-        temp: bool = False,
-        overwrite: bool = False,
-    ) -> ir.Table:
-        """Create a new table.
-
-        Parameters
-        ----------
-        name
-            Name of the new table.
-        obj
-            An Ibis table expression or pandas table that will be used to
-            extract the schema and the data of the new table. If not provided,
-            `schema` must be given.
-        schema
-            The schema for the new table. Only one of `schema` or `obj` can be
-            provided.
-        database
-            Name of the database where the table will be created, if not the
-            default.
-        temp
-            Whether a table is temporary or not
-        overwrite
-            Whether to clobber existing data
-
-        Returns
-        -------
-        Table
-            The table that was created.
-
-        """
-
-    @abc.abstractmethod
-    def drop_table(
-        self,
-        name: str,
-        *,
-        database: str | None = None,
-        force: bool = False,
-    ) -> None:
-        """Drop a table.
-
-        Parameters
-        ----------
-        name
-            Name of the table to drop.
-        database
-            Name of the database where the table exists, if not the default.
-        force
-            If `False`, an exception is raised if the table does not exist.
-
-        """
-        raise NotImplementedError(
-            f'Backend "{self.name}" does not implement "drop_table"'
-        )
-
-    def rename_table(self, old_name: str, new_name: str) -> None:
-        """Rename an existing table.
-
-        Parameters
-        ----------
-        old_name
-            The old name of the table.
-        new_name
-            The new name of the table.
-
-        """
-        raise NotImplementedError(
-            f'Backend "{self.name}" does not implement "rename_table"'
-        )
-
-    @abc.abstractmethod
-    def create_view(
-        self,
-        name: str,
-        obj: ir.Table,
-        *,
-        database: str | None = None,
-        overwrite: bool = False,
-    ) -> ir.Table:
-        """Create a new view from an expression.
-
-        Parameters
-        ----------
-        name
-            Name of the new view.
-        obj
-            An Ibis table expression that will be used to create the view.
-        database
-            Name of the database where the view will be created, if not
-            provided the database's default is used.
-        overwrite
-            Whether to clobber an existing view with the same name
-
-        Returns
-        -------
-        Table
-            The view that was created.
-
-        """
-
-    @abc.abstractmethod
-    def drop_view(
-        self, name: str, *, database: str | None = None, force: bool = False
-    ) -> None:
-        """Drop a view.
-
-        Parameters
-        ----------
-        name
-            Name of the view to drop.
-        database
-            Name of the database where the view exists, if not the default.
-        force
-            If `False`, an exception is raised if the view does not exist.
-
-        """
-
-    @classmethod
-    def has_operation(cls, operation: type[ops.Value]) -> bool:
-        """Return whether the backend implements support for `operation`.
-
-        Parameters
-        ----------
-        operation
-            A class corresponding to an operation.
-
-        Returns
-        -------
-        bool
-            Whether the backend implements the operation.
-
-        Examples
-        --------
-        >>> import ibis
-        >>> import ibis.expr.operations as ops
-        >>> ibis.sqlite.has_operation(ops.ArrayIndex)
-        False
-        >>> ibis.postgres.has_operation(ops.ArrayIndex)
-        True
-
-        """
-        raise NotImplementedError(
-            f"{cls.name} backend has not implemented `has_operation` API"
-        )
-
-    def _cached(self, expr: ir.Table):
-        """Cache the provided expression.
-
-        All subsequent operations on the returned expression will be performed on the cached data.
-
-        Parameters
-        ----------
-        expr
-            Table expression to cache
-
-        Returns
-        -------
-        Expr
-            Cached table
-
-        """
-        op = expr.op()
-        if (result := self._query_cache.get(op)) is None:
-            self._query_cache.store(expr)
-            result = self._query_cache[op]
-        return ir.CachedTable(result)
-
-    def _release_cached(self, expr: ir.CachedTable) -> None:
-        """Releases the provided cached expression.
-
-        Parameters
-        ----------
-        expr
-            Cached expression to release
-
-        """
-        del self._query_cache[expr.op()]
-
-    def _load_into_cache(self, name, expr):
-        raise NotImplementedError(self.name)
-
-    def _clean_up_cached_table(self, op):
-        raise NotImplementedError(self.name)
-
-    def _transpile_sql(self, query: str, *, dialect: str | None = None) -> str:
-        # only transpile if dialect was passed
-        if dialect is None:
-            return query
-
-        import sqlglot as sg
-
-        # only transpile if the backend dialect doesn't match the input dialect
-        name = self.name
-        if (output_dialect := self.dialect) is None:
-            raise NotImplementedError(f"No known sqlglot dialect for backend {name}")
-
-        if dialect != output_dialect:
-            (query,) = sg.transpile(query, read=dialect, write=output_dialect)
-        return query
-
-
-@functools.cache
-def _get_backend_names(*, exclude: tuple[str] = ()) -> frozenset[str]:
-    """Return the set of known backend names.
-
-    Parameters
-    ----------
-    exclude
-        Exclude these backend names from the result
-
-    Notes
-    -----
-    This function returns a frozenset to prevent cache pollution.
+        return result
+
+cast.__doc__ = """
+Cast value(s) to indicated data type. Values that cannot be
+successfully casted
+
+Parameters
+----------
+target_type : data type name
+
+Notes
+-----
+{0}
+
+Returns
+-------
+cast_expr : ValueExpr
+""".format(_data_type_docs)
+
+
+def typeof(arg):
+    """
+    Return the data type of the argument according to the current backend
+
+    Returns
+    -------
+    typeof_arg : string
+    """
+    return _ops.TypeOf(arg).to_expr()
+
+
+def hash(arg, how='fnv'):
+    """
+    Compute an integer hash value for the indicated value expression.
+
+    Parameters
+    ----------
+    arg : value expression
+    how : {'fnv'}, default 'fnv'
+      Hash algorithm to use
+
+    Returns
+    -------
+    hash_value : int64 expression
+    """
+    return _ops.Hash(arg, how).to_expr()
+
+
+def fillna(arg, fill_value):
+    """
+    Replace any null values with the indicated fill value
+
+    Parameters
+    ----------
+    fill_value : scalar / array value or expression
+
+    Examples
+    --------
+    result = table.col.fillna(5)
+    result2 = table.col.fillna(table.other_col * 3)
+
+    Returns
+    -------
+    filled : type of caller
+    """
+    return _ops.IfNull(arg, fill_value).to_expr()
+
+
+def coalesce(*args):
+    """
+    Compute the first non-null value(s) from the passed arguments in
+    left-to-right order. This is also known as "combine_first" in pandas.
+
+    Parameters
+    ----------
+    *args : variable-length value list
+
+    Examples
+    --------
+    result = coalesce(expr1, expr2, 5)
+
+    Returns
+    -------
+    coalesced : type of first provided argument
+    """
+    return _ops.Coalesce(*args).to_expr()
+
+
+def greatest(*args):
+    """
+    Compute the largest value (row-wise, if any arrays are present) among the
+    supplied arguments.
+
+    Returns
+    -------
+    greatest : type depending on arguments
+    """
+    return _ops.Greatest(*args).to_expr()
+
+
+def least(*args):
+    """
+    Compute the smallest value (row-wise, if any arrays are present) among the
+    supplied arguments.
+
+    Returns
+    -------
+    least : type depending on arguments
+    """
+    return _ops.Least(*args).to_expr()
+
+
+def where(boolean_expr, true_expr, false_null_expr):
+    """
+    Equivalent to the ternary expression: if X then Y else Z
+
+    Parameters
+    ----------
+    boolean_expr : BooleanValue (array or scalar)
+    true_expr : value
+      Values for each True value
+    false_null_expr : value
+      Values for False or NULL values
+
+    Returns
+    -------
+    result : arity depending on inputs
+      Type of true_expr used to determine output type
+    """
+    op = _ops.Where(boolean_expr, true_expr, false_null_expr)
+    return op.to_expr()
+
+
+def over(expr, window):
+    """
+    Turn an aggregation or full-sample analytic operation into a windowed
+    operation. See ibis.window for more details on window configuration
+
+    Parameters
+    ----------
+    expr : value expression
+    window : ibis.Window
+
+    Returns
+    -------
+    expr : type of input
+    """
+    prior_op = expr.op()
+
+    if isinstance(prior_op, _ops.WindowOp):
+        op = prior_op.over(window)
+    else:
+        op = _ops.WindowOp(expr, window)
+
+    result = op.to_expr()
+
+    try:
+        result = result.name(expr.get_name())
+    except:
+        pass
+
+    return result
+
+
+def value_counts(arg, metric_name='count'):
+    """
+    Compute a frequency table for this value expression
+
+    Parameters
+    ----------
+
+    Returns
+    -------
+    counts : TableExpr
+      Aggregated table
+    """
+    base = ir.find_base_table(arg)
+    metric = base.count().name(metric_name)
+
+    try:
+        arg.get_name()
+    except _com.ExpressionError:
+        arg = arg.name('unnamed')
+
+    return base.group_by(arg).aggregate(metric)
+
+
+def nullif(value, null_if_expr):
+    """
+    Set values to null if they match/equal a particular expression (scalar or
+    array-valued).
+
+    Common use to avoid divide-by-zero problems (get NULL instead of INF on
+    divide-by-zero): 5 / expr.nullif(0)
+
+    Parameters
+    ----------
+    value : value expression
+      Value to modify
+    null_if_expr : value expression (array or scalar)
+
+    Returns
+    -------
+    null_if : type of caller
+    """
+    return _ops.NullIf(value, null_if_expr).to_expr()
+
+
+def between(arg, lower, upper):
+    """
+    Check if the input expr falls between the lower/upper bounds
+    passed. Bounds are inclusive. All arguments must be comparable.
+
+    Returns
+    -------
+    is_between : BooleanValue
+    """
+    lower = _ops.as_value_expr(lower)
+    upper = _ops.as_value_expr(upper)
+    op = _ops.Between(arg, lower, upper)
+    return op.to_expr()
+
+
+def isin(arg, values):
+    """
+    Check whether the value expression is contained within the indicated
+    list of values.
+
+    Parameters
+    ----------
+    values : list, tuple, or array expression
+      The values can be scalar or array-like. Each of them must be
+      comparable with the calling expression, or None (NULL).
+
+    Examples
+    --------
+    expr = table.strings.isin(['foo', 'bar', 'baz'])
+
+    expr2 = table.strings.isin(table2.other_string_col)
+
+    Returns
+    -------
+    contains : BooleanValue
+    """
+    op = _ops.Contains(arg, values)
+    return op.to_expr()
+
+
+def notin(arg, values):
+    """
+    Like isin, but checks whether this expression's value(s) are not
+    contained in the passed values. See isin docs for full usage.
+    """
+    op = _ops.NotContains(arg, values)
+    return op.to_expr()
+
+
+add = _binop_expr('__add__', _ops.Add)
+sub = _binop_expr('__sub__', _ops.Subtract)
+mul = _binop_expr('__mul__', _ops.Multiply)
+div = _binop_expr('__div__', _ops.Divide)
+floordiv = _binop_expr('__floordiv__', _ops.FloorDivide)
+pow = _binop_expr('__pow__', _ops.Power)
+mod = _binop_expr('__mod__', _ops.Modulus)
+
+rsub = _rbinop_expr('__rsub__', _ops.Subtract)
+rdiv = _rbinop_expr('__rdiv__', _ops.Divide)
+rfloordiv = _rbinop_expr('__rfloordiv__', _ops.FloorDivide)
+
+
+def substitute(arg, value, replacement=None, else_=None):
+    """
+    Substitute (replace) one or more values in a value expression
+
+    Parameters
+    ----------
+    value : expr-like or dict
+    replacement : expr-like, optional
+      If an expression is passed to value, this must be passed
+    else_ : expr, optional
+
+    Returns
+    -------
+    replaced : case statement (for now!)
+
+    """
+    expr = arg.case()
+    if isinstance(value, dict):
+        for k, v in sorted(value.items()):
+            expr = expr.when(k, v)
+    else:
+        expr = expr.when(value, replacement)
+
+    if else_ is not None:
+        expr = expr.else_(else_)
+    else:
+        expr = expr.else_(arg)
+
+    return expr.end()
+
+
+def _case(arg):
+    """
+    Create a new SimpleCaseBuilder to chain multiple if-else
+    statements. Add new search expressions with the .when method. These
+    must be comparable with this array expression. Conclude by calling
+    .end()
+
+    Examples
+    --------
+    case_expr = (expr.case()
+                 .when(case1, output1)
+                 .when(case2, output2)
+                 .default(default_output)
+                 .end())
+
+    Returns
+    -------
+    builder : CaseBuilder
+    """
+    return _ops.SimpleCaseBuilder(arg)
+
+
+def cases(arg, case_result_pairs, default=None):
+    """
+    Create a case expression in one shot.
+
+    Returns
+    -------
+    case_expr : SimpleCase
+    """
+    builder = arg.case()
+    for case, result in case_result_pairs:
+        builder = builder.when(case, result)
+    if default is not None:
+        builder = builder.else_(default)
+    return builder.end()
+
+
+_generic_value_methods = dict(
+    hash=hash,
+    cast=cast,
+    coalesce=coalesce,
+    typeof=typeof,
+    fillna=fillna,
+    nullif=nullif,
+    between=between,
+    isin=isin,
+    notin=notin,
+    isnull=_unary_op('isnull', _ops.IsNull),
+    notnull=_unary_op('notnull', _ops.NotNull),
+
+    over=over,
+
+    case=_case,
+    cases=cases,
+    substitute=substitute,
+
+    __add__=add,
+    add=add,
+
+    __sub__=sub,
+    sub=sub,
+
+    __mul__=mul,
+    mul=mul,
+
+    __div__=div,
+    __truediv__=div,
+    __floordiv__=floordiv,
+    div=div,
+    floordiv=floordiv,
+
+    __rdiv__=rdiv,
+    __rtruediv__=rdiv,
+    __rfloordiv__=rfloordiv,
+    rdiv=rdiv,
+    rfloordiv=rfloordiv,
+
+    __pow__=pow,
+    pow=pow,
+
+    __radd__=add,
+
+    __rsub__=rsub,
+    rsub=rsub,
+
+    __rmul__=_rbinop_expr('__rmul__', _ops.Multiply),
+    __rpow__=_binop_expr('__rpow__', _ops.Power),
+
+    __mod__=mod,
+    __rmod__=_rbinop_expr('__rmod__', _ops.Modulus),
+
+    __eq__=_binop_expr('__eq__', _ops.Equals),
+    __ne__=_binop_expr('__ne__', _ops.NotEquals),
+    __ge__=_binop_expr('__ge__', _ops.GreaterEqual),
+    __gt__=_binop_expr('__gt__', _ops.Greater),
+    __le__=_binop_expr('__le__', _ops.LessEqual),
+    __lt__=_binop_expr('__lt__', _ops.Less)
+)
+
+
+approx_nunique = _agg_function('approx_nunique', _ops.HLLCardinality, True)
+approx_median = _agg_function('approx_median', _ops.CMSMedian, True)
+max = _agg_function('max', _ops.Max, True)
+min = _agg_function('min', _ops.Min, True)
+
+
+def lag(arg, offset=None, default=None):
+    return _ops.Lag(arg, offset, default).to_expr()
+
+
+def lead(arg, offset=None, default=None):
+    return _ops.Lead(arg, offset, default).to_expr()
+
+
+first = _unary_op('first', _ops.FirstValue)
+last = _unary_op('last', _ops.LastValue)
+rank = _unary_op('rank', _ops.MinRank)
+dense_rank = _unary_op('dense_rank', _ops.DenseRank)
+cummin = _unary_op('cummin', _ops.CumulativeMin)
+cummax = _unary_op('cummax', _ops.CumulativeMax)
+
+
+def nth(arg, k):
+    """
+    Analytic operation computing nth value from start of sequence
+
+    Parameters
+    ----------
+    arg : array expression
+    k : int
+        Desired rank value
+
+    Returns
+    -------
+    nth : type of argument
+    """
+    return _ops.NthValue(arg, k).to_expr()
+
+
+def distinct(arg):
+    """
+    Compute set of unique values occurring in this array. Can not be used
+    in conjunction with other array expressions from the same context
+    (because it's a cardinality-modifying pseudo-reduction).
+    """
+    op = _ops.DistinctArray(arg)
+    return op.to_expr()
+
+
+def nunique(arg):
+    """
+    Shorthand for foo.distinct().count(); computing the number of unique
+    values in an array.
+    """
+    return _ops.CountDistinct(arg).to_expr()
+
+
+def topk(arg, k, by=None):
+    """
+    Produces
+
+    Returns
+    -------
+    topk : TopK filter expression
+    """
+    op = _ops.TopK(arg, k, by=by)
+    return op.to_expr()
+
+
+def bottomk(arg, k, by=None):
+    raise NotImplementedError
+
+
+def _generic_summary(arg, exact_nunique=False, prefix=None):
+    """
+    Compute a set of summary metrics from the input value expression
+
+    Parameters
+    ----------
+    arg : value expression
+    exact_nunique : boolean, default False
+      Compute the exact number of distinct values (slower)
+    prefix : string, default None
+      String prefix for metric names
+
+    Returns
+    -------
+    summary : (count, # nulls, nunique)
+    """
+    metrics = [
+        arg.count(),
+        arg.isnull().sum().name('nulls')
+    ]
+
+    if exact_nunique:
+        unique_metric = arg.nunique().name('uniques')
+    else:
+        unique_metric = arg.approx_nunique().name('uniques')
+
+    metrics.append(unique_metric)
+    return _wrap_summary_metrics(metrics, prefix)
+
+
+def _numeric_summary(arg, exact_nunique=False, prefix=None):
+    """
+    Compute a set of summary metrics from the input numeric value expression
+
+    Parameters
+    ----------
+    arg : numeric value expression
+    exact_nunique : boolean, default False
+    prefix : string, default None
+      String prefix for metric names
+
+    Returns
+    -------
+    summary : (count, # nulls, min, max, sum, mean, nunique)
+    """
+    metrics = [
+        arg.count(),
+        arg.isnull().sum().name('nulls'),
+        arg.min(),
+        arg.max(),
+        arg.sum(),
+        arg.mean()
+    ]
+
+    if exact_nunique:
+        unique_metric = arg.nunique().name('nunique')
+    else:
+        unique_metric = arg.approx_nunique().name('approx_nunique')
+
+    metrics.append(unique_metric)
+    return _wrap_summary_metrics(metrics, prefix)
+
+
+def _wrap_summary_metrics(metrics, prefix):
+    result = expr_list(metrics)
+    if prefix is not None:
+        result = result.prefix(prefix)
+    return result
+
+
+def expr_list(exprs):
+    for e in exprs:
+        e.get_name()
+    return ir.ExpressionList(exprs).to_expr()
+
+
+_generic_array_methods = dict(
+    bottomk=bottomk,
+    distinct=distinct,
+    nunique=nunique,
+    topk=topk,
+    summary=_generic_summary,
+    count=count,
+    min=min,
+    max=max,
+    approx_median=approx_median,
+    approx_nunique=approx_nunique,
+    group_concat=group_concat,
+    value_counts=value_counts,
+
+    first=first,
+    last=last,
+    dense_rank=dense_rank,
+    rank=rank,
+    # nth=nth,
+    lag=lag,
+    lead=lead,
+    cummin=cummin,
+    cummax=cummax,
+)
+
+
+_add_methods(ValueExpr, _generic_value_methods)
+_add_methods(ArrayExpr, _generic_array_methods)
+
+
+# ---------------------------------------------------------------------
+# Numeric API
+
+def round(arg, digits=None):
+    """
+    Round values either to integer or indicated number of decimal places.
+
+    Returns
+    -------
+    rounded : type depending on digits argument
+      digits None or 0
+        decimal types: decimal
+        other numeric types: bigint
+      digits nonzero
+        decimal types: decimal
+        other numeric types: double
+    """
+    op = _ops.Round(arg, digits)
+    return op.to_expr()
+
+
+def log(arg, base=None):
+    """
+    Perform the logarithm using a specified base
+
+    Parameters
+    ----------
+    base : number, default None
+      If None, base e is used
+
+    Returns
+    -------
+    logarithm : double type
+    """
+    op = _ops.Log(arg, base)
+    return op.to_expr()
+
+
+def _integer_to_timestamp(arg, unit='s'):
+    """
+    Convert integer UNIX timestamp (at some resolution) to a timestamp type
+
+    Parameters
+    ----------
+    unit : {'s', 'ms', 'us'}
+      Second (s), millisecond (ms), or microsecond (us) resolution
+
+    Returns
+    -------
+    timestamp : timestamp value expression
+    """
+    op = _ops.TimestampFromUNIX(arg, unit)
+    return op.to_expr()
+
+
+abs = _unary_op('abs', _ops.Abs)
+ceil = _unary_op('ceil', _ops.Ceil)
+exp = _unary_op('exp', _ops.Exp)
+floor = _unary_op('floor', _ops.Floor)
+log2 = _unary_op('log2', _ops.Log2)
+log10 = _unary_op('log10', _ops.Log10)
+ln = _unary_op('ln', _ops.Ln)
+sign = _unary_op('sign', _ops.Sign)
+sqrt = _unary_op('sqrt', _ops.Sqrt)
+
+
+_numeric_value_methods = dict(
+    __neg__=negate,
+    abs=abs,
+    ceil=ceil,
+    floor=floor,
+    sign=sign,
+    exp=exp,
+    sqrt=sqrt,
+    log=log,
+    ln=ln,
+    log2=log2,
+    log10=log10,
+    round=round,
+    nullifzero=_unary_op('nullifzero', _ops.NullIfZero),
+    zeroifnull=_unary_op('zeroifnull', _ops.ZeroIfNull),
+)
+
+
+def convert_base(arg, from_base, to_base):
+    """
+    Convert number (as integer or string) from one base to another
+
+    Parameters
+    ----------
+    arg : string or integer
+    from_base : integer
+    to_base : integer
+
+    Returns
+    -------
+    converted : string
+    """
+    return _ops.BaseConvert(arg, from_base, to_base).to_expr()
+
+
+_integer_value_methods = dict(
+    to_timestamp=_integer_to_timestamp,
+    convert_base=convert_base
+)
+
+
+mean = _agg_function('mean', _ops.Mean, True)
+cummean = _unary_op('cummean', _ops.CumulativeMean)
+
+sum = _agg_function('sum', _ops.Sum, True)
+cumsum = _unary_op('cumsum', _ops.CumulativeSum)
+
+
+def std(arg, where=None, how='sample'):
+    """
+    Compute standard deviation of numeric array
+
+    Parameters
+    ----------
+    how : {'sample', 'pop'}, default 'sample'
+
+    Returns
+    -------
+    stdev : double scalar
+    """
+    expr = _ops.StandardDev(arg, where, how).to_expr()
+    expr = expr.name('std')
+    return expr
+
+
+def variance(arg, where=None, how='sample'):
+    """
+    Compute standard deviation of numeric array
+
+    Parameters
+    ----------
+    how : {'sample', 'pop'}, default 'sample'
+
+    Returns
+    -------
+    stdev : double scalar
+    """
+    expr = _ops.Variance(arg, where, how).to_expr()
+    expr = expr.name('var')
+    return expr
+
+
+_numeric_array_methods = dict(
+    mean=mean,
+    cummean=cummean,
+
+    sum=sum,
+    cumsum=cumsum,
+
+    std=std,
+    var=variance,
+
+    bucket=bucket,
+    histogram=histogram,
+    summary=_numeric_summary,
+)
+
+_add_methods(NumericValue, _numeric_value_methods)
+_add_methods(IntegerValue, _integer_value_methods)
+
+_add_methods(NumericArray, _numeric_array_methods)
+
+
+# ----------------------------------------------------------------------
+# Boolean API
+
+
+# TODO: logical binary operators for BooleanValue
+
+
+def ifelse(arg, true_expr, false_expr):
+    """
+    Shorthand for implementing ternary expressions
+
+    bool_expr.ifelse(0, 1)
+    e.g., in SQL: CASE WHEN bool_expr THEN 0 else 1 END
+    """
+    # Result will be the result of promotion of true/false exprs. These
+    # might be conflicting types; same type resolution as case expressions
+    # must be used.
+    case = _ops.SearchedCaseBuilder()
+    return case.when(arg, true_expr).else_(false_expr).end()
+
+
+_boolean_value_methods = dict(
+    ifelse=ifelse,
+    __and__=_boolean_binary_op('__and__', _ops.And),
+    __or__=_boolean_binary_op('__or__', _ops.Or),
+    __xor__=_boolean_binary_op('__xor__', _ops.Xor),
+    __rand__=_boolean_binary_rop('__rand__', _ops.And),
+    __ror__=_boolean_binary_rop('__ror__', _ops.Or),
+    __rxor__=_boolean_binary_rop('__rxor__', _ops.Xor)
+)
+
+
+_boolean_array_methods = dict(
+    any=_unary_op('any', _ops.Any),
+    notany=_unary_op('notany', _ops.NotAny),
+    all=_unary_op('all', _ops.All),
+    notall=_unary_op('notany', _ops.NotAll),
+    cumany=_unary_op('cumany', _ops.CumulativeAny),
+    cumall=_unary_op('cumall', _ops.CumulativeAll)
+)
+
+
+_add_methods(BooleanValue, _boolean_value_methods)
+_add_methods(BooleanArray, _boolean_array_methods)
+
+
+# ---------------------------------------------------------------------
+# String API
+
+def _string_substr(self, start, length=None):
+    """
+    Pull substrings out of each string value by position and maximum
+    length.
+
+    Parameters
+    ----------
+    start : int
+      First character to start splitting, indices starting at 0 (like
+      Python)
+    length : int, optional
+      Maximum length of each substring. If not supplied, splits each string
+      to the end
+
+    Returns
+    -------
+    substrings : type of caller
+    """
+    op = _ops.Substring(self, start, length)
+    return op.to_expr()
+
+
+def _string_left(self, nchars):
+    """
+    Return left-most up to N characters from each string. Convenience
+    use of substr.
+
+    Returns
+    -------
+    substrings : type of caller
+    """
+    return self.substr(0, length=nchars)
+
+
+def _string_right(self, nchars):
+    """
+    Split up to nchars starting from end of each string.
+
+    Returns
+    -------
+    substrings : type of caller
+    """
+    return _ops.StrRight(self, nchars).to_expr()
+
+
+def repeat(self, n):
+    """
+    Returns the argument string repeated n times
+
+    Parameters
+    ----------
+    n : int
+
+    Returns
+    -------
+    result : string
+    """
+    return _ops.Repeat(self, n).to_expr()
+
+
+def _translate(self, from_str, to_str):
+    """
+    Returns string with set of 'from' characters replaced
+    by set of 'to' characters.
+    from_str[x] is replaced by to_str[x].
+    To avoid unexpected behavior, from_str should be
+    shorter than to_string.
+
+    Parameters
+    ----------
+    from_str : string
+    to_str : string
+
+    Examples
+    --------
+    expr = table.strings.translate('a', 'b')
+    expr = table.string.translate('a', 'bc')
+    Returns
+    -------
+    translated : string
+    """
+    return _ops.Translate(self, from_str, to_str).to_expr()
+
+
+def _string_find(self, substr, start=None, end=None):
+    """
+    Returns position (0 indexed) of first occurence of substring,
+    optionally after a particular position (0 indexed)
+
+    Parameters
+    ----------
+    substr : string
+    start : int, default None
+    end : int, default None
+        Not currently implemented
+
+    Returns
+    -------
+    position : int, 0 indexed
+    """
+    if end is not None:
+        raise NotImplementedError
+    return _ops.StringFind(self, substr, start, end).to_expr()
+
+
+def _lpad(self, length, pad=' '):
+    """
+    Returns string of given length by truncating (on right)
+    or padding (on left) original string
+
+    Parameters
+    ----------
+    length : int
+    pad : string, default is ' '
+
+    Examples
+    --------
+    table.strings.lpad(5, '-')
+    'a' becomes '----a'
+    'abcdefg' becomes 'abcde'
+
+    Returns
+    -------
+    padded : string
+    """
+    return _ops.LPad(self, length, pad).to_expr()
+
+
+def _rpad(self, length, pad=' '):
+    """
+    Returns string of given length by truncating (on right)
+    or padding (on right) original string
+
+    Parameters
+    ----------
+    length : int
+    pad : string, default is ' '
+
+    Examples
+    --------
+    table.strings.rpad(5, '-')
+    'a' becomes 'a----'
+    'abcdefg' becomes 'abcde'
+
+    Returns
+    -------
+    padded : string
+    """
+    return _ops.RPad(self, length, pad).to_expr()
+
+
+def _find_in_set(self, str_list):
+    """
+    Returns postion (0 indexed) of first occurence of argument within
+    a list of strings. No string in list can have a comma
+    Returns -1 if search string isn't found or if search string contains ','
+
+
+    Parameters
+    ----------
+    str_list : list of strings
+
+    Examples
+    --------
+    table.strings.find_in_set(['a', 'b'])
+
+    Returns
+    -------
+    position : int
+    """
+    return _ops.FindInSet(self, str_list).to_expr()
+
+
+def _string_join(self, strings):
+    """
+    Joins a list of strings together using the calling string as a separator
+
+    Parameters
+    ----------
+    strings : list of strings
+
+    Examples
+    --------
+    sep = ibis.literal(',')
+    sep.join(['a','b','c'])
+
+    Returns
+    -------
+    joined : string
+    """
+    return _ops.StringJoin(self, strings).to_expr()
+
+
+def _string_like(self, pattern):
+    """
+    Wildcard fuzzy matching function equivalent to the SQL LIKE directive. Use
+    % as a multiple-character wildcard or _ (underscore) as a single-character
+    wildcard.
+
+    Use re_search or rlike for regex-based matching.
+
+    Parameters
+    ----------
+    pattern : string
+
+    Returns
+    -------
+    matched : boolean value
+    """
+    return _ops.StringSQLLike(self, pattern).to_expr()
+
+
+def re_search(arg, pattern):
+    """
+    Search string values using a regular expression. Returns True if the regex
+    matches a string and False otherwise.
+
+    Parameters
+    ----------
+    pattern : string (regular expression string)
+
+    Returns
+    -------
+    searched : boolean value
+    """
+    return _ops.RegexSearch(arg, pattern).to_expr()
+
 
-    If a `set` is used, then any in-place modifications to the set
-    are visible to every caller of this function.
+def regex_extract(arg, pattern, index):
+    """
+    Returns specified index, 0 indexed, from string based on regex pattern
+    given
+
+    Parameters
+    ----------
+    pattern : string (regular expression string)
+    index : int, 0 indexed
+
+    Returns
+    -------
+    extracted : string
+    """
+    return _ops.RegexExtract(arg, pattern, index).to_expr()
+
+
+def regex_replace(arg, pattern, replacement):
+    """
+    Replaces match found by regex with replacement string.
+    Replacement string can also be a regex
+
+    Parameters
+    ----------
+    pattern : string (regular expression string)
+    replacement : string (can be regular expression string)
+
+    Examples
+    --------
+    table.strings.replace('(b+)', r'<\1>')
+    'aaabbbaa' becomes 'aaa<bbb>aaa'
 
+    Returns
+    -------
+    modified : string
     """
+    return _ops.RegexReplace(arg, pattern, replacement).to_expr()
+
+
+def _string_replace(arg, pattern, replacement):
+    """
+    Replaces each exactly occurrence of pattern with given replacement
+    string. Like Python built-in str.replace
+
+    Parameters
+    ----------
+    pattern : string
+    replacement : string
+
+    Examples
+    --------
+    table.strings.replace('aaa', 'foo')
+    'aaabbbaaa' becomes 'foobbbfoo'
+
+    Returns
+    -------
+    replaced : string
+    """
+    return _ops.StringReplace(arg, pattern, replacement).to_expr()
+
+
+def parse_url(arg, extract, key=None):
+    """
+    Returns the portion of a URL corresponding to a part specified
+    by 'extract'
+    Can optionally specify a key to retrieve an associated value
+    if extract parameter is 'QUERY'
+
+    Parameters
+    ----------
+    extract : one of {'PROTOCOL', 'HOST', 'PATH', 'REF',
+                'AUTHORITY', 'FILE', 'USERINFO', 'QUERY'}
+    key : string (optional)
+
+    Examples
+    --------
+    parse_url("https://www.youtube.com/watch?v=kEuEcWfewf8&t=10", 'QUERY', 'v')
+    yields 'kEuEcWfewf8'
+
+    Returns
+    -------
+    extracted : string
+    """
+    return _ops.ParseURL(arg, extract, key).to_expr()
+
+
+def _string_contains(arg, substr):
+    """
+    Determine if indicated string is exactly contained in the calling string.
+
+    Parameters
+    ----------
+    substr
 
-    if sys.version_info < (3, 10):
-        entrypoints = importlib.metadata.entry_points()["ibis.backends"]
+    Returns
+    -------
+    contains : boolean
+    """
+    return arg.like('%{0}%'.format(substr))
+
+
+def _string_dunder_contains(arg, substr):
+    raise TypeError('Use val.contains(arg)')
+
+
+def _string_getitem(self, key):
+    if isinstance(key, slice):
+        start, stop, step = key.start, key.stop, key.step
+        if step and step != 1:
+            raise ValueError('Step can only be 1')
+
+        start = start or 0
+
+        if start < 0 or stop < 0:
+            raise ValueError('negative slicing not yet supported')
+
+        return self.substr(start, stop - start)
     else:
-        entrypoints = importlib.metadata.entry_points(group="ibis.backends")
-    return frozenset(ep.name for ep in entrypoints).difference(exclude)
+        raise NotImplementedError
+
+
+_string_value_methods = dict(
+    __getitem__=_string_getitem,
+
+    length=_unary_op('length', _ops.StringLength),
+    lower=_unary_op('lower', _ops.Lowercase),
+    upper=_unary_op('upper', _ops.Uppercase),
+    reverse=_unary_op('reverse', _ops.Reverse),
+    ascii_str=_unary_op('ascii', _ops.StringAscii),
+    strip=_unary_op('strip', _ops.Strip),
+    lstrip=_unary_op('lstrip', _ops.LStrip),
+    rstrip=_unary_op('rstrip', _ops.RStrip),
+    capitalize=_unary_op('initcap', _ops.Capitalize),
+
+    convert_base=convert_base,
+
+    __contains__=_string_dunder_contains,
+    contains=_string_contains,
+    like=_string_like,
+    rlike=re_search,
+    replace=_string_replace,
+    re_search=re_search,
+    re_extract=regex_extract,
+    re_replace=regex_replace,
+    parse_url=parse_url,
+
+    substr=_string_substr,
+    left=_string_left,
+    right=_string_right,
+    repeat=repeat,
+    find=_string_find,
+    translate=_translate,
+    find_in_set=_find_in_set,
+    join=_string_join,
+    lpad=_lpad,
+    rpad=_rpad,
+)
+
+
+_add_methods(StringValue, _string_value_methods)
+
+
+# ---------------------------------------------------------------------
+# Timestamp API
+
+def _timestamp_truncate(arg, unit):
+    """
+    Zero out smaller-size units beyond indicated unit. Commonly used for time
+    series resampling.
+
+    Parameters
+    ----------
+    unit : string, one of below table
+      'Y': year
+      'Q': quarter
+      'M': month
+      'D': day
+      'W': week
+      'H': hour
+      'MI': minute
+
+    Returns
+    -------
+    truncated : timestamp
+    """
+    return _ops.Truncate(arg, unit).to_expr()
+
+
+def _timestamp_strftime(arg, format_str):
+    """
+    Format timestamp according to the passed format string. Format string may
+    depend on backend, but we try to conform to ANSI strftime (e.g. Python
+    built-in datetime.strftime)
+
+    Parameters
+    ----------
+    format_str : string
+
+    Returns
+    -------
+    formatted : string
+    """
+    return _ops.Strftime(arg, format_str).to_expr()
+
+
+_timestamp_value_methods = dict(
+    strftime=_timestamp_strftime,
+    year=_extract_field('year', _ops.ExtractYear),
+    month=_extract_field('month', _ops.ExtractMonth),
+    day=_extract_field('day', _ops.ExtractDay),
+    hour=_extract_field('hour', _ops.ExtractHour),
+    minute=_extract_field('minute', _ops.ExtractMinute),
+    second=_extract_field('second', _ops.ExtractSecond),
+    millisecond=_extract_field('millisecond', _ops.ExtractMillisecond),
+    truncate=_timestamp_truncate
+)
+
+
+_add_methods(TimestampValue, _timestamp_value_methods)
+
+
+# ---------------------------------------------------------------------
+# Decimal API
+
+_decimal_value_methods = dict(
+    precision=_unary_op('precision', _ops.DecimalPrecision),
+    scale=_unary_op('scale', _ops.DecimalScale),
+)
+
+
+_add_methods(DecimalValue, _decimal_value_methods)
+
+
+# ----------------------------------------------------------------------
+# Category API
+
+
+_category_value_methods = dict(
+    label=_analytics.category_label
+)
 
+_add_methods(CategoryValue, _category_value_methods)
 
-def connect(resource: Path | str, **kwargs: Any) -> BaseBackend:
-    """Connect to `resource`, inferring the backend automatically.
 
-    The general pattern for `ibis.connect` is
+# ---------------------------------------------------------------------
+# Table API
 
-    ```python
-    con = ibis.connect("backend://connection-parameters")
-    ```
+_join_classes = {
+    'inner': _ops.InnerJoin,
+    'left': _ops.LeftJoin,
+    'outer': _ops.OuterJoin,
+    'left_semi': _ops.LeftSemiJoin,
+    'semi': _ops.LeftSemiJoin,
+    'anti': _ops.LeftAntiJoin,
+    'cross': _ops.CrossJoin
+}
 
-    With many backends that looks like
 
-    ```python
-    con = ibis.connect("backend://user:password@host:port/database")
-    ```
+def join(left, right, predicates=(), how='inner'):
+    """
+    Perform a relational join between two tables. Does not resolve resulting
+    table schema.
+
+    Parameters
+    ----------
+    left : TableExpr
+    right : TableExpr
+    predicates : join expression(s)
+    how : string, default 'inner'
+      - 'inner': inner join
+      - 'left': left join
+      - 'outer': full outer join
+      - 'semi' or 'left_semi': left semi join
+      - 'anti': anti join
+
+    Returns
+    -------
+    joined : TableExpr
+      Note, schema is not materialized yet
+    """
+    klass = _join_classes[how.lower()]
+    if isinstance(predicates, Expr):
+        predicates = _L.unwrap_ands(predicates)
+
+    op = klass(left, right, predicates)
+    return TableExpr(op)
 
-    See the connection syntax for each backend for details about URL connection
-    requirements.
+
+def cross_join(*args, **kwargs):
+    """
+    Perform a cross join (cartesian product) amongst a list of tables, with
+    optional set of prefixes to apply to overlapping column names
 
     Parameters
     ----------
-    resource
-        A URL or path to the resource to be connected to.
-    kwargs
-        Backend specific keyword arguments
+    positional args: tables to join
+    prefixes keyword : prefixes for each table
+      Not yet implemented
 
     Examples
     --------
-    Connect to an in-memory DuckDB database:
+    >>> joined1 = ibis.cross_join(a, b, c, d, e)
+    >>> joined2 = ibis.cross_join(a, b, c, prefixes=['a_', 'b_', 'c_']))
+
+    Returns
+    -------
+    joined : TableExpr
+      If prefixes not provided, the result schema is not yet materialized
+    """
+    op = _ops.CrossJoin(*args, **kwargs)
+    return TableExpr(op)
+
 
-    >>> import ibis
-    >>> con = ibis.connect("duckdb://")
+def _table_count(self):
+    """
+    Returns the computed number of rows in the table expression
+
+    Returns
+    -------
+    count : Int64Scalar
+    """
+    return _ops.Count(self, None).to_expr().name('count')
 
-    Connect to an on-disk SQLite database:
 
-    >>> con = ibis.connect("sqlite://relative.db")
-    >>> con = ibis.connect(
-    ...     "sqlite:///absolute/path/to/data.db"
-    ... )  # quartodoc: +SKIP # doctest: +SKIP
+def _table_info(self, buf=None):
+    """
+    Similar to pandas DataFrame.info. Show column names, types, and null
+    counts. Output to stdout by default
+    """
+    metrics = [self.count().name('nrows')]
+    for col in self.columns:
+        metrics.append(self[col].count().name(col))
+
+    metrics = self.aggregate(metrics).execute().loc[0]
 
-    Connect to a PostgreSQL server:
+    names = ['Column', '------'] + self.columns
+    types = ['Type', '----'] + [repr(x) for x in self.schema().types]
+    counts = ['Non-null #', '----------'] + [str(x) for x in metrics[1:]]
+    col_metrics = util.adjoin(2, names, types, counts)
 
-    >>> con = ibis.connect(
-    ...     "postgres://user:password@hostname:5432"
-    ... )  # quartodoc: +SKIP # doctest: +SKIP
+    if buf is None:
+        import sys
+        buf = sys.stdout
 
-    Connect to BigQuery:
+    result = ('Table rows: {0}\n\n'
+              '{1}'
+              .format(metrics[0], col_metrics))
 
-    >>> con = ibis.connect(
-    ...     "bigquery://my-project/my-dataset"
-    ... )  # quartodoc: +SKIP # doctest: +SKIP
+    buf.write(result)
 
+
+def _table_set_column(table, name, expr):
     """
-    url = resource = str(resource)
+    Replace an existing column with a new expression
 
-    if re.match("[A-Za-z]:", url):
-        # windows path with drive, treat it as a file
-        url = f"file://{url}"
+    Parameters
+    ----------
+    name : string
+      Column name to replace
+    expr : value expression
+      New data for column
+
+    Returns
+    -------
+    set_table : TableExpr
+      New table expression
+    """
+    expr = table._ensure_expr(expr)
 
-    parsed = urllib.parse.urlparse(url)
-    scheme = parsed.scheme or "file"
+    if expr._name != name:
+        expr = expr.name(name)
 
-    orig_kwargs = kwargs.copy()
-    kwargs = dict(urllib.parse.parse_qsl(parsed.query))
+    if name not in table:
+        raise KeyError('{0} is not in the table'.format(name))
 
-    if scheme == "file":
-        path = parsed.netloc + parsed.path
-        # Merge explicit kwargs with query string, explicit kwargs
-        # taking precedence
-        kwargs.update(orig_kwargs)
-        if path.endswith(".duckdb"):
-            return ibis.duckdb.connect(path, **kwargs)
-        elif path.endswith((".sqlite", ".db")):
-            return ibis.sqlite.connect(path, **kwargs)
-        elif path.endswith((".parquet", ".csv", ".csv.gz")):
-            # Load parquet/csv/csv.gz files with duckdb by default
-            con = ibis.duckdb.connect(**kwargs)
-            con.register(path)
-            return con
+    # TODO: This assumes that projection is required; may be backend-dependent
+    proj_exprs = []
+    for key in table.columns:
+        if key == name:
+            proj_exprs.append(expr)
         else:
-            raise ValueError(f"Don't know how to connect to {resource!r}")
+            proj_exprs.append(table[key])
 
-    if kwargs:
-        # If there are kwargs (either explicit or from the query string),
-        # re-add them to the parsed URL
-        query = urllib.parse.urlencode(kwargs)
-        parsed = parsed._replace(query=query)
-
-    if scheme in ("postgres", "postgresql"):
-        # Treat `postgres://` and `postgresql://` the same
-        scheme = "postgres"
-
-    # Convert all arguments back to a single URL string
-    url = parsed.geturl()
-    if "://" not in url:
-        # urllib may roundtrip `duckdb://` to `duckdb:`. Here we re-add the
-        # missing `//`.
-        url = url.replace(":", "://", 1)
+    return table.projection(proj_exprs)
 
-    try:
-        backend = getattr(ibis, scheme)
-    except AttributeError:
-        raise ValueError(f"Don't know how to connect to {resource!r}") from None
-
-    return backend._from_url(url, **orig_kwargs)
-
-
-class UrlFromPath:
-    __slots__ = ()
-
-    def _from_url(self, url: str, **kwargs) -> BaseBackend:
-        """Connect to a backend using a URL `url`.
-
-        Parameters
-        ----------
-        url
-            URL with which to connect to a backend.
-        kwargs
-            Additional keyword arguments
-
-        Returns
-        -------
-        BaseBackend
-            A backend instance
-
-        """
-        url = urlparse(url)
-        netloc = url.netloc
-        parts = list(filter(None, (netloc, url.path[bool(netloc) :])))
-        database = Path(*parts) if parts and parts != [":memory:"] else ":memory:"
-        if (strdatabase := str(database)).startswith("md:") or strdatabase.startswith(
-            "motherduck:"
-        ):
-            database = strdatabase
-        elif isinstance(database, Path):
-            database = database.absolute()
-
-        query_params = parse_qs(url.query)
-
-        for name, value in query_params.items():
-            if len(value) > 1:
-                kwargs[name] = value
-            elif len(value) == 1:
-                kwargs[name] = value[0]
+
+def _regular_join_method(name, how, doc=None):
+    def f(self, other, predicates=()):
+        return self.join(other, predicates, how=how)
+    if doc:
+        f.__doc__ = doc
+    else:
+        # XXX
+        f.__doc__ = join.__doc__
+    f.__name__ = name
+    return f
+
+
+def filter(table, predicates):
+    """
+    Select rows from table based on boolean expressions
+
+    Parameters
+    ----------
+    predicates : boolean array expressions, or list thereof
+
+    Returns
+    -------
+    filtered_expr : TableExpr
+    """
+    if isinstance(predicates, Expr):
+        predicates = _L.unwrap_ands(predicates)
+    predicates = util.promote_list(predicates)
+
+    predicates = [ir.bind_expr(table, x) for x in predicates]
+
+    resolved_predicates = []
+    for pred in predicates:
+        if isinstance(pred, ir.AnalyticExpr):
+            pred = pred.to_filter()
+        resolved_predicates.append(pred)
+
+    op = _L.apply_filter(table, resolved_predicates)
+    return TableExpr(op)
+
+
+def aggregate(table, metrics=None, by=None, having=None, **kwds):
+    """
+    Aggregate a table with a given set of reductions, with grouping
+    expressions, and post-aggregation filters.
+
+    Parameters
+    ----------
+    table : table expression
+    metrics : expression or expression list
+    by : optional, default None
+      Grouping expressions
+    having : optional, default None
+      Post-aggregation filters
+
+    Returns
+    -------
+    agg_expr : TableExpr
+    """
+    if metrics is None:
+        metrics = []
+
+    for k, v in sorted(kwds.items()):
+        v = table._ensure_expr(v)
+        metrics.append(v.name(k))
+
+    op = _ops.Aggregation(table, metrics, by=by, having=having)
+    return TableExpr(op)
+
+
+def _table_distinct(self):
+    """
+    Compute set of unique rows/tuples occurring in this table
+    """
+    op = _ops.Distinct(self)
+    return op.to_expr()
+
+
+def _table_limit(table, n, offset=0):
+    """
+    Select the first n rows at beginning of table (may not be deterministic
+    depending on implementatino and presence of a sorting).
+
+    Parameters
+    ----------
+    n : int
+      Rows to include
+    offset : int, default 0
+      Number of rows to skip first
+
+    Returns
+    -------
+    limited : TableExpr
+    """
+    op = _ops.Limit(table, n, offset=offset)
+    return TableExpr(op)
+
+
+def _table_sort_by(table, sort_exprs):
+    """
+    Sort table by the indicated column expressions and sort orders
+    (ascending/descending)
+
+    Parameters
+    ----------
+    sort_exprs : sorting expressions
+      Must be one of:
+        - Column name or expression
+        - Sort key, e.g. desc(col)
+        - (column name, True (ascending) / False (descending))
+
+    Examples
+    --------
+    sorted = table.sort_by([('a', True), ('b', False)])
+
+    Returns
+    -------
+    sorted : TableExpr
+    """
+    op = _ops.SortBy(table, sort_exprs)
+    return TableExpr(op)
+
+
+def _table_union(left, right, distinct=False):
+    """
+    Form the table set union of two table expressions having identical
+    schemas.
+
+    Parameters
+    ----------
+    right : TableExpr
+    distinct : boolean, default False
+        Only union distinct rows not occurring in the calling table (this
+        can be very expensive, be careful)
+
+    Returns
+    -------
+    union : TableExpr
+    """
+    op = _ops.Union(left, right, distinct=distinct)
+    return TableExpr(op)
+
+
+def _table_to_array(self):
+    """
+    Single column tables can be viewed as arrays.
+    """
+    op = _ops.TableArrayView(self)
+    return op.to_expr()
+
+
+def _table_materialize(table):
+    """
+    Force schema resolution for a joined table, selecting all fields from
+    all tables.
+    """
+    if table._is_materialized():
+        return table
+    else:
+        op = _ops.MaterializedJoin(table)
+        return TableExpr(op)
+
+
+def mutate(table, exprs=None, **kwds):
+    """
+    Convenience function for table projections involving adding columns
+
+    Parameters
+    ----------
+    exprs : list, default None
+      List of named expressions to add as columns
+    kwds : keywords for new columns
+
+    Examples
+    --------
+    expr = table.mutate(qux=table.foo + table.bar, baz=5)
+
+    Returns
+    -------
+    mutated : TableExpr
+    """
+    if exprs is None:
+        exprs = []
+    else:
+        exprs = util.promote_list(exprs)
+
+    for k, v in sorted(kwds.items()):
+        if util.is_function(v):
+            v = v(table)
+        else:
+            v = as_value_expr(v)
+        exprs.append(v.name(k))
+
+    has_replacement = False
+    for expr in exprs:
+        if expr.get_name() in table:
+            has_replacement = True
+
+    if has_replacement:
+        by_name = dict((x.get_name(), x) for x in exprs)
+        used = set()
+        proj_exprs = []
+        for c in table.columns:
+            if c in by_name:
+                proj_exprs.append(by_name[c])
+                used.add(c)
             else:
-                raise exc.IbisError(f"Invalid URL parameter: {name}")
+                proj_exprs.append(c)
+
+        for x in exprs:
+            if x.get_name() not in used:
+                proj_exprs.append(x)
+
+        return table.projection(proj_exprs)
+    else:
+        return table.projection([table] + exprs)
+
+
+def projection(table, exprs):
+    """
+    Compute new table expression with the indicated column expressions from
+    this table.
+
+    Parameters
+    ----------
+    exprs : column expression, or string, or list of column expressions and
+      strings. If strings passed, must be columns in the table already
+
+    Returns
+    -------
+    projection : TableExpr
+    """
+    import ibis.expr.analysis as L
+
+    if isinstance(exprs, (Expr,) + six.string_types):
+        exprs = [exprs]
+
+    exprs = [table._ensure_expr(e) for e in exprs]
+    op = L.Projector(table, exprs).get_result()
+    return TableExpr(op)
+
+
+def _table_relabel(table, substitutions, replacements=None):
+    """
+    Change table column names, otherwise leaving table unaltered
 
-        self._convert_kwargs(kwargs)
-        return self.connect(database=database, **kwargs)
+    Parameters
+    ----------
+    substitutions
+
+    Returns
+    -------
+    relabeled : TableExpr
+    """
+    if replacements is not None:
+        raise NotImplementedError
 
+    observed = set()
+
+    exprs = []
+    for c in table.columns:
+        expr = table[c]
+        if c in substitutions:
+            expr = expr.name(substitutions[c])
+            observed.add(c)
+        exprs.append(expr)
+
+    for c in substitutions:
+        if c not in observed:
+            raise KeyError('{0!r} is not an existing column'.format(c))
+
+    return table.projection(exprs)
+
+
+def _table_view(self):
+    """
+    Create a new table expression that is semantically equivalent to the
+    current one, but is considered a distinct relation for evaluation
+    purposes (e.g. in SQL).
+
+    For doing any self-referencing operations, like a self-join, you will
+    use this operation to create a reference to the current table
+    expression.
+
+    Returns
+    -------
+    expr : TableExpr
+    """
+    return TableExpr(_ops.SelfReference(self))
+
+
+def _table_drop(self, fields):
+    if len(fields) == 0:
+        # noop
+        return self
+
+    fields = set(fields)
+    to_project = []
+    for name in self.schema():
+        if name in fields:
+            fields.remove(name)
+        else:
+            to_project.append(name)
 
-class NoUrl:
-    __slots__ = ()
+    if len(fields) > 0:
+        raise KeyError('Fields not in table: {0!s}'.format(fields))
 
-    name: str
+    return self.projection(to_project)
 
-    def _from_url(self, url: str, **kwargs) -> BaseBackend:
-        """Connect to the backend with empty url.
 
-        Parameters
-        ----------
-        url : str
-            The URL with which to connect to the backend. This parameter is not used
-            in this method but is kept for consistency.
-        kwargs
-            Additional keyword arguments.
+_table_methods = dict(
+    aggregate=aggregate,
+    count=_table_count,
+    distinct=_table_distinct,
+    drop=_table_drop,
+    info=_table_info,
+    limit=_table_limit,
+    set_column=_table_set_column,
+    filter=filter,
+    materialize=_table_materialize,
+    mutate=mutate,
+    projection=projection,
+    select=projection,
+    relabel=_table_relabel,
+    join=join,
+    cross_join=cross_join,
+    inner_join=_regular_join_method('inner_join', 'inner'),
+    left_join=_regular_join_method('left_join', 'left'),
+    outer_join=_regular_join_method('outer_join', 'outer'),
+    semi_join=_regular_join_method('semi_join', 'semi'),
+    anti_join=_regular_join_method('anti_join', 'anti'),
+    sort_by=_table_sort_by,
+    to_array=_table_to_array,
+    union=_table_union,
+    view=_table_view
+)
 
-        Returns
-        -------
-        BaseBackend
-            A backend instance
 
-        """
-        return self.connect(**kwargs)
+_add_methods(TableExpr, _table_methods)
```

### Comparing `ibis_framework-9.0.0.dev686/ibis/backends/bigquery/compiler.py` & `ibis-framework-v0.6.0/ibis/impala/ddl.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,698 +1,811 @@
-"""Module to convert from Ibis expression to SQL string."""
-
-from __future__ import annotations
+# Copyright 2014 Cloudera Inc.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+# http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
+from ibis.compat import StringIO
 import re
 
-import sqlglot as sg
-import sqlglot.expressions as sge
-from sqlglot.dialects import BigQuery
-
-import ibis.common.exceptions as com
-import ibis.expr.datatypes as dt
-import ibis.expr.operations as ops
-from ibis import util
-from ibis.backends.sql.compiler import NULL, STAR, SQLGlotCompiler
-from ibis.backends.sql.datatypes import BigQueryType, BigQueryUDFType
-from ibis.backends.sql.rewrites import (
-    exclude_unsupported_window_frame_from_ops,
-    exclude_unsupported_window_frame_from_rank,
-    exclude_unsupported_window_frame_from_row_number,
-    rewrite_sample_as_filter,
-)
-from ibis.common.temporal import DateUnit, IntervalUnit, TimestampUnit, TimeUnit
-from ibis.expr.rewrites import rewrite_stringslice
-
-_NAME_REGEX = re.compile(r'[^!"$()*,./;?@[\\\]^`{}~\n]+')
-
-
-class BigQueryCompiler(SQLGlotCompiler):
-    dialect = BigQuery
-    type_mapper = BigQueryType
-    udf_type_mapper = BigQueryUDFType
-    rewrites = (
-        rewrite_sample_as_filter,
-        exclude_unsupported_window_frame_from_ops,
-        exclude_unsupported_window_frame_from_row_number,
-        exclude_unsupported_window_frame_from_rank,
-        rewrite_stringslice,
-        *SQLGlotCompiler.rewrites,
-    )
-
-    UNSUPPORTED_OPERATIONS = frozenset(
-        (
-            ops.CountDistinctStar,
-            ops.DateDiff,
-            ops.ExtractAuthority,
-            ops.ExtractFile,
-            ops.ExtractFragment,
-            ops.ExtractHost,
-            ops.ExtractPath,
-            ops.ExtractProtocol,
-            ops.ExtractQuery,
-            ops.ExtractUserInfo,
-            ops.FindInSet,
-            ops.Median,
-            ops.Quantile,
-            ops.MultiQuantile,
-            ops.RegexSplit,
-            ops.RowID,
-            ops.TimestampBucket,
-            ops.TimestampDiff,
-        )
-    )
-
-    NAN = sge.Cast(
-        this=sge.convert("NaN"), to=sge.DataType(this=sge.DataType.Type.DOUBLE)
-    )
-    POS_INF = sge.Cast(
-        this=sge.convert("Infinity"), to=sge.DataType(this=sge.DataType.Type.DOUBLE)
-    )
-    NEG_INF = sge.Cast(
-        this=sge.convert("-Infinity"), to=sge.DataType(this=sge.DataType.Type.DOUBLE)
-    )
-
-    SIMPLE_OPS = {
-        ops.Arbitrary: "any_value",
-        ops.StringAscii: "ascii",
-        ops.BitAnd: "bit_and",
-        ops.BitOr: "bit_or",
-        ops.BitXor: "bit_xor",
-        ops.DateFromYMD: "date",
-        ops.Divide: "ieee_divide",
-        ops.EndsWith: "ends_with",
-        ops.GeoArea: "st_area",
-        ops.GeoAsBinary: "st_asbinary",
-        ops.GeoAsText: "st_astext",
-        ops.GeoAzimuth: "st_azimuth",
-        ops.GeoBuffer: "st_buffer",
-        ops.GeoCentroid: "st_centroid",
-        ops.GeoContains: "st_contains",
-        ops.GeoCoveredBy: "st_coveredby",
-        ops.GeoCovers: "st_covers",
-        ops.GeoDWithin: "st_dwithin",
-        ops.GeoDifference: "st_difference",
-        ops.GeoDisjoint: "st_disjoint",
-        ops.GeoDistance: "st_distance",
-        ops.GeoEndPoint: "st_endpoint",
-        ops.GeoEquals: "st_equals",
-        ops.GeoGeometryType: "st_geometrytype",
-        ops.GeoIntersection: "st_intersection",
-        ops.GeoIntersects: "st_intersects",
-        ops.GeoLength: "st_length",
-        ops.GeoMaxDistance: "st_maxdistance",
-        ops.GeoNPoints: "st_numpoints",
-        ops.GeoPerimeter: "st_perimeter",
-        ops.GeoPoint: "st_geogpoint",
-        ops.GeoPointN: "st_pointn",
-        ops.GeoStartPoint: "st_startpoint",
-        ops.GeoTouches: "st_touches",
-        ops.GeoUnaryUnion: "st_union_agg",
-        ops.GeoUnion: "st_union",
-        ops.GeoWithin: "st_within",
-        ops.GeoX: "st_x",
-        ops.GeoY: "st_y",
-        ops.Hash: "farm_fingerprint",
-        ops.IsInf: "is_inf",
-        ops.IsNan: "is_nan",
-        ops.Log10: "log10",
-        ops.LPad: "lpad",
-        ops.RPad: "rpad",
-        ops.Levenshtein: "edit_distance",
-        ops.Modulus: "mod",
-        ops.RegexReplace: "regexp_replace",
-        ops.RegexSearch: "regexp_contains",
-        ops.Time: "time",
-        ops.TimeFromHMS: "time",
-        ops.TimestampFromYMDHMS: "datetime",
-        ops.TimestampNow: "current_timestamp",
-    }
-
-    def _aggregate(self, funcname: str, *args, where):
-        func = self.f[funcname]
-
-        if where is not None:
-            args = tuple(self.if_(where, arg, NULL) for arg in args)
-
-        return func(*args, dialect=self.dialect)
-
-    @staticmethod
-    def _minimize_spec(start, end, spec):
-        if (
-            start is None
-            and isinstance(getattr(end, "value", None), ops.Literal)
-            and end.value.value == 0
-            and end.following
-        ):
-            return None
-        return spec
-
-    def visit_BoundingBox(self, op, *, arg):
-        name = type(op).__name__[len("Geo") :].lower()
-        return sge.Dot(
-            this=self.f.st_boundingbox(arg), expression=sg.to_identifier(name)
-        )
-
-    visit_GeoXMax = visit_GeoXMin = visit_GeoYMax = visit_GeoYMin = visit_BoundingBox
-
-    def visit_GeoSimplify(self, op, *, arg, tolerance, preserve_collapsed):
-        if (
-            not isinstance(op.preserve_collapsed, ops.Literal)
-            or op.preserve_collapsed.value
-        ):
-            raise com.UnsupportedOperationError(
-                "BigQuery simplify does not support preserving collapsed geometries, "
-                "pass preserve_collapsed=False"
-            )
-        return self.f.st_simplify(arg, tolerance)
-
-    def visit_ApproxMedian(self, op, *, arg, where):
-        return self.agg.approx_quantiles(arg, 2, where=where)[self.f.offset(1)]
-
-    def visit_Pi(self, op):
-        return self.f.acos(-1)
-
-    def visit_E(self, op):
-        return self.f.exp(1)
-
-    def visit_TimeDelta(self, op, *, left, right, part):
-        return self.f.time_diff(left, right, part, dialect=self.dialect)
-
-    def visit_DateDelta(self, op, *, left, right, part):
-        return self.f.date_diff(left, right, part, dialect=self.dialect)
-
-    def visit_TimestampDelta(self, op, *, left, right, part):
-        left_tz = op.left.dtype.timezone
-        right_tz = op.right.dtype.timezone
-
-        if left_tz is None and right_tz is None:
-            return self.f.datetime_diff(left, right, part)
-        elif left_tz is not None and right_tz is not None:
-            return self.f.timestamp_diff(left, right, part)
-
-        raise com.UnsupportedOperationError(
-            "timestamp difference with mixed timezone/timezoneless values is not implemented"
-        )
-
-    def visit_GroupConcat(self, op, *, arg, sep, where):
-        if where is not None:
-            arg = self.if_(where, arg, NULL)
-        return self.f.string_agg(arg, sep)
-
-    def visit_FloorDivide(self, op, *, left, right):
-        return self.cast(self.f.floor(self.f.ieee_divide(left, right)), op.dtype)
-
-    def visit_Log2(self, op, *, arg):
-        return self.f.log(arg, 2, dialect=self.dialect)
-
-    def visit_Log(self, op, *, arg, base):
-        if base is None:
-            return self.f.ln(arg)
-        return self.f.log(arg, base, dialect=self.dialect)
-
-    def visit_ArrayRepeat(self, op, *, arg, times):
-        start = step = 1
-        array_length = self.f.array_length(arg)
-        stop = self.f.greatest(times, 0) * array_length
-        i = sg.to_identifier("i")
-        idx = self.f.coalesce(
-            self.f.nullif(self.f.mod(i, array_length), 0), array_length
-        )
-        series = self.f.generate_array(start, stop, step)
-        return self.f.array(
-            sg.select(arg[self.f.safe_ordinal(idx)]).from_(self._unnest(series, as_=i))
-        )
-
-    def visit_NthValue(self, op, *, arg, nth):
-        if not isinstance(op.nth, ops.Literal):
-            raise com.UnsupportedOperationError(
-                f"BigQuery `nth` must be a literal; got {type(op.nth)}"
-            )
-        return self.f.nth_value(arg, nth)
-
-    def visit_StrRight(self, op, *, arg, nchars):
-        return self.f.substr(arg, -self.f.least(self.f.length(arg), nchars))
-
-    def visit_StringJoin(self, op, *, arg, sep):
-        return self.f.array_to_string(self.f.array(*arg), sep)
-
-    def visit_DayOfWeekIndex(self, op, *, arg):
-        return self.f.mod(self.f.extract(self.v.dayofweek, arg) + 5, 7)
-
-    def visit_DayOfWeekName(self, op, *, arg):
-        return self.f.initcap(sge.Cast(this=arg, to="STRING FORMAT 'DAY'"))
-
-    def visit_StringToTimestamp(self, op, *, arg, format_str):
-        if (timezone := op.dtype.timezone) is not None:
-            return self.f.parse_timestamp(format_str, arg, timezone)
-        return self.f.parse_datetime(format_str, arg)
-
-    def visit_ArrayCollect(self, op, *, arg, where):
-        if where is not None:
-            arg = self.if_(where, arg, NULL)
-        return self.f.array_agg(sge.IgnoreNulls(this=arg))
-
-    def _neg_idx_to_pos(self, arg, idx):
-        return self.if_(idx < 0, self.f.array_length(arg) + idx, idx)
-
-    def visit_ArraySlice(self, op, *, arg, start, stop):
-        index = sg.to_identifier("bq_arr_slice")
-        cond = [index >= self._neg_idx_to_pos(arg, start)]
-
-        if stop is not None:
-            cond.append(index < self._neg_idx_to_pos(arg, stop))
-
-        el = sg.to_identifier("el")
-        return self.f.array(
-            sg.select(el).from_(self._unnest(arg, as_=el, offset=index)).where(*cond)
-        )
-
-    def visit_ArrayIndex(self, op, *, arg, index):
-        return arg[self.f.safe_offset(index)]
-
-    def visit_ArrayContains(self, op, *, arg, other):
-        name = sg.to_identifier(util.gen_name("bq_arr_contains"))
-        return sge.Exists(
-            this=sg.select(sge.convert(1))
-            .from_(self._unnest(arg, as_=name))
-            .where(name.eq(other))
-        )
-
-    def visit_StringContains(self, op, *, haystack, needle):
-        return self.f.strpos(haystack, needle) > 0
-
-    def visti_StringFind(self, op, *, arg, substr, start, end):
-        if start is not None:
-            raise NotImplementedError(
-                "`start` not implemented for BigQuery string find"
-            )
-        if end is not None:
-            raise NotImplementedError("`end` not implemented for BigQuery string find")
-        return self.f.strpos(arg, substr)
-
-    def visit_NonNullLiteral(self, op, *, value, dtype):
-        if dtype.is_inet() or dtype.is_macaddr():
-            return sge.convert(str(value))
-        elif dtype.is_timestamp():
-            funcname = "datetime" if dtype.timezone is None else "timestamp"
-            return self.f[funcname](value.isoformat())
-        elif dtype.is_date():
-            return self.f.datefromparts(value.year, value.month, value.day)
-        elif dtype.is_time():
-            return self.f.time(value.hour, value.minute, value.second)
-        elif dtype.is_binary():
-            return sge.Cast(
-                this=sge.convert(value.hex()),
-                to=sge.DataType(this=sge.DataType.Type.BINARY),
-                format=sge.convert("HEX"),
-            )
-        elif dtype.is_interval():
-            if dtype.unit == IntervalUnit.NANOSECOND:
-                raise com.UnsupportedOperationError(
-                    "BigQuery does not support nanosecond intervals"
-                )
-        elif dtype.is_uuid():
-            return sge.convert(str(value))
-        return None
+from ibis.sql.compiler import DDL
+from .compiler import quote_identifier, _type_to_sql_string
+
+from ibis.expr.datatypes import validate_type
+from ibis.compat import py_string
+import ibis.expr.rules as rules
+
+
+fully_qualified_re = re.compile("(.*)\.(?:`(.*)`|(.*))")
+
+
+def _is_fully_qualified(x):
+    m = fully_qualified_re.search(x)
+    return bool(m)
 
-    def visit_IntervalFromInteger(self, op, *, arg, unit):
-        if unit == IntervalUnit.NANOSECOND:
-            raise com.UnsupportedOperationError(
-                "BigQuery does not support nanosecond intervals"
-            )
-        return sge.Interval(this=arg, unit=self.v[unit.singular])
-
-    def visit_Strftime(self, op, *, arg, format_str):
-        arg_dtype = op.arg.dtype
-        if arg_dtype.is_timestamp():
-            if (timezone := arg_dtype.timezone) is None:
-                return self.f.format_datetime(format_str, arg)
+
+def _is_quoted(x):
+    regex = re.compile("(?:`(.*)`|(.*))")
+    quoted, unquoted = regex.match(x).groups()
+    return quoted is not None
+
+
+class ImpalaDDL(DDL):
+
+    def _get_scoped_name(self, obj_name, database):
+        if database:
+            scoped_name = '{0}.`{1}`'.format(database, obj_name)
+        else:
+            if not _is_fully_qualified(obj_name):
+                if _is_quoted(obj_name):
+                    return obj_name
+                else:
+                    return '`{0}`'.format(obj_name)
             else:
-                return self.f.format_timestamp(format_str, arg, timezone)
-        elif arg_dtype.is_date():
-            return self.f.format_date(format_str, arg)
+                return obj_name
+        return scoped_name
+
+
+class CreateDDL(ImpalaDDL):
+
+    def _if_exists(self):
+        return 'IF NOT EXISTS ' if self.can_exist else ''
+
+
+_format_aliases = {
+    'TEXT': 'TEXTFILE'
+}
+
+
+def _sanitize_format(format):
+    if format is None:
+        return
+    format = format.upper()
+    format = _format_aliases.get(format, format)
+    if format not in ('PARQUET', 'AVRO', 'TEXTFILE'):
+        raise ValueError('Invalid format: {0}'.format(format))
+
+    return format
+
+
+def _format_properties(props):
+    tokens = []
+    for k, v in sorted(props.items()):
+        tokens.append("'{0!s}'='{1!s}'".format(k, v))
+
+    return '({0})'.format(', '.join(tokens))
+
+
+class CreateTable(CreateDDL):
+
+    """
+
+    Parameters
+    ----------
+    partition :
+
+    """
+
+    def __init__(self, table_name, database=None, external=False,
+                 format='parquet', can_exist=False,
+                 partition=None, path=None):
+        self.table_name = table_name
+        self.database = database
+        self.partition = partition
+        self.path = path
+        self.external = external
+        self.can_exist = can_exist
+        self.format = _sanitize_format(format)
+
+    def _create_line(self):
+        scoped_name = self._get_scoped_name(self.table_name, self.database)
+
+        if self.external:
+            create_decl = 'CREATE EXTERNAL TABLE'
         else:
-            assert arg_dtype.is_time(), arg_dtype
-            return self.f.format_time(format_str, arg)
+            create_decl = 'CREATE TABLE'
 
-    def visit_IntervalMultiply(self, op, *, left, right):
-        unit = self.v[op.left.dtype.resolution.upper()]
-        return sge.Interval(this=self.f.extract(unit, left) * right, unit=unit)
-
-    def visit_TimestampFromUNIX(self, op, *, arg, unit):
-        unit = op.unit
-        if unit == TimestampUnit.SECOND:
-            return self.f.timestamp_seconds(arg)
-        elif unit == TimestampUnit.MILLISECOND:
-            return self.f.timestamp_millis(arg)
-        elif unit == TimestampUnit.MICROSECOND:
-            return self.f.timestamp_micros(arg)
-        elif unit == TimestampUnit.NANOSECOND:
-            return self.f.timestamp_micros(
-                self.cast(self.f.round(arg / 1_000), dt.int64)
-            )
+        create_line = '{0} {1}{2}'.format(create_decl, self._if_exists(),
+                                          scoped_name)
+        return create_line
+
+    def _location(self):
+        if self.path:
+            return "\nLOCATION '{0}'".format(self.path)
+        return ''
+
+    def _storage(self):
+        storage_lines = {
+            'PARQUET': '\nSTORED AS PARQUET',
+            'AVRO': '\nSTORED AS AVRO'
+        }
+        return storage_lines[self.format]
+
+
+class CTAS(CreateTable):
+
+    """
+    Create Table As Select
+    """
+
+    def __init__(self, table_name, select, database=None,
+                 external=False, format='parquet', can_exist=False,
+                 path=None):
+        self.select = select
+        CreateTable.__init__(self, table_name, database=database,
+                             external=external, format=format,
+                             can_exist=can_exist, path=path)
+
+    def compile(self):
+        buf = StringIO()
+        buf.write(self._create_line())
+        buf.write(self._storage())
+        buf.write(self._location())
+
+        select_query = self.select.compile()
+        buf.write('\nAS\n{0}'.format(select_query))
+        return buf.getvalue()
+
+
+class CreateView(CreateDDL):
+
+    """
+    Create Table As Select
+    """
+
+    def __init__(self, name, select, database=None, can_exist=False):
+        self.name = name
+        self.database = database
+        self.select = select
+        self.can_exist = can_exist
+
+    def compile(self):
+        buf = StringIO()
+        buf.write(self._create_line())
+
+        select_query = self.select.compile()
+        buf.write('\nAS\n{0}'.format(select_query))
+        return buf.getvalue()
+
+    def _create_line(self):
+        scoped_name = self._get_scoped_name(self.name, self.database)
+        return '{0} {1}{2}'.format('CREATE VIEW', self._if_exists(),
+                                   scoped_name)
+
+
+class CreateTableParquet(CreateTable):
+
+    def __init__(self, table_name, path,
+                 example_file=None,
+                 example_table=None,
+                 schema=None,
+                 external=True,
+                 **kwargs):
+        self.example_file = example_file
+        self.example_table = example_table
+        self.schema = schema
+        CreateTable.__init__(self, table_name, external=external,
+                             format='parquet', path=path, **kwargs)
+
+        self._validate()
+
+    def _validate(self):
+        pass
+
+    def compile(self):
+        buf = StringIO()
+        buf.write(self._create_line())
+
+        if self.example_file is not None:
+            buf.write("\nLIKE PARQUET '{0}'".format(self.example_file))
+        elif self.example_table is not None:
+            buf.write("\nLIKE {0}".format(self.example_table))
+        elif self.schema is not None:
+            schema = format_schema(self.schema)
+            buf.write('\n{0}'.format(schema))
         else:
-            raise com.UnsupportedOperationError(f"Unit not supported: {unit}")
+            raise NotImplementedError
+
+        buf.write(self._storage())
+        buf.write(self._location())
+        return buf.getvalue()
+
+
+class CreateTableWithSchema(CreateTable):
+
+    def __init__(self, table_name, schema, table_format, **kwargs):
+        self.schema = schema
+        self.table_format = table_format
+
+        CreateTable.__init__(self, table_name, **kwargs)
+
+    def compile(self):
+        from ibis.expr.api import Schema
+
+        buf = StringIO()
+        buf.write(self._create_line())
+
+        def _push_schema(x):
+            formatted = format_schema(x)
+            buf.write('{0}'.format(formatted))
 
-    def visit_Cast(self, op, *, arg, to):
-        from_ = op.arg.dtype
-        if from_.is_timestamp() and to.is_integer():
-            return self.f.unix_micros(arg)
-        elif from_.is_integer() and to.is_timestamp():
-            return self.f.timestamp_seconds(arg)
-        elif from_.is_interval() and to.is_integer():
-            if from_.unit in {
-                IntervalUnit.WEEK,
-                IntervalUnit.QUARTER,
-                IntervalUnit.NANOSECOND,
-            }:
-                raise com.UnsupportedOperationError(
-                    f"BigQuery does not allow extracting date part `{from_.unit}` from intervals"
-                )
-            return self.f.extract(self.v[to.resolution.upper()], arg)
-        elif from_.is_integer() and to.is_interval():
-            return sge.Interval(this=arg, unit=self.v[to.unit.singular])
-        elif from_.is_floating() and to.is_integer():
-            return self.cast(self.f.trunc(arg), dt.int64)
-        return super().visit_Cast(op, arg=arg, to=to)
-
-    def visit_JSONGetItem(self, op, *, arg, index):
-        return arg[index]
-
-    def visit_UnwrapJSONString(self, op, *, arg):
-        return self.f.anon["safe.string"](arg)
-
-    def visit_UnwrapJSONInt64(self, op, *, arg):
-        return self.f.anon["safe.int64"](arg)
-
-    def visit_UnwrapJSONFloat64(self, op, *, arg):
-        return self.f.anon["safe.float64"](arg)
-
-    def visit_UnwrapJSONBoolean(self, op, *, arg):
-        return self.f.anon["safe.bool"](arg)
-
-    def visit_ExtractEpochSeconds(self, op, *, arg):
-        return self.f.unix_seconds(arg)
-
-    def visit_ExtractWeekOfYear(self, op, *, arg):
-        return self.f.extract(self.v.isoweek, arg)
-
-    def visit_ExtractMillisecond(self, op, *, arg):
-        return self.f.extract(self.v.millisecond, arg)
-
-    def visit_ExtractMicrosecond(self, op, *, arg):
-        return self.f.extract(self.v.microsecond, arg)
-
-    def visit_TimestampTruncate(self, op, *, arg, unit):
-        if unit == IntervalUnit.NANOSECOND:
-            raise com.UnsupportedOperationError(
-                f"BigQuery does not support truncating {op.arg.dtype} values to unit {unit!r}"
-            )
-        elif unit == IntervalUnit.WEEK:
-            unit = "WEEK(MONDAY)"
+        if self.partition is not None:
+            main_schema = self.schema
+            part_schema = self.partition
+            if not isinstance(part_schema, Schema):
+                part_schema = Schema(
+                    part_schema,
+                    [self.schema[name] for name in part_schema])
+
+            to_delete = []
+            for name in self.partition:
+                if name in self.schema:
+                    to_delete.append(name)
+
+            if len(to_delete):
+                main_schema = main_schema.delete(to_delete)
+
+            buf.write('\n')
+            _push_schema(main_schema)
+            buf.write('\nPARTITIONED BY ')
+            _push_schema(part_schema)
         else:
-            unit = unit.name
-        return self.f.timestamp_trunc(arg, self.v[unit], dialect=self.dialect)
+            buf.write('\n')
+            _push_schema(self.schema)
+
+        format_ddl = self.table_format.to_ddl()
+        if format_ddl:
+            buf.write(format_ddl)
+
+        buf.write(self._location())
+
+        return buf.getvalue()
+
+
+class NoFormat(object):
+
+    def to_ddl(self):
+        return None
+
+
+class DelimitedFormat(object):
+
+    def __init__(self, path, delimiter=None, escapechar=None,
+                 na_rep=None, lineterminator=None):
+        self.path = path
+        self.delimiter = delimiter
+        self.escapechar = escapechar
+        self.lineterminator = lineterminator
+        self.na_rep = na_rep
 
-    def visit_DateTruncate(self, op, *, arg, unit):
-        if unit == DateUnit.WEEK:
-            unit = "WEEK(MONDAY)"
+    def to_ddl(self):
+        buf = StringIO()
+
+        buf.write("\nROW FORMAT DELIMITED")
+
+        if self.delimiter is not None:
+            buf.write("\nFIELDS TERMINATED BY '{0}'".format(self.delimiter))
+
+        if self.escapechar is not None:
+            buf.write("\nESCAPED BY '{0}'".format(self.escapechar))
+
+        if self.lineterminator is not None:
+            buf.write("\nLINES TERMINATED BY '{0}'"
+                      .format(self.lineterminator))
+
+        buf.write("\nLOCATION '{0}'".format(self.path))
+
+        if self.na_rep is not None:
+            buf.write("\nTBLPROPERTIES('serialization.null.format'='{0}')"
+                      .format(self.na_rep))
+
+        return buf.getvalue()
+
+
+class AvroFormat(object):
+
+    def __init__(self, path, avro_schema):
+        self.path = path
+        self.avro_schema = avro_schema
+
+    def to_ddl(self):
+        import json
+
+        buf = StringIO()
+        buf.write('\nSTORED AS AVRO')
+        buf.write("\nLOCATION '{0}'".format(self.path))
+
+        schema = json.dumps(self.avro_schema, indent=2, sort_keys=True)
+        schema = '\n'.join([x.rstrip() for x in schema.split('\n')])
+        buf.write("\nTBLPROPERTIES ('avro.schema.literal'='{0}')"
+                  .format(schema))
+
+        return buf.getvalue()
+
+
+class CreateTableDelimited(CreateTableWithSchema):
+
+    def __init__(self, table_name, path, schema,
+                 delimiter=None, escapechar=None, lineterminator=None,
+                 na_rep=None, external=True, **kwargs):
+        table_format = DelimitedFormat(path, delimiter=delimiter,
+                                       escapechar=escapechar,
+                                       lineterminator=lineterminator,
+                                       na_rep=na_rep)
+        CreateTableWithSchema.__init__(self, table_name, schema,
+                                       table_format, external=external,
+                                       **kwargs)
+
+
+class CreateTableAvro(CreateTable):
+
+    def __init__(self, table_name, path, avro_schema, external=True, **kwargs):
+        self.table_format = AvroFormat(path, avro_schema)
+
+        CreateTable.__init__(self, table_name, external=external, **kwargs)
+
+    def compile(self):
+        buf = StringIO()
+        buf.write(self._create_line())
+
+        format_ddl = self.table_format.to_ddl()
+        buf.write(format_ddl)
+
+        return buf.getvalue()
+
+
+class InsertSelect(ImpalaDDL):
+
+    def __init__(self, table_name, select_expr, database=None,
+                 partition=None,
+                 partition_schema=None,
+                 overwrite=False):
+        self.table_name = table_name
+        self.database = database
+        self.select = select_expr
+
+        self.partition = partition
+        self.partition_schema = partition_schema
+
+        self.overwrite = overwrite
+
+    def compile(self):
+        if self.overwrite:
+            cmd = 'INSERT OVERWRITE'
         else:
-            unit = unit.name
-        return self.f.date_trunc(arg, self.v[unit], dialect=self.dialect)
+            cmd = 'INSERT INTO'
+
+        if self.partition is not None:
+            part = _format_partition(self.partition,
+                                     self.partition_schema)
+            partition = ' {0} '.format(part)
+        else:
+            partition = ''
+
+        select_query = self.select.compile()
+        scoped_name = self._get_scoped_name(self.table_name, self.database)
+        return'{0} {1}{2}\n{3}'.format(cmd, scoped_name, partition,
+                                       select_query)
+
+
+def _format_partition(partition, partition_schema):
+    tokens = []
+    if isinstance(partition, dict):
+        for name in partition_schema:
+            if name in partition:
+                tok = '{0}={1}'.format(name, partition[name])
+            else:
+                # dynamic partitioning
+                tok = name
+            tokens.append(tok)
+    else:
+        for name, value in zip(partition_schema, partition):
+            tok = '{0}={1}'.format(name, value)
+            tokens.append(tok)
+
+    return 'PARTITION ({0})'.format(', '.join(tokens))
+
+
+class LoadData(ImpalaDDL):
+
+    """
+    Generate DDL for LOAD DATA command. Cannot be cancelled
+    """
+
+    def __init__(self, table_name, path, database=None,
+                 partition=None, partition_schema=None,
+                 overwrite=False):
+        self.table_name = table_name
+        self.database = database
+        self.path = path
+
+        self.partition = partition
+        self.partition_schema = partition_schema
+
+        self.overwrite = overwrite
+
+    def compile(self):
+        overwrite = 'OVERWRITE ' if self.overwrite else ''
+
+        if self.partition is not None:
+            partition = '\n' + _format_partition(self.partition,
+                                                 self.partition_schema)
+        else:
+            partition = ''
+
+        scoped_name = self._get_scoped_name(self.table_name, self.database)
+        return ("LOAD DATA INPATH '{0}' {1}INTO TABLE {2}{3}"
+                .format(self.path, overwrite, scoped_name, partition))
+
+
+class AlterTable(ImpalaDDL):
+
+    def __init__(self, table, location=None, format=None, tbl_properties=None,
+                 serde_properties=None):
+        self.table = table
+        self.location = location
+        self.format = _sanitize_format(format)
+        self.tbl_properties = tbl_properties
+        self.serde_properties = serde_properties
+
+    def _wrap_command(self, cmd):
+        return 'ALTER TABLE {0}'.format(cmd)
 
-    def visit_TimeTruncate(self, op, *, arg, unit):
-        if unit == TimeUnit.NANOSECOND:
-            raise com.UnsupportedOperationError(
-                f"BigQuery does not support truncating {op.arg.dtype} values to unit {unit!r}"
-            )
+    def _format_properties(self, prefix=''):
+        tokens = []
+
+        if self.location is not None:
+            tokens.append("LOCATION '{0}'".format(self.location))
+
+        if self.format is not None:
+            tokens.append("FILEFORMAT {0}".format(self.format))
+
+        if self.tbl_properties is not None:
+            props = _format_properties(self.tbl_properties)
+            tokens.append('TBLPROPERTIES {0}'.format(props))
+
+        if self.serde_properties is not None:
+            props = _format_properties(self.serde_properties)
+            tokens.append('SERDEPROPERTIES {0}'.format(props))
+
+        if len(tokens) > 0:
+            return '\n{0}{1}'.format(prefix, '\n'.join(tokens))
         else:
-            unit = unit.name
-        return self.f.time_trunc(arg, self.v[unit], dialect=self.dialect)
+            return ''
+
+    def compile(self):
+        props = self._format_properties()
+        action = '{0} SET {1}'.format(self.table, props)
+        return self._wrap_command(action)
+
+
+class PartitionProperties(AlterTable):
+
+    def __init__(self, table, partition, partition_schema,
+                 location=None, format=None,
+                 tbl_properties=None, serde_properties=None):
+        self.partition = partition
+        self.partition_schema = partition_schema
+
+        AlterTable.__init__(self, table, location=location, format=format,
+                            tbl_properties=tbl_properties,
+                            serde_properties=serde_properties)
+
+    def _compile(self, cmd, property_prefix=''):
+        part = _format_partition(self.partition, self.partition_schema)
+        if cmd:
+            part = '{0} {1}'.format(cmd, part)
+
+        props = self._format_properties(property_prefix)
+        action = '{0} {1}{2}'.format(self.table, part, props)
+        return self._wrap_command(action)
+
+
+class AddPartition(PartitionProperties):
+
+    def __init__(self, table, partition, partition_schema, location=None):
+        PartitionProperties.__init__(self, table, partition,
+                                     partition_schema,
+                                     location=location)
+
+    def compile(self):
+        return self._compile('ADD')
+
+
+class AlterPartition(PartitionProperties):
+
+    def compile(self):
+        return self._compile('', 'SET ')
+
+
+class DropPartition(PartitionProperties):
+
+    def __init__(self, table, partition, partition_schema):
+        PartitionProperties.__init__(self, table, partition,
+                                     partition_schema)
+
+    def compile(self):
+        return self._compile('DROP')
+
+
+class RenameTable(AlterTable):
+
+    def __init__(self, old_name, new_name, old_database=None,
+                 new_database=None):
+        # if either database is None, the name is assumed to be fully scoped
+        self.old_name = old_name
+        self.old_database = old_database
+        self.new_name = new_name
+        self.new_database = new_database
+
+        new_qualified_name = new_name
+        if new_database is not None:
+            new_qualified_name = self._get_scoped_name(new_name, new_database)
+
+        old_qualified_name = old_name
+        if old_database is not None:
+            old_qualified_name = self._get_scoped_name(old_name, old_database)
+
+        self.old_qualified_name = old_qualified_name
+        self.new_qualified_name = new_qualified_name
+
+    def compile(self):
+        cmd = '{0} RENAME TO {1}'.format(self.old_qualified_name,
+                                         self.new_qualified_name)
+        return self._wrap_command(cmd)
+
+
+class DropObject(ImpalaDDL):
+
+    def __init__(self, must_exist=True):
+        self.must_exist = must_exist
+
+    def compile(self):
+        if_exists = '' if self.must_exist else 'IF EXISTS '
+        object_name = self._object_name()
+        drop_line = 'DROP {0} {1}{2}'.format(self._object_type, if_exists,
+                                             object_name)
+        return drop_line
+
+
+class DropTable(DropObject):
+
+    _object_type = 'TABLE'
+
+    def __init__(self, table_name, database=None, must_exist=True):
+        self.table_name = table_name
+        self.database = database
+        DropObject.__init__(self, must_exist=must_exist)
+
+    def _object_name(self):
+        return self._get_scoped_name(self.table_name, self.database)
+
+
+class TruncateTable(ImpalaDDL):
+
+    _object_type = 'TABLE'
+
+    def __init__(self, table_name, database=None):
+        self.table_name = table_name
+        self.database = database
+
+    def compile(self):
+        name = self._get_scoped_name(self.table_name, self.database)
+        return 'TRUNCATE TABLE {0}'.format(name)
+
+
+class DropView(DropTable):
+
+    _object_type = 'VIEW'
+
+
+class CacheTable(ImpalaDDL):
+
+    def __init__(self, table_name, database=None, pool='default'):
+        self.table_name = table_name
+        self.database = database
+        self.pool = pool
+
+    def compile(self):
+        scoped_name = self._get_scoped_name(self.table_name, self.database)
+        cache_line = ('ALTER TABLE {0} SET CACHED IN \'{1}\''
+                      .format(scoped_name, self.pool))
+        return cache_line
+
 
-    def _nullifzero(self, step, zero, step_dtype):
-        if step_dtype.is_interval():
-            return self.if_(step.eq(zero), NULL, step)
-        return self.f.nullif(step, zero)
-
-    def _zero(self, dtype):
-        if dtype.is_interval():
-            return self.f.make_interval()
-        return sge.convert(0)
-
-    def _sign(self, value, dtype):
-        if dtype.is_interval():
-            zero = self._zero(dtype)
-            return sge.Case(
-                ifs=[
-                    self.if_(value < zero, -1),
-                    self.if_(value.eq(zero), 0),
-                    self.if_(value > zero, 1),
-                ],
-                default=NULL,
-            )
-        return self.f.sign(value)
-
-    def _make_range(self, func, start, stop, step, step_dtype):
-        step_sign = self._sign(step, step_dtype)
-        delta_sign = self._sign(stop - start, step_dtype)
-        zero = self._zero(step_dtype)
-        nullifzero = self._nullifzero(step, zero, step_dtype)
-        condition = sg.and_(sg.not_(nullifzero.is_(NULL)), step_sign.eq(delta_sign))
-        gen_array = func(start, stop, step)
-        name = sg.to_identifier(util.gen_name("bq_arr_range"))
-        inner = (
-            sg.select(name)
-            .from_(self._unnest(gen_array, as_=name))
-            .where(name.neq(stop))
-        )
-        return self.if_(condition, self.f.array(inner), self.f.array())
-
-    def visit_IntegerRange(self, op, *, start, stop, step):
-        return self._make_range(self.f.generate_array, start, stop, step, op.step.dtype)
-
-    def visit_TimestampRange(self, op, *, start, stop, step):
-        if op.start.dtype.timezone is None or op.stop.dtype.timezone is None:
-            raise com.IbisTypeError(
-                "Timestamps without timezone values are not supported when generating timestamp ranges"
-            )
-        return self._make_range(
-            self.f.generate_timestamp_array, start, stop, step, op.step.dtype
-        )
-
-    def visit_First(self, op, *, arg, where):
-        if where is not None:
-            arg = self.if_(where, arg, NULL)
-        array = self.f.array_agg(
-            sge.Limit(this=sge.IgnoreNulls(this=arg), expression=sge.convert(1)),
-        )
-        return array[self.f.safe_offset(0)]
-
-    def visit_Last(self, op, *, arg, where):
-        if where is not None:
-            arg = self.if_(where, arg, NULL)
-        array = self.f.array_reverse(self.f.array_agg(sge.IgnoreNulls(this=arg)))
-        return array[self.f.safe_offset(0)]
-
-    def visit_ArrayFilter(self, op, *, arg, body, param):
-        return self.f.array(
-            sg.select(param).from_(self._unnest(arg, as_=param)).where(body)
-        )
-
-    def visit_ArrayMap(self, op, *, arg, body, param):
-        return self.f.array(sg.select(body).from_(self._unnest(arg, as_=param)))
-
-    def visit_ArrayZip(self, op, *, arg):
-        lengths = [self.f.array_length(arr) - 1 for arr in arg]
-        idx = sg.to_identifier(util.gen_name("bq_arr_idx"))
-        indices = self._unnest(
-            self.f.generate_array(0, self.f.greatest(*lengths)), as_=idx
-        )
-        struct_fields = [
-            arr[self.f.safe_offset(idx)].as_(name)
-            for name, arr in zip(op.dtype.value_type.names, arg)
-        ]
-        return self.f.array(
-            sge.Select(kind="STRUCT", expressions=struct_fields).from_(indices)
-        )
-
-    def visit_ArrayPosition(self, op, *, arg, other):
-        name = sg.to_identifier(util.gen_name("bq_arr"))
-        idx = sg.to_identifier(util.gen_name("bq_arr_idx"))
-        unnest = self._unnest(arg, as_=name, offset=idx)
-        return self.f.coalesce(
-            sg.select(idx + 1).from_(unnest).where(name.eq(other)).limit(1).subquery(),
-            0,
-        )
-
-    def _unnest(self, expression, *, as_, offset=None):
-        alias = sge.TableAlias(columns=[sg.to_identifier(as_)])
-        return sge.Unnest(expressions=[expression], alias=alias, offset=offset)
-
-    def visit_ArrayRemove(self, op, *, arg, other):
-        name = sg.to_identifier(util.gen_name("bq_arr"))
-        unnest = self._unnest(arg, as_=name)
-        return self.f.array(sg.select(name).from_(unnest).where(name.neq(other)))
-
-    def visit_ArrayDistinct(self, op, *, arg):
-        name = util.gen_name("bq_arr")
-        return self.f.array(
-            sg.select(name).distinct().from_(self._unnest(arg, as_=name))
-        )
-
-    def visit_ArraySort(self, op, *, arg):
-        name = util.gen_name("bq_arr")
-        return self.f.array(
-            sg.select(name).from_(self._unnest(arg, as_=name)).order_by(name)
-        )
-
-    def visit_ArrayUnion(self, op, *, left, right):
-        lname = util.gen_name("bq_arr_left")
-        rname = util.gen_name("bq_arr_right")
-        lhs = sg.select(lname).from_(self._unnest(left, as_=lname))
-        rhs = sg.select(rname).from_(self._unnest(right, as_=rname))
-        return self.f.array(sg.union(lhs, rhs, distinct=True))
-
-    def visit_ArrayIntersect(self, op, *, left, right):
-        lname = util.gen_name("bq_arr_left")
-        rname = util.gen_name("bq_arr_right")
-        lhs = sg.select(lname).from_(self._unnest(left, as_=lname))
-        rhs = sg.select(rname).from_(self._unnest(right, as_=rname))
-        return self.f.array(sg.intersect(lhs, rhs, distinct=True))
-
-    def visit_RegexExtract(self, op, *, arg, pattern, index):
-        matches = self.f.regexp_contains(arg, pattern)
-        nonzero_index_replace = self.f.regexp_replace(
-            arg,
-            self.f.concat(".*?", pattern, ".*"),
-            self.f.concat("\\", self.cast(index, dt.string)),
-        )
-        zero_index_replace = self.f.regexp_replace(
-            arg, self.f.concat(".*?", self.f.concat("(", pattern, ")"), ".*"), "\\1"
-        )
-        extract = self.if_(index.eq(0), zero_index_replace, nonzero_index_replace)
-        return self.if_(matches, extract, NULL)
-
-    def visit_TimestampAddSub(self, op, *, left, right):
-        if not isinstance(right, sge.Interval):
-            raise com.OperationNotDefinedError(
-                "BigQuery does not support non-literals on the right side of timestamp add/subtract"
-            )
-        if (unit := op.right.dtype.unit) == IntervalUnit.NANOSECOND:
-            raise com.UnsupportedOperationError(
-                f"BigQuery does not allow binary operation {type(op).__name__} with "
-                f"INTERVAL offset {unit}"
-            )
-
-        opname = type(op).__name__[len("Timestamp") :]
-        funcname = f"TIMESTAMP_{opname.upper()}"
-        return self.f.anon[funcname](left, right)
-
-    visit_TimestampAdd = visit_TimestampSub = visit_TimestampAddSub
-
-    def visit_DateAddSub(self, op, *, left, right):
-        if not isinstance(right, sge.Interval):
-            raise com.OperationNotDefinedError(
-                "BigQuery does not support non-literals on the right side of date add/subtract"
-            )
-        if not (unit := op.right.dtype.unit).is_date():
-            raise com.UnsupportedOperationError(
-                f"BigQuery does not allow binary operation {type(op).__name__} with "
-                f"INTERVAL offset {unit}"
-            )
-        opname = type(op).__name__[len("Date") :]
-        funcname = f"DATE_{opname.upper()}"
-        return self.f.anon[funcname](left, right)
-
-    visit_DateAdd = visit_DateSub = visit_DateAddSub
-
-    def visit_Covariance(self, op, *, left, right, how, where):
-        if where is not None:
-            left = self.if_(where, left, NULL)
-            right = self.if_(where, right, NULL)
-
-        if op.left.dtype.is_boolean():
-            left = self.cast(left, dt.int64)
-
-        if op.right.dtype.is_boolean():
-            right = self.cast(right, dt.int64)
-
-        how = op.how[:4].upper()
-        assert how in ("POP", "SAMP"), 'how not in ("POP", "SAMP")'
-        return self.agg[f"COVAR_{how}"](left, right, where=where)
-
-    def visit_Correlation(self, op, *, left, right, how, where):
-        if how == "sample":
-            raise ValueError(f"Correlation with how={how!r} is not supported.")
-
-        if where is not None:
-            left = self.if_(where, left, NULL)
-            right = self.if_(where, right, NULL)
-
-        if op.left.dtype.is_boolean():
-            left = self.cast(left, dt.int64)
-
-        if op.right.dtype.is_boolean():
-            right = self.cast(right, dt.int64)
-
-        return self.agg.corr(left, right, where=where)
-
-    def visit_TypeOf(self, op, *, arg):
-        name = sg.to_identifier(util.gen_name("bq_typeof"))
-        from_ = self._unnest(self.f.array(self.f.format("%T", arg)), as_=name)
-        ifs = [
-            self.if_(
-                self.f.regexp_contains(name, '^[A-Z]+ "'),
-                self.f.regexp_extract(name, '^([A-Z]+) "'),
-            ),
-            self.if_(self.f.regexp_contains(name, "^-?[0-9]*$"), "INT64"),
-            self.if_(
-                self.f.regexp_contains(
-                    name, r'^(-?[0-9]+[.e].*|CAST\("([^"]*)" AS FLOAT64\))$'
-                ),
-                "FLOAT64",
-            ),
-            self.if_(name.isin(sge.convert("true"), sge.convert("false")), "BOOL"),
-            self.if_(
-                sg.or_(self.f.starts_with(name, '"'), self.f.starts_with(name, "'")),
-                "STRING",
-            ),
-            self.if_(self.f.starts_with(name, 'b"'), "BYTES"),
-            self.if_(self.f.starts_with(name, "["), "ARRAY"),
-            self.if_(self.f.regexp_contains(name, r"^(STRUCT)?\("), "STRUCT"),
-            self.if_(self.f.starts_with(name, "ST_"), "GEOGRAPHY"),
-            self.if_(name.eq(sge.convert("NULL")), "NULL"),
-        ]
-        case = sge.Case(ifs=ifs, default=sge.convert("UNKNOWN"))
-        return sg.select(case).from_(from_).subquery()
-
-    def visit_Xor(self, op, *, left, right):
-        return sg.or_(sg.and_(left, sg.not_(right)), sg.and_(sg.not_(left), right))
-
-    def visit_HashBytes(self, op, *, arg, how):
-        if how not in ("md5", "sha1", "sha256", "sha512"):
-            raise NotImplementedError(how)
-        return self.f[how](arg)
-
-    @staticmethod
-    def _gen_valid_name(name: str) -> str:
-        return "_".join(_NAME_REGEX.findall(name)) or "tmp"
-
-    def visit_CountStar(self, op, *, arg, where):
-        if where is not None:
-            return self.f.countif(where)
-        return self.f.count(STAR)
-
-    def visit_Degrees(self, op, *, arg):
-        return sge.paren(180 * arg / self.f.acos(-1), copy=False)
-
-    def visit_Radians(self, op, *, arg):
-        return sge.paren(self.f.acos(-1) * arg / 180, copy=False)
-
-    def visit_CountDistinct(self, op, *, arg, where):
-        if where is not None:
-            arg = self.if_(where, arg, NULL)
-        return self.f.count(sge.Distinct(expressions=[arg]))
+class CreateDatabase(CreateDDL):
 
-    def visit_RandomUUID(self, op, **kwargs):
-        return self.f.generate_uuid()
+    def __init__(self, name, path=None, can_exist=False):
+        self.name = name
+        self.path = path
+        self.can_exist = can_exist
+
+    def compile(self):
+        name = quote_identifier(self.name)
+
+        create_decl = 'CREATE DATABASE'
+        create_line = '{0} {1}{2}'.format(create_decl, self._if_exists(),
+                                          name)
+        if self.path is not None:
+            create_line += "\nLOCATION '{0}'".format(self.path)
+
+        return create_line
+
+
+class DropDatabase(DropObject):
+
+    _object_type = 'DATABASE'
+
+    def __init__(self, name, must_exist=True):
+        self.name = name
+        DropObject.__init__(self, must_exist=must_exist)
+
+    def _object_name(self):
+        return self.name
+
+
+def format_schema(schema):
+    elements = [_format_schema_element(name, t)
+                for name, t in zip(schema.names, schema.types)]
+    return '({0})'.format(',\n '.join(elements))
+
+
+def _format_schema_element(name, t):
+    return '{0} {1}'.format(quote_identifier(name, force=True),
+                            _type_to_sql_string(t))
+
+
+class CreateFunctionBase(ImpalaDDL):
+
+    _object_type = 'FUNCTION'
+
+    def __init__(self, lib_path, inputs, output, name, database=None):
+        self.lib_path = lib_path
+
+        self.inputs, self.output = inputs, output
+        self.input_sig = _impala_signature(inputs)
+        self.output_sig = _arg_to_string(output)
+
+        self.name = name
+        self.database = database
+
+    def _create_line(self):
+        scoped_name = self._get_scoped_name(self.name, self.database)
+        return ('{0!s}({1!s}) returns {2!s}'
+                .format(scoped_name, self.input_sig, self.output_sig))
+
+
+class CreateFunction(CreateFunctionBase):
+
+    def __init__(self, lib_path, so_symbol, inputs, output,
+                 name, database=None):
+        self.so_symbol = so_symbol
+
+        CreateFunctionBase.__init__(self, lib_path, inputs, output,
+                                    name, database=database)
+
+    def compile(self):
+        create_decl = 'CREATE FUNCTION'
+        create_line = self._create_line()
+        param_line = ("location '{0!s}' symbol='{1!s}'"
+                      .format(self.lib_path, self.so_symbol))
+        full_line = ' '.join([create_decl, create_line, param_line])
+        return full_line
+
+
+class CreateAggregateFunction(CreateFunction):
+
+    def __init__(self, lib_path, inputs, output, update_fn, init_fn,
+                 merge_fn, serialize_fn, finalize_fn, name, database):
+        self.init = init_fn
+        self.update = update_fn
+        self.merge = merge_fn
+        self.serialize = serialize_fn
+        self.finalize = finalize_fn
+
+        CreateFunctionBase.__init__(self, lib_path, inputs, output,
+                                    name, database=database)
+
+    def compile(self):
+        create_decl = 'CREATE AGGREGATE FUNCTION'
+        create_line = self._create_line()
+        tokens = ["location '{0!s}'".format(self.lib_path)]
+
+        if self.init is not None:
+            tokens.append("init_fn='{0}'".format(self.init))
+
+        tokens.append("update_fn='{0}'".format(self.update))
+
+        if self.merge is not None:
+            tokens.append("merge_fn='{0}'".format(self.merge))
+
+        if self.serialize is not None:
+            tokens.append("serialize_fn='{0}'".format(self.serialize))
+
+        if self.finalize is not None:
+            tokens.append("finalize_fn='{0}'".format(self.finalize))
+
+        full_line = (' '.join([create_decl, create_line]) + ' ' +
+                     '\n'.join(tokens))
+        return full_line
+
+
+class DropFunction(DropObject):
+
+    def __init__(self, name, inputs, must_exist=True,
+                 aggregate=False, database=None):
+        self.name = name
+
+        self.inputs = inputs
+        self.input_sig = _impala_signature(inputs)
+
+        self.must_exist = must_exist
+        self.aggregate = aggregate
+        self.database = database
+        DropObject.__init__(self, must_exist=must_exist)
+
+    def _object_name(self):
+        return self.name
+
+    def _function_sig(self):
+        full_name = self._get_scoped_name(self.name, self.database)
+        return '{0!s}({1!s})'.format(full_name, self.input_sig)
+
+    def compile(self):
+        tokens = ['DROP']
+        if self.aggregate:
+            tokens.append('AGGREGATE')
+        tokens.append('FUNCTION')
+        if not self.must_exist:
+            tokens.append('IF EXISTS')
+
+        tokens.append(self._function_sig())
+        return ' '.join(tokens)
+
+
+class ListFunction(ImpalaDDL):
+
+    def __init__(self, database, like=None, aggregate=False):
+        self.database = database
+        self.like = like
+        self.aggregate = aggregate
+
+    def compile(self):
+        statement = 'SHOW '
+        if self.aggregate:
+            statement += 'AGGREGATE '
+        statement += 'FUNCTIONS IN {0}'.format(self.database)
+        if self.like:
+            statement += " LIKE '{0}'".format(self.like)
+        return statement
+
+
+def _impala_signature(sig):
+    if isinstance(sig, rules.TypeSignature):
+        if isinstance(sig, rules.VarArgs):
+            val = _arg_to_string(sig.arg_type)
+            return '{0}...'.format(val)
+        else:
+            return ', '.join([_arg_to_string(arg) for arg in sig.types])
+    else:
+        return ', '.join([_type_to_sql_string(validate_type(x))
+                          for x in sig])
+
+
+def _arg_to_string(arg):
+    if isinstance(arg, rules.ValueTyped):
+        types = arg.types
+        if len(types) > 1:
+            raise NotImplementedError
+        return _type_to_sql_string(types[0])
+    elif isinstance(arg, py_string):
+        return _type_to_sql_string(validate_type(arg))
+    else:
+        raise NotImplementedError
```

### Comparing `ibis_framework-9.0.0.dev686/ibis/backends/datafusion/__init__.py` & `ibis-framework-v0.6.0/ibis/config.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,672 +1,725 @@
-from __future__ import annotations
+# This file has been adapted from pandas/core/config.py. pandas 3-clause BSD
+# license. See LICENSES/pandas
+#
+# Further modifications:
+#
+# Copyright 2014 Cloudera Inc.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+# http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-import contextlib
-import inspect
-import typing
-from collections.abc import Mapping
-from pathlib import Path
-from typing import TYPE_CHECKING, Any
-
-import datafusion as df
-import pyarrow as pa
-import pyarrow.dataset as ds
-import pyarrow_hotfix  # noqa: F401
-import sqlglot as sg
-import sqlglot.expressions as sge
-
-import ibis
-import ibis.common.exceptions as com
-import ibis.expr.datatypes as dt
-import ibis.expr.operations as ops
-import ibis.expr.schema as sch
-import ibis.expr.types as ir
-from ibis.backends import CanCreateCatalog, CanCreateDatabase, CanCreateSchema, NoUrl
-from ibis.backends.datafusion.compiler import DataFusionCompiler
-from ibis.backends.sql import SQLBackend
-from ibis.backends.sql.compiler import C
-from ibis.expr.operations.udf import InputType
-from ibis.formats.pyarrow import PyArrowType
-from ibis.util import gen_name, normalize_filename
-
-try:
-    from datafusion import ExecutionContext as SessionContext
-except ImportError:
-    from datafusion import SessionContext
-
-try:
-    from datafusion import SessionConfig
-except ImportError:
-    SessionConfig = None
-
-if TYPE_CHECKING:
-    import pandas as pd
-
-
-class Backend(SQLBackend, CanCreateCatalog, CanCreateDatabase, CanCreateSchema, NoUrl):
-    name = "datafusion"
-    supports_in_memory_tables = True
-    supports_arrays = True
-    compiler = DataFusionCompiler()
+import re
 
-    @property
-    def version(self):
-        import importlib.metadata
+from collections import namedtuple
+from contextlib import contextmanager
+import pprint
+import warnings
+import sys
 
-        return importlib.metadata.version("datafusion")
+from six import StringIO
 
-    def do_connect(
-        self, config: Mapping[str, str | Path] | SessionContext | None = None
-    ) -> None:
-        """Create a Datafusion backend for use with Ibis.
-
-        Parameters
-        ----------
-        config
-            Mapping of table names to files.
-
-        Examples
-        --------
-        >>> import ibis
-        >>> config = {"t": "path/to/file.parquet", "s": "path/to/file.csv"}
-        >>> ibis.datafusion.connect(config)
-
-        """
-        if isinstance(config, SessionContext):
-            (self.con, config) = (config, None)
-        else:
-            if config is not None and not isinstance(config, Mapping):
-                raise TypeError("Input to ibis.datafusion.connect must be a mapping")
-            if SessionConfig is not None:
-                df_config = SessionConfig(
-                    {"datafusion.sql_parser.dialect": "PostgreSQL"}
-                ).with_information_schema(True)
-            else:
-                df_config = None
-            self.con = SessionContext(df_config)
+PY3 = (sys.version_info[0] >= 3)
 
-        self._register_builtin_udfs()
+if PY3:
+    def u(s):
+        return s
+else:
+    def u(s):
+        return unicode(s, "unicode_escape")
 
-        if not config:
-            config = {}
 
-        for name, path in config.items():
-            self.register(path, table_name=name)
-
-    def disconnect(self) -> None:
-        pass
-
-    @contextlib.contextmanager
-    def _safe_raw_sql(self, sql: sge.Statement) -> Any:
-        yield self.raw_sql(sql).collect()
-
-    def _get_schema_using_query(self, query: str) -> sch.Schema:
-        name = gen_name("datafusion_metadata_view")
-        table = sg.table(name, quoted=self.compiler.quoted)
-        src = sge.Create(
-            this=table,
-            kind="VIEW",
-            expression=sg.parse_one(query, read="datafusion"),
-            properties=sge.Properties(expressions=[sge.TemporaryProperty()]),
-        )
-
-        with self._safe_raw_sql(src):
-            pass
-
-        try:
-            result = (
-                self.raw_sql(f"DESCRIBE {table.sql(self.name)}")
-                .to_arrow_table()
-                .to_pydict()
-            )
-        finally:
-            self.drop_view(name)
-        return sch.Schema(
-            {
-                name: self.compiler.type_mapper.from_string(
-                    type_string, nullable=is_nullable == "YES"
-                )
-                for name, type_string, is_nullable in zip(
-                    result["column_name"], result["data_type"], result["is_nullable"]
-                )
-            }
-        )
-
-    def _register_builtin_udfs(self):
-        from ibis.backends.datafusion import udfs
-
-        for name, func in inspect.getmembers(
-            udfs,
-            predicate=lambda m: callable(m)
-            and not m.__name__.startswith("_")
-            and m.__module__ == udfs.__name__,
-        ):
-            annotations = typing.get_type_hints(func)
-            argnames = list(inspect.signature(func).parameters.keys())
-            input_types = [
-                PyArrowType.from_ibis(dt.dtype(annotations.get(arg_name)))
-                for arg_name in argnames
-            ]
-            return_type = PyArrowType.from_ibis(dt.dtype(annotations["return"]))
-            udf = df.udf(
-                func,
-                input_types=input_types,
-                return_type=return_type,
-                volatility="immutable",
-                name=name,
-            )
-            self.con.register_udf(udf)
+DeprecatedOption = namedtuple('DeprecatedOption', 'key msg rkey removal_ver')
+RegisteredOption = namedtuple(
+    'RegisteredOption', 'key defval doc validator cb')
 
-    def _register_udfs(self, expr: ir.Expr) -> None:
-        for udf_node in expr.op().find(ops.ScalarUDF):
-            if udf_node.__input_type__ == InputType.PYARROW:
-                udf = self._compile_pyarrow_udf(udf_node)
-                self.con.register_udf(udf)
-
-        for udf_node in expr.op().find(ops.ElementWiseVectorizedUDF):
-            udf = self._compile_elementwise_udf(udf_node)
-            self.con.register_udf(udf)
-
-    def _compile_pyarrow_udf(self, udf_node):
-        return df.udf(
-            udf_node.__func__,
-            input_types=[PyArrowType.from_ibis(arg.dtype) for arg in udf_node.args],
-            return_type=PyArrowType.from_ibis(udf_node.dtype),
-            volatility=getattr(udf_node, "__config__", {}).get(
-                "volatility", "volatile"
-            ),
-            name=udf_node.__func_name__,
-        )
-
-    def _compile_elementwise_udf(self, udf_node):
-        return df.udf(
-            udf_node.func,
-            input_types=list(map(PyArrowType.from_ibis, udf_node.input_type)),
-            return_type=PyArrowType.from_ibis(udf_node.return_type),
-            volatility="volatile",
-            name=udf_node.func.__name__,
-        )
-
-    def raw_sql(self, query: str | sge.Expression) -> Any:
-        """Execute a SQL string `query` against the database.
-
-        Parameters
-        ----------
-        query
-            Raw SQL string
-        kwargs
-            Backend specific query arguments
-
-        """
-        with contextlib.suppress(AttributeError):
-            query = query.sql(dialect=self.dialect, pretty=True)
-        self._log(query)
-        return self.con.sql(query)
+_deprecated_options = {}  # holds deprecated option metdata
+_registered_options = {}  # holds registered option metdata
+_global_config = {}  # holds the current values for registered options
+_reserved_keys = ['all']  # keys which have a special meaning
 
-    @property
-    def current_catalog(self) -> str:
-        raise NotImplementedError()
 
-    @property
-    def current_database(self) -> str:
-        return NotImplementedError()
+class OptionError(AttributeError, KeyError):
 
-    def list_catalogs(self, like: str | None = None) -> list[str]:
-        code = (
-            sg.select(C.table_catalog)
-            .from_(sg.table("tables", db="information_schema"))
-            .distinct()
-        ).sql()
-        result = self.con.sql(code).to_pydict()
-        return self._filter_with_like(result["table_catalog"], like)
-
-    def create_catalog(self, name: str, force: bool = False) -> None:
-        with self._safe_raw_sql(
-            sge.Create(kind="DATABASE", this=sg.to_identifier(name), exists=force)
-        ):
-            pass
-
-    def drop_catalog(self, name: str, force: bool = False) -> None:
-        raise com.UnsupportedOperationError(
-            "DataFusion does not support dropping databases"
-        )
-
-    def list_databases(
-        self, like: str | None = None, catalog: str | None = None
-    ) -> list[str]:
-        return self._filter_with_like(
-            self.con.catalog(catalog if catalog is not None else "datafusion").names(),
-            like=like,
-        )
-
-    def create_database(
-        self, name: str, catalog: str | None = None, force: bool = False
-    ) -> None:
-        # not actually a table, but this is how sqlglot represents schema names
-        db_name = sg.table(name, db=catalog)
-        with self._safe_raw_sql(sge.Create(kind="SCHEMA", this=db_name, exists=force)):
-            pass
-
-    def drop_database(
-        self, name: str, catalog: str | None = None, force: bool = False
-    ) -> None:
-        db_name = sg.table(name, db=catalog)
-        with self._safe_raw_sql(sge.Drop(kind="SCHEMA", this=db_name, exists=force)):
-            pass
-
-    def list_tables(
-        self,
-        like: str | None = None,
-        database: str | None = None,
-    ) -> list[str]:
-        """Return the list of table names in the current database.
-
-        Parameters
-        ----------
-        like
-            A pattern in Python's regex format.
-        database
-            Unused in the datafusion backend.
-
-        Returns
-        -------
-        list[str]
-            The list of the table names that match the pattern `like`.
-        """
-        return self._filter_with_like(self.con.tables(), like)
-
-    def get_schema(
-        self,
-        table_name: str,
-        *,
-        catalog: str | None = None,
-        database: str | None = None,
-    ) -> sch.Schema:
-        if catalog is not None:
-            catalog = self.con.catalog(catalog)
-        else:
-            catalog = self.con.catalog()
+    """Exception for ibis.options, backwards compatible with KeyError
+    checks"""
 
-        if database is not None:
-            database = catalog.database(database)
-        else:
-            database = catalog.database()
 
-        table = database.table(table_name)
-        return sch.schema(table.schema)
+#
+# User API
 
-    def register(
-        self,
-        source: str | Path | pa.Table | pa.RecordBatch | pa.Dataset | pd.DataFrame,
-        table_name: str | None = None,
-        **kwargs: Any,
-    ) -> ir.Table:
-        """Register a data set with `table_name` located at `source`.
-
-        Parameters
-        ----------
-        source
-            The data source(s). May be a path to a file or directory of
-            parquet/csv files, a pandas dataframe, or a pyarrow table, dataset
-            or record batch.
-        table_name
-            The name of the table
-        kwargs
-            Datafusion-specific keyword arguments
-
-        Examples
-        --------
-        Register a csv:
-
-        >>> import ibis
-        >>> conn = ibis.datafusion.connect(config)
-        >>> conn.register("path/to/data.csv", "my_table")
-        >>> conn.table("my_table")
-
-        Register a PyArrow table:
-
-        >>> import pyarrow as pa
-        >>> tab = pa.table({"x": [1, 2, 3]})
-        >>> conn.register(tab, "my_table")
-        >>> conn.table("my_table")
-
-        Register a PyArrow dataset:
-
-        >>> import pyarrow.dataset as ds
-        >>> dataset = ds.dataset("path/to/table")
-        >>> conn.register(dataset, "my_table")
-        >>> conn.table("my_table")
-
-        """
-        import pandas as pd
-
-        if isinstance(source, (str, Path)):
-            first = str(source)
-        elif isinstance(source, pa.Table):
-            self.con.deregister_table(table_name)
-            self.con.register_record_batches(table_name, [source.to_batches()])
-            return self.table(table_name)
-        elif isinstance(source, pa.RecordBatch):
-            self.con.deregister_table(table_name)
-            self.con.register_record_batches(table_name, [[source]])
-            return self.table(table_name)
-        elif isinstance(source, pa.dataset.Dataset):
-            self.con.deregister_table(table_name)
-            self.con.register_dataset(table_name, source)
-            return self.table(table_name)
-        elif isinstance(source, pd.DataFrame):
-            return self.register(pa.Table.from_pandas(source), table_name, **kwargs)
-        else:
-            raise ValueError("`source` must be either a string or a pathlib.Path")
+def _get_single_key(pat, silent):
+    keys = _select_options(pat)
+    if len(keys) == 0:
+        if not silent:
+            _warn_if_deprecated(pat)
+        raise OptionError('No such keys(s): %r' % pat)
+    if len(keys) > 1:
+        raise OptionError('Pattern matched multiple keys')
+    key = keys[0]
 
-        if first.startswith(("parquet://", "parq://")) or first.endswith(
-            ("parq", "parquet")
-        ):
-            return self.read_parquet(source, table_name=table_name, **kwargs)
-        elif first.startswith(("csv://", "txt://")) or first.endswith(
-            ("csv", "tsv", "txt")
-        ):
-            return self.read_csv(source, table_name=table_name, **kwargs)
-        else:
-            self._register_failure()
-            return None
+    if not silent:
+        _warn_if_deprecated(key)
 
-    def _register_failure(self):
-        import inspect
+    key = _translate_key(key)
 
-        msg = ", ".join(
-            m[0] for m in inspect.getmembers(self) if m[0].startswith("read_")
-        )
-        raise ValueError(
-            f"Cannot infer appropriate read function for input, "
-            f"please call one of {msg} directly"
-        )
-
-    def _register_in_memory_table(self, op: ops.InMemoryTable) -> None:
-        name = op.name
-        schema = op.schema
-
-        self.con.deregister_table(name)
-        if batches := op.data.to_pyarrow(schema).to_batches():
-            self.con.register_record_batches(name, [batches])
-        else:
-            empty_dataset = ds.dataset([], schema=schema.to_pyarrow())
-            self.con.register_dataset(name=name, dataset=empty_dataset)
+    return key
 
-    def _register_in_memory_tables(self, expr: ir.Expr) -> None:
-        if self.supports_in_memory_tables:
-            for memtable in expr.op().find(ops.InMemoryTable):
-                self._register_in_memory_table(memtable)
-
-    def read_csv(
-        self, path: str | Path, table_name: str | None = None, **kwargs: Any
-    ) -> ir.Table:
-        """Register a CSV file as a table in the current database.
-
-        Parameters
-        ----------
-        path
-            The data source. A string or Path to the CSV file.
-        table_name
-            An optional name to use for the created table. This defaults to
-            a sequentially generated name.
-        **kwargs
-            Additional keyword arguments passed to Datafusion loading function.
-
-        Returns
-        -------
-        ir.Table
-            The just-registered table
-
-        """
-        path = normalize_filename(path)
-        table_name = table_name or gen_name("read_csv")
-        # Our other backends support overwriting views / tables when reregistering
-        self.con.deregister_table(table_name)
-        self.con.register_csv(table_name, path, **kwargs)
-        return self.table(table_name)
-
-    def read_parquet(
-        self, path: str | Path, table_name: str | None = None, **kwargs: Any
-    ) -> ir.Table:
-        """Register a parquet file as a table in the current database.
-
-        Parameters
-        ----------
-        path
-            The data source.
-        table_name
-            An optional name to use for the created table. This defaults to
-            a sequentially generated name.
-        **kwargs
-            Additional keyword arguments passed to Datafusion loading function.
-
-        Returns
-        -------
-        ir.Table
-            The just-registered table
-
-        """
-        path = normalize_filename(path)
-        table_name = table_name or gen_name("read_parquet")
-        # Our other backends support overwriting views / tables when reregistering
-        self.con.deregister_table(table_name)
-        self.con.register_parquet(table_name, path, **kwargs)
-        return self.table(table_name)
-
-    def read_delta(
-        self, source_table: str | Path, table_name: str | None = None, **kwargs: Any
-    ) -> ir.Table:
-        """Register a Delta Lake table as a table in the current database.
-
-        Parameters
-        ----------
-        source_table
-            The data source. Must be a directory
-            containing a Delta Lake table.
-        table_name
-            An optional name to use for the created table. This defaults to
-            a sequentially generated name.
-        **kwargs
-            Additional keyword arguments passed to deltalake.DeltaTable.
-
-        Returns
-        -------
-        ir.Table
-            The just-registered table
-
-        """
-        source_table = normalize_filename(source_table)
-
-        table_name = table_name or gen_name("read_delta")
-
-        # Our other backends support overwriting views / tables when reregistering
-        self.con.deregister_table(table_name)
-
-        try:
-            from deltalake import DeltaTable
-        except ImportError:
-            raise ImportError(
-                "The deltalake extra is required to use the "
-                "read_delta method. You can install it using pip:\n\n"
-                "pip install 'ibis-framework[deltalake]'\n"
-            )
 
-        delta_table = DeltaTable(source_table, **kwargs)
+def _get_option(pat, silent=False):
+    key = _get_single_key(pat, silent)
 
-        return self.register(delta_table.to_pyarrow_dataset(), table_name=table_name)
+    # walk the nested dict
+    root, k = _get_root(key)
+    return root[k]
 
-    def to_pyarrow_batches(
-        self,
-        expr: ir.Expr,
-        *,
-        chunk_size: int = 1_000_000,
-        **kwargs: Any,
-    ) -> pa.ipc.RecordBatchReader:
-        pa = self._import_pyarrow()
-
-        self._register_udfs(expr)
-        self._register_in_memory_tables(expr)
-
-        table_expr = expr.as_table()
-        raw_sql = self.compile(table_expr, **kwargs)
-
-        frame = self.con.sql(raw_sql)
-
-        schema = table_expr.schema()
-        names = schema.names
-
-        struct_schema = schema.as_struct().to_pyarrow()
-
-        def make_gen():
-            yield from (
-                # convert the renamed + casted columns into a record batch
-                pa.RecordBatch.from_struct_array(
-                    # rename columns to match schema because datafusion lowercases things
-                    pa.RecordBatch.from_arrays(batch.columns, names=names)
-                    # cast the struct array to the desired types to work around
-                    # https://github.com/apache/arrow-datafusion-python/issues/534
-                    .to_struct_array()
-                    .cast(struct_schema)
-                )
-                for batch in frame.collect()
-            )
 
-        return pa.ipc.RecordBatchReader.from_batches(
-            schema.to_pyarrow(),
-            make_gen(),
-        )
-
-    def to_pyarrow(self, expr: ir.Expr, **kwargs: Any) -> pa.Table:
-        batch_reader = self.to_pyarrow_batches(expr, **kwargs)
-        arrow_table = batch_reader.read_all()
-        return expr.__pyarrow_result__(arrow_table)
-
-    def execute(self, expr: ir.Expr, **kwargs: Any):
-        batch_reader = self.to_pyarrow_batches(expr, **kwargs)
-        return expr.__pandas_result__(
-            batch_reader.read_pandas(timestamp_as_object=True)
-        )
-
-    def create_table(
-        self,
-        name: str,
-        obj: pd.DataFrame | pa.Table | ir.Table | None = None,
-        *,
-        schema: sch.Schema | None = None,
-        database: str | None = None,
-        temp: bool = False,
-        overwrite: bool = False,
-    ):
-        """Create a table in Datafusion.
-
-        Parameters
-        ----------
-        name
-            Name of the table to create
-        obj
-            The data with which to populate the table; optional, but at least
-            one of `obj` or `schema` must be specified
-        schema
-            The schema of the table to create; optional, but at least one of
-            `obj` or `schema` must be specified
-        database
-            The name of the database in which to create the table; if not
-            passed, the current database is used.
-        temp
-            Create a temporary table
-        overwrite
-            If `True`, replace the table if it already exists, otherwise fail
-            if the table exists
-
-        """
-        if obj is None and schema is None:
-            raise ValueError("Either `obj` or `schema` must be specified")
-
-        properties = []
-
-        if temp:
-            properties.append(sge.TemporaryProperty())
-
-        quoted = self.compiler.quoted
-
-        if obj is not None:
-            if not isinstance(obj, ir.Expr):
-                table = ibis.memtable(obj)
-            else:
-                table = obj
+def _set_option(*args, **kwargs):
+    # must at least 1 arg deal with constraints later
+    nargs = len(args)
+    if not nargs or nargs % 2 != 0:
+        raise ValueError("Must provide an even number of non-keyword "
+                         "arguments")
 
-            self._run_pre_execute_hooks(table)
+    # default to false
+    silent = kwargs.get('silent', False)
 
-            relname = "_"
-            query = sg.select(
-                *(
-                    self.compiler.cast(
-                        sg.column(col, table=relname, quoted=quoted), dtype
-                    ).as_(col, quoted=quoted)
-                    for col, dtype in table.schema().items()
-                )
-            ).from_(
-                self._to_sqlglot(table).subquery(
-                    sg.to_identifier(relname, quoted=quoted)
-                )
-            )
+    for k, v in zip(args[::2], args[1::2]):
+        key = _get_single_key(k, silent)
+
+        o = _get_registered_option(key)
+        if o and o.validator:
+            o.validator(v)
+
+        # walk the nested dict
+        root, k = _get_root(key)
+        root[k] = v
+
+        if o.cb:
+            o.cb(key)
+
+
+def _describe_option(pat='', _print_desc=True):
+
+    keys = _select_options(pat)
+    if len(keys) == 0:
+        raise OptionError('No such keys(s)')
+
+    s = u('')
+    for k in keys:  # filter by pat
+        s += _build_option_description(k)
+
+    if _print_desc:
+        print(s)
+    else:
+        return s
+
+
+def _reset_option(pat, silent=False):
+
+    keys = _select_options(pat)
+
+    if len(keys) == 0:
+        raise OptionError('No such keys(s)')
+
+    if len(keys) > 1 and len(pat) < 4 and pat != 'all':
+        raise ValueError('You must specify at least 4 characters when '
+                         'resetting multiple keys, use the special keyword '
+                         '"all" to reset all the options to their default '
+                         'value')
+
+    for k in keys:
+        _set_option(k, _registered_options[k].defval, silent=silent)
+
+
+def get_default_val(pat):
+    key = _get_single_key(pat, silent=True)
+    return _get_registered_option(key).defval
+
+
+class DictWrapper(object):
+
+    """ provide attribute-style access to a nested dict
+    """
+
+    def __init__(self, d, prefix=""):
+        object.__setattr__(self, "d", d)
+        object.__setattr__(self, "prefix", prefix)
+
+    def __repr__(self):
+        buf = StringIO()
+        pprint.pprint(self.d, stream=buf)
+        return buf.getvalue()
+
+    def __setattr__(self, key, val):
+        prefix = object.__getattribute__(self, "prefix")
+        if prefix:
+            prefix += "."
+        prefix += key
+        # you can't set new keys
+        # can you can't overwrite subtrees
+        if key in self.d and not isinstance(self.d[key], dict):
+            _set_option(prefix, val)
         else:
-            query = None
+            raise OptionError("You can only set the value of existing options")
 
-        table_ident = sg.to_identifier(name, quoted=quoted)
+    def __getattr__(self, key):
+        prefix = object.__getattribute__(self, "prefix")
+        if prefix:
+            prefix += "."
+        prefix += key
+        v = object.__getattribute__(self, "d")[key]
+        if isinstance(v, dict):
+            return DictWrapper(v, prefix)
+        else:
+            return _get_option(prefix)
+
+    def __dir__(self):
+        return list(self.d.keys())
+
+
+# For user convenience,  we'd like to have the available options described
+# in the docstring. For dev convenience we'd like to generate the docstrings
+# dynamically instead of maintaining them by hand. To this, we use the
+# class below which wraps functions inside a callable, and converts
+# __doc__ into a propery function. The doctsrings below are templates
+# using the py2.6+ advanced formatting syntax to plug in a concise list
+# of options, and option descriptions.
+
+
+class CallableDynamicDoc(object):
 
-        if query is None:
-            column_defs = [
-                sge.ColumnDef(
-                    this=sg.to_identifier(colname, quoted=quoted),
-                    kind=self.compiler.type_mapper.from_ibis(typ),
-                    constraints=(
-                        None
-                        if typ.nullable
-                        else [sge.ColumnConstraint(kind=sge.NotNullColumnConstraint())]
-                    ),
-                )
-                for colname, typ in (schema or table.schema()).items()
-            ]
+    def __init__(self, func, doc_tmpl):
+        self.__doc_tmpl__ = doc_tmpl
+        self.__func__ = func
 
-            target = sge.Schema(this=table_ident, expressions=column_defs)
+    def __call__(self, *args, **kwds):
+        return self.__func__(*args, **kwds)
+
+    @property
+    def __doc__(self):
+        opts_desc = _describe_option('all', _print_desc=False)
+        opts_list = pp_options_list(list(_registered_options.keys()))
+        return self.__doc_tmpl__.format(opts_desc=opts_desc,
+                                        opts_list=opts_list)
+
+_get_option_tmpl = """
+get_option(pat)
+Retrieves the value of the specified option.
+Available options:
+{opts_list}
+Parameters
+----------
+pat : str
+    Regexp which should match a single option.
+    Note: partial matches are supported for convenience, but unless you use the
+    full option name (e.g. x.y.z.option_name), your code may break in future
+    versions if new options with similar names are introduced.
+Returns
+-------
+result : the value of the option
+Raises
+------
+OptionError : if no such option exists
+Notes
+-----
+The available options with its descriptions:
+{opts_desc}
+"""
+
+_set_option_tmpl = """
+set_option(pat, value)
+Sets the value of the specified option.
+Available options:
+{opts_list}
+Parameters
+----------
+pat : str
+    Regexp which should match a single option.
+    Note: partial matches are supported for convenience, but unless you use the
+    full option name (e.g. x.y.z.option_name), your code may break in future
+    versions if new options with similar names are introduced.
+value :
+    new value of option.
+Returns
+-------
+None
+Raises
+------
+OptionError if no such option exists
+Notes
+-----
+The available options with its descriptions:
+{opts_desc}
+"""
+
+_describe_option_tmpl = """
+describe_option(pat, _print_desc=False)
+Prints the description for one or more registered options.
+Call with not arguments to get a listing for all registered options.
+Available options:
+{opts_list}
+Parameters
+----------
+pat : str
+    Regexp pattern. All matching keys will have their description displayed.
+_print_desc : bool, default True
+    If True (default) the description(s) will be printed to stdout.
+    Otherwise, the description(s) will be returned as a unicode string
+    (for testing).
+Returns
+-------
+None by default, the description(s) as a unicode string if _print_desc
+is False
+Notes
+-----
+The available options with its descriptions:
+{opts_desc}
+"""
+
+_reset_option_tmpl = """
+reset_option(pat)
+Reset one or more options to their default value.
+Pass "all" as argument to reset all options.
+Available options:
+{opts_list}
+Parameters
+----------
+pat : str/regex
+    If specified only options matching `prefix*` will be reset.
+    Note: partial matches are supported for convenience, but unless you
+    use the full option name (e.g. x.y.z.option_name), your code may break
+    in future versions if new options with similar names are introduced.
+Returns
+-------
+None
+Notes
+-----
+The available options with its descriptions:
+{opts_desc}
+"""
+
+# bind the functions with their docstrings into a Callable
+# and use that as the functions exposed in pd.api
+get_option = CallableDynamicDoc(_get_option, _get_option_tmpl)
+set_option = CallableDynamicDoc(_set_option, _set_option_tmpl)
+reset_option = CallableDynamicDoc(_reset_option, _reset_option_tmpl)
+describe_option = CallableDynamicDoc(_describe_option, _describe_option_tmpl)
+options = DictWrapper(_global_config)
+
+#
+# Functions for use by pandas developers, in addition to User - api
+
+
+class option_context(object):
+
+    """
+    Context manager to temporarily set options in the `with` statement context.
+    You need to invoke as ``option_context(pat, val, [(pat, val), ...])``.
+    Examples
+    --------
+    >>> with option_context('display.max_rows', 10, 'display.max_columns', 5):
+            ...
+    """
+
+    def __init__(self, *args):
+        if not (len(args) % 2 == 0 and len(args) >= 2):
+            raise ValueError(
+                'Need to invoke as'
+                'option_context(pat, val, [(pat, val), ...)).'
+            )
+
+        self.ops = list(zip(args[::2], args[1::2]))
+
+    def __enter__(self):
+        undo = []
+        for pat, val in self.ops:
+            undo.append((pat, _get_option(pat, silent=True)))
+
+        self.undo = undo
+
+        for pat, val in self.ops:
+            _set_option(pat, val, silent=True)
+
+    def __exit__(self, *args):
+        if self.undo:
+            for pat, val in self.undo:
+                _set_option(pat, val, silent=True)
+
+
+def register_option(key, defval, doc='', validator=None, cb=None):
+    """Register an option in the package-wide ibis config object
+    Parameters
+    ----------
+    key       - a fully-qualified key, e.g. "x.y.option - z".
+    defval    - the default value of the option
+    doc       - a string description of the option
+    validator - a function of a single argument, should raise `ValueError` if
+                called with a value which is not a legal value for the option.
+    cb        - a function of a single argument "key", which is called
+                immediately after an option value is set/reset. key is
+                the full name of the option.
+    Returns
+    -------
+    Nothing.
+    Raises
+    ------
+    ValueError if `validator` is specified and `defval` is not a valid value.
+    """
+    import tokenize
+    import keyword
+    key = key.lower()
+
+    if key in _registered_options:
+        raise OptionError("Option '%s' has already been registered" % key)
+    if key in _reserved_keys:
+        raise OptionError("Option '%s' is a reserved key" % key)
+
+    # the default value should be legal
+    if validator:
+        validator(defval)
+
+    # walk the nested dict, creating dicts as needed along the path
+    path = key.split('.')
+
+    for k in path:
+        if not bool(re.match('^' + tokenize.Name + '$', k)):
+            raise ValueError("%s is not a valid identifier" % k)
+        if keyword.iskeyword(k):
+            raise ValueError("%s is a python keyword" % k)
+
+    cursor = _global_config
+    for i, p in enumerate(path[:-1]):
+        if not isinstance(cursor, dict):
+            raise OptionError("Path prefix to option '%s' is already an option"
+                              % '.'.join(path[:i]))
+        if p not in cursor:
+            cursor[p] = {}
+        cursor = cursor[p]
+
+    if not isinstance(cursor, dict):
+        raise OptionError("Path prefix to option '%s' is already an option"
+                          % '.'.join(path[:-1]))
+
+    cursor[path[-1]] = defval  # initialize
+
+    # save the option metadata
+    _registered_options[key] = RegisteredOption(key=key, defval=defval,
+                                                doc=doc, validator=validator,
+                                                cb=cb)
+
+
+def deprecate_option(key, msg=None, rkey=None, removal_ver=None):
+    """
+    Mark option `key` as deprecated, if code attempts to access this option,
+    a warning will be produced, using `msg` if given, or a default message
+    if not.
+    if `rkey` is given, any access to the key will be re-routed to `rkey`.
+    Neither the existence of `key` nor that if `rkey` is checked. If they
+    do not exist, any subsequence access will fail as usual, after the
+    deprecation warning is given.
+    Parameters
+    ----------
+    key - the name of the option to be deprecated. must be a fully-qualified
+          option name (e.g "x.y.z.rkey").
+    msg - (Optional) a warning message to output when the key is referenced.
+          if no message is given a default message will be emitted.
+    rkey - (Optional) the name of an option to reroute access to.
+           If specified, any referenced `key` will be re-routed to `rkey`
+           including set/get/reset.
+           rkey must be a fully-qualified option name (e.g "x.y.z.rkey").
+           used by the default message if no `msg` is specified.
+    removal_ver - (Optional) specifies the version in which this option will
+                  be removed. used by the default message if no `msg`
+                  is specified.
+    Returns
+    -------
+    Nothing
+    Raises
+    ------
+    OptionError - if key has already been deprecated.
+    """
+
+    key = key.lower()
+
+    if key in _deprecated_options:
+        raise OptionError("Option '%s' has already been defined as deprecated."
+                          % key)
+
+    _deprecated_options[key] = DeprecatedOption(key, msg, rkey, removal_ver)
+
+
+#
+# functions internal to the module
+
+def _select_options(pat):
+    """returns a list of keys matching `pat`
+    if pat=="all", returns all registered options
+    """
+
+    # short-circuit for exact key
+    if pat in _registered_options:
+        return [pat]
+
+    # else look through all of them
+    keys = sorted(_registered_options.keys())
+    if pat == 'all':  # reserved key
+        return keys
+
+    return [k for k in keys if re.search(pat, k, re.I)]
+
+
+def _get_root(key):
+    path = key.split('.')
+    cursor = _global_config
+    for p in path[:-1]:
+        cursor = cursor[p]
+    return cursor, path[-1]
+
+
+def _is_deprecated(key):
+    """ Returns True if the given option has been deprecated """
+
+    key = key.lower()
+    return key in _deprecated_options
+
+
+def _get_deprecated_option(key):
+    """
+    Retrieves the metadata for a deprecated option, if `key` is deprecated.
+    Returns
+    -------
+    DeprecatedOption (namedtuple) if key is deprecated, None otherwise
+    """
+
+    try:
+        d = _deprecated_options[key]
+    except KeyError:
+        return None
+    else:
+        return d
+
+
+def _get_registered_option(key):
+    """
+    Retrieves the option metadata if `key` is a registered option.
+    Returns
+    -------
+    RegisteredOption (namedtuple) if key is deprecated, None otherwise
+    """
+    return _registered_options.get(key)
+
+
+def _translate_key(key):
+    """
+    if key id deprecated and a replacement key defined, will return the
+    replacement key, otherwise returns `key` as - is
+    """
+
+    d = _get_deprecated_option(key)
+    if d:
+        return d.rkey or key
+    else:
+        return key
+
+
+def _warn_if_deprecated(key):
+    """
+    Checks if `key` is a deprecated option and if so, prints a warning.
+    Returns
+    -------
+    bool - True if `key` is deprecated, False otherwise.
+    """
+
+    d = _get_deprecated_option(key)
+    if d:
+        if d.msg:
+            print(d.msg)
+            warnings.warn(d.msg, DeprecationWarning)
         else:
-            target = table_ident
+            msg = "'%s' is deprecated" % key
+            if d.removal_ver:
+                msg += ' and will be removed in %s' % d.removal_ver
+            if d.rkey:
+                msg += ", please use '%s' instead." % d.rkey
+            else:
+                msg += ', please refrain from using it.'
 
-        create_stmt = sge.Create(
-            kind="TABLE",
-            this=target,
-            properties=sge.Properties(expressions=properties),
-            expression=query,
-            replace=overwrite,
-        )
-
-        with self._safe_raw_sql(create_stmt):
-            pass
-
-        return self.table(name, database=database)
-
-    def truncate_table(
-        self, name: str, database: str | None = None, schema: str | None = None
-    ) -> None:
-        """Delete all rows from a table.
-
-        Parameters
-        ----------
-        name
-            Table name
-        database
-            Database name
-        schema
-            Schema name
-
-        """
-        # datafusion doesn't support `TRUNCATE TABLE` so we use `DELETE FROM`
-        #
-        # however datafusion as of 34.0.0 doesn't implement DELETE DML yet
-        table_loc = self._warn_and_create_table_loc(database, schema)
-        catalog, db = self._to_catalog_db_tuple(table_loc)
-
-        ident = sg.table(name, db=db, catalog=catalog).sql(self.name)
-        with self._safe_raw_sql(sge.delete(ident)):
-            pass
+            warnings.warn(msg, DeprecationWarning)
+        return True
+    return False
+
+
+def _build_option_description(k):
+    """ Builds a formatted description of a registered option and prints it """
+
+    o = _get_registered_option(k)
+    d = _get_deprecated_option(k)
+
+    s = u('%s ') % k
+
+    if o.doc:
+        s += '\n'.join(o.doc.strip().split('\n'))
+    else:
+        s += 'No description available.'
+
+    if o:
+        s += u('\n    [default: %s] [currently: %s]') % (o.defval,
+                                                         _get_option(k, True))
+
+    if d:
+        s += u('\n    (Deprecated')
+        s += (u(', use `%s` instead.') % d.rkey if d.rkey else '')
+        s += u(')')
+
+    s += '\n\n'
+    return s
+
+
+def pp_options_list(keys, width=80, _print=False):
+    """ Builds a concise listing of available options, grouped by prefix """
+
+    from textwrap import wrap
+    from itertools import groupby
+
+    def pp(name, ks):
+        pfx = ('- ' + name + '.[' if name else '')
+        ls = wrap(', '.join(ks), width, initial_indent=pfx,
+                  subsequent_indent='  ', break_long_words=False)
+        if ls and ls[-1] and name:
+            ls[-1] = ls[-1] + ']'
+        return ls
+
+    ls = []
+    singles = [x for x in sorted(keys) if x.find('.') < 0]
+    if singles:
+        ls += pp('', singles)
+    keys = [x for x in keys if x.find('.') >= 0]
+
+    for k, g in groupby(sorted(keys), lambda x: x[:x.rfind('.')]):
+        ks = [x[len(k) + 1:] for x in list(g)]
+        ls += pp(k, ks)
+    s = '\n'.join(ls)
+    if _print:
+        print(s)
+    else:
+        return s
+
+
+#
+# helpers
+
+
+@contextmanager
+def config_prefix(prefix):
+    """contextmanager for multiple invocations of API  with a common prefix
+    supported API functions: (register / get / set )__option
+    Warning: This is not thread - safe, and won't work properly if you import
+    the API functions into your module using the "from x import y" construct.
+    Example:
+    import ibis.config as cf
+    with cf.config_prefix("display.font"):
+        cf.register_option("color", "red")
+        cf.register_option("size", " 5 pt")
+        cf.set_option(size, " 6 pt")
+        cf.get_option(size)
+        ...
+        etc'
+    will register options "display.font.color", "display.font.size", set the
+    value of "display.font.size"... and so on.
+    """
+
+    # Note: reset_option relies on set_option, and on key directly
+    # it does not fit in to this monkey-patching scheme
+
+    global register_option, get_option, set_option, reset_option
+
+    def wrap(func):
+
+        def inner(key, *args, **kwds):
+            pkey = '%s.%s' % (prefix, key)
+            return func(pkey, *args, **kwds)
+
+        return inner
+
+    _register_option = register_option
+    _get_option = get_option
+    _set_option = set_option
+    set_option = wrap(set_option)
+    get_option = wrap(get_option)
+    register_option = wrap(register_option)
+    yield None
+    set_option = _set_option
+    get_option = _get_option
+    register_option = _register_option
+
+
+# These factories and methods are handy for use as the validator
+# arg in register_option
+
+def is_type_factory(_type):
+    """
+    Parameters
+    ----------
+    `_type` - a type to be compared against (e.g. type(x) == `_type`)
+    Returns
+    -------
+    validator - a function of a single argument x , which returns the
+                True if type(x) is equal to `_type`
+    """
+
+    def inner(x):
+        if type(x) != _type:
+            raise ValueError("Value must have type '%s'" % str(_type))
+
+    return inner
+
+
+def is_instance_factory(_type):
+    """
+    Parameters
+    ----------
+    `_type` - the type to be checked against
+    Returns
+    -------
+    validator - a function of a single argument x , which returns the
+                True if x is an instance of `_type`
+    """
+    if isinstance(_type, (tuple, list)):
+        _type = tuple(_type)
+        type_repr = "|".join(map(str, _type))
+    else:
+        type_repr = "'%s'" % _type
+
+    def inner(x):
+        if not isinstance(x, _type):
+            raise ValueError("Value must be an instance of %s" % type_repr)
+
+    return inner
+
+
+def is_one_of_factory(legal_values):
+    def inner(x):
+        if x not in legal_values:
+            pp_values = map(str, legal_values)
+            raise ValueError("Value must be one of %s"
+                             % str("|".join(pp_values)))
+
+    return inner
+
+# common type validators, for convenience
+# usage: register_option(... , validator = is_int)
+is_int = is_type_factory(int)
+is_bool = is_type_factory(bool)
+is_float = is_type_factory(float)
+is_str = is_type_factory(str)
+# is_unicode = is_type_factory(compat.text_type)
+is_text = is_instance_factory((str, bytes))
```

### Comparing `ibis_framework-9.0.0.dev686/ibis/backends/duckdb/__init__.py` & `ibis-framework-v0.6.0/ibis/impala/client.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,1570 +1,1956 @@
-"""DuckDB backend."""
+# Copyright 2014 Cloudera Inc.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+# http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from posixpath import join as pjoin
+import re
+import six
+import threading
+import time
+import weakref
+
+import hdfs
+import numpy as np
+import pandas as pd
+
+import ibis.common as com
+
+from ibis.config import options
+from ibis.client import (Query, AsyncQuery, Database,
+                         DatabaseEntity, SQLClient)
+from ibis.compat import lzip
+from ibis.filesystems import HDFS, WebHDFS
+from ibis.impala import udf, ddl
+from ibis.impala.compat import impyla, ImpylaError, HS2Error
+from ibis.impala.compiler import build_ast
+from ibis.util import log
+from ibis.sql.compiler import DDL
+import ibis.expr.datatypes as dt
+import ibis.expr.operations as ops
+import ibis.expr.types as ir
+import ibis.util as util
 
-from __future__ import annotations
 
-import ast
-import contextlib
-import os
-import urllib
-import warnings
-from operator import itemgetter
-from pathlib import Path
-from typing import TYPE_CHECKING, Any
-
-import duckdb
-import pyarrow as pa
-import pyarrow_hotfix  # noqa: F401
-import sqlglot as sg
-import sqlglot.expressions as sge
+if six.PY2:
+    import Queue as queue
+else:
+    import queue
 
-import ibis
-import ibis.common.exceptions as exc
-import ibis.expr.operations as ops
-import ibis.expr.schema as sch
-import ibis.expr.types as ir
-from ibis import util
-from ibis.backends import CanCreateDatabase, CanCreateSchema, UrlFromPath
-from ibis.backends.duckdb.compiler import DuckDBCompiler
-from ibis.backends.duckdb.converter import DuckDBPandasData
-from ibis.backends.sql import SQLBackend
-from ibis.backends.sql.compiler import STAR, C, F
-from ibis.expr.operations.udf import InputType
 
-if TYPE_CHECKING:
-    from collections.abc import Iterable, Mapping, MutableMapping, Sequence
+class ImpalaDatabase(Database):
 
-    import pandas as pd
-    import torch
-    from fsspec import AbstractFileSystem
+    def create_table(self, table_name, obj=None, **kwargs):
+        """
+        Dispatch to ImpalaClient.create_table. See docs for more
+        """
+        return self.client.create_table(table_name, obj=obj,
+                                        database=self.name, **kwargs)
 
+    def list_udfs(self, like=None):
+        return self.client.list_udfs(like=self._qualify_like(like),
+                                     database=self.name)
 
-def normalize_filenames(source_list):
-    # Promote to list
-    source_list = util.promote_list(source_list)
+    def list_udas(self, like=None):
+        return self.client.list_udas(like=self._qualify_like(like),
+                                     database=self.name)
 
-    return list(map(util.normalize_filename, source_list))
 
+class ImpalaConnection(object):
 
-_UDF_INPUT_TYPE_MAPPING = {
-    InputType.PYARROW: duckdb.functional.ARROW,
-    InputType.PYTHON: duckdb.functional.NATIVE,
-}
+    """
+    Database connection wrapper
+    """
 
+    def __init__(self, pool_size=8, database='default', **params):
+        self.params = params
+        self.database = database
 
-class _Settings:
-    def __init__(self, con: duckdb.DuckDBPyConnection) -> None:
-        self.con = con
+        self.lock = threading.Lock()
 
-    def __getitem__(self, key: str) -> Any:
-        maybe_value = self.con.execute(
-            f"select value from duckdb_settings() where name = '{key}'"
-        ).fetchone()
-        if maybe_value is not None:
-            return maybe_value[0]
-        raise KeyError(key)
-
-    def __setitem__(self, key, value):
-        self.con.execute(f"SET {key} = '{value}'")
-
-    def __repr__(self):
-        ((kv,),) = self.con.execute(
-            "select map(array_agg(name), array_agg(value)) from duckdb_settings()"
-        ).fetch()
-
-        return repr(dict(zip(kv["key"], kv["value"])))
-
-
-class Backend(SQLBackend, CanCreateDatabase, CanCreateSchema, UrlFromPath):
-    name = "duckdb"
-    compiler = DuckDBCompiler()
+        self.options = {}
 
-    def _define_udf_translation_rules(self, expr):
-        """No-op: UDF translation rules are defined in the compiler."""
+        self.connection_pool = queue.Queue(pool_size)
+        self.connection_pool_size = 0
+        self.max_pool_size = pool_size
 
-    @property
-    def settings(self) -> _Settings:
-        return _Settings(self.con)
+        self._connections = weakref.WeakValueDictionary()
 
-    @property
-    def current_catalog(self) -> str:
-        with self._safe_raw_sql(sg.select(self.compiler.f.current_database())) as cur:
-            [(db,)] = cur.fetchall()
-        return db
+        self.ping()
 
-    @property
-    def current_database(self) -> str:
-        with self._safe_raw_sql(sg.select(self.compiler.f.current_schema())) as cur:
-            [(db,)] = cur.fetchall()
-        return db
-
-    # TODO(kszucs): should be moved to the base SQLGLot backend
-    def raw_sql(self, query: str | sg.Expression, **kwargs: Any) -> Any:
-        with contextlib.suppress(AttributeError):
-            query = query.sql(dialect=self.name)
-        return self.con.execute(query, **kwargs)
-
-    def _to_sqlglot(
-        self, expr: ir.Expr, limit: str | None = None, params=None, **_: Any
-    ):
-        sql = super()._to_sqlglot(expr, limit=limit, params=params)
-
-        table_expr = expr.as_table()
-        geocols = frozenset(
-            name for name, typ in table_expr.schema().items() if typ.is_geospatial()
-        )
-
-        if not geocols:
-            return sql
-
-        return sg.select(
-            *(
-                self.compiler.f.st_aswkb(
-                    sg.column(col, quoted=self.compiler.quoted)
-                ).as_(col)
-                if col in geocols
-                else col
-                for col in table_expr.columns
-            )
-        ).from_(sql.subquery())
-
-    def create_table(
-        self,
-        name: str,
-        obj: pd.DataFrame | pa.Table | ir.Table | None = None,
-        *,
-        schema: ibis.Schema | None = None,
-        database: str | None = None,
-        temp: bool = False,
-        overwrite: bool = False,
-    ):
-        """Create a table in DuckDB.
-
-        Parameters
-        ----------
-        name
-            Name of the table to create
-        obj
-            The data with which to populate the table; optional, but at least
-            one of `obj` or `schema` must be specified
-        schema
-            The schema of the table to create; optional, but at least one of
-            `obj` or `schema` must be specified
-        database
-            The name of the database in which to create the table; if not
-            passed, the current database is used.
-        temp
-            Create a temporary table
-        overwrite
-            If `True`, replace the table if it already exists, otherwise fail
-            if the table exists
+    def set_options(self, options):
+        self.options.update(options)
 
+    def close(self):
+        """
+        Close all open Impyla sessions
         """
-        if obj is None and schema is None:
-            raise ValueError("Either `obj` or `schema` must be specified")
+        for k, con in self._connections.items():
+            con.close()
 
-        properties = []
+    def set_database(self, name):
+        self.database = name
 
-        if temp:
-            properties.append(sge.TemporaryProperty())
+    def disable_codegen(self, disabled=True):
+        key = 'DISABLE_CODEGEN'
+        if disabled:
+            self.options[key] = '1'
+        elif key in self.options:
+            del self.options[key]
 
-        if obj is not None:
-            if not isinstance(obj, ir.Expr):
-                table = ibis.memtable(obj)
+    def execute(self, query, async=False):
+        if isinstance(query, DDL):
+            query = query.compile()
+
+        cursor = self._get_cursor()
+        self.log(query)
+
+        try:
+            cursor.execute(query, async=async)
+        except:
+            cursor.release()
+
+            import traceback
+            buf = six.StringIO()
+            traceback.print_exc(file=buf)
+            self.error('Exception caused by {0}: {1}'.format(query,
+                                                             buf.getvalue()))
+            raise
+
+        return cursor
+
+    def log(self, msg):
+        log(msg)
+
+    def error(self, msg):
+        self.log(msg)
+
+    def fetchall(self, query):
+        with self.execute(query) as cur:
+            results = cur.fetchall()
+        return results
+
+    def _get_cursor(self):
+        try:
+            cur = self.connection_pool.get(False)
+            if (cur.database != self.database or
+                    cur.options != self.options):
+                cur = self._new_cursor()
+
+            return cur
+        except queue.Empty:
+            if self.connection_pool_size < self.max_pool_size:
+                cursor = self._new_cursor()
+                self.connection_pool_size += 1
+                return cursor
             else:
-                table = obj
+                raise com.InternalError('Too many concurrent / hung queries')
+
+    def _new_cursor(self):
+        params = self.params.copy()
+        con = impyla.connect(database=self.database, **params)
+
+        self._connections[id(con)] = con
+
+        # make sure the connection works
+        cursor = con.cursor(convert_types=True)
+        cursor.ping()
+
+        wrapper = ImpalaCursor(cursor, self, con, self.database,
+                               self.options.copy())
+        wrapper.set_options()
+
+        return wrapper
+
+    def ping(self):
+        self._new_cursor()
+
+    def release(self, cur):
+        self.connection_pool.put(cur)
 
-            self._run_pre_execute_hooks(table)
 
-            query = self._to_sqlglot(table)
+class ImpalaCursor(object):
+
+    def __init__(self, cursor, con, impyla_con, database,
+                 options):
+        self.cursor = cursor
+        self.con = con
+        self.impyla_con = impyla_con
+        self.database = database
+        self.options = options
+
+    def __del__(self):
+        self._close_cursor()
+
+    def _close_cursor(self):
+        try:
+            self.cursor.close()
+        except HS2Error as e:
+            # connection was closed elsewhere
+            if 'invalid session' not in e.args[0].lower():
+                raise
+
+    def __enter__(self):
+        return self
+
+    def __exit__(self, type, value, tb):
+        self.release()
+
+    def set_options(self):
+        for k, v in self.options.items():
+            query = 'SET {0}={1}'.format(k, v)
+            self.cursor.execute(query)
+
+    @property
+    def description(self):
+        return self.cursor.description
+
+    def release(self):
+        self.con.release(self)
+
+    def execute(self, stmt, async=False):
+        self.cursor.execute_async(stmt)
+        if async:
+            return
         else:
-            query = None
+            self._wait_synchronous()
 
-        column_defs = [
-            sge.ColumnDef(
-                this=sg.to_identifier(colname, quoted=self.compiler.quoted),
-                kind=self.compiler.type_mapper.from_ibis(typ),
-                constraints=(
-                    None
-                    if typ.nullable
-                    else [sge.ColumnConstraint(kind=sge.NotNullColumnConstraint())]
-                ),
-            )
-            for colname, typ in (schema or table.schema()).items()
-        ]
+    def _wait_synchronous(self):
+        # Wait to finish, but cancel if KeyboardInterrupt
+        from impala.hiveserver2 import OperationalError
+        loop_start = time.time()
+
+        def _sleep_interval(start_time):
+            elapsed = time.time() - start_time
+            if elapsed < 0.05:
+                return 0.01
+            elif elapsed < 1.0:
+                return 0.05
+            elif elapsed < 10.0:
+                return 0.1
+            elif elapsed < 60.0:
+                return 0.5
+            return 1.0
 
-        if overwrite:
-            temp_name = util.gen_name("duckdb_table")
+        cur = self.cursor
+        try:
+            while True:
+                state = cur.status()
+                if self.cursor._op_state_is_error(state):
+                    raise OperationalError("Operation is in ERROR_STATE")
+                if not cur._op_state_is_executing(state):
+                    break
+                time.sleep(_sleep_interval(loop_start))
+        except KeyboardInterrupt:
+            print('Canceling query')
+            self.cancel()
+            raise
+
+    def is_finished(self):
+        return not self.is_executing()
+
+    def is_executing(self):
+        return self.cursor.is_executing()
+
+    def cancel(self):
+        self.cursor.cancel_operation()
+
+    def fetchall(self, columnar=False):
+        if columnar:
+            return self.cursor.fetchcolumnar()
         else:
-            temp_name = name
+            return self.cursor.fetchall()
 
-        initial_table = sg.table(
-            temp_name, catalog=database, quoted=self.compiler.quoted
-        )
-        target = sge.Schema(this=initial_table, expressions=column_defs)
-
-        create_stmt = sge.Create(
-            kind="TABLE",
-            this=target,
-            properties=sge.Properties(expressions=properties),
-        )
-
-        # This is the same table as initial_table unless overwrite == True
-        final_table = sg.table(name, catalog=database, quoted=self.compiler.quoted)
-        with self._safe_raw_sql(create_stmt) as cur:
-            if query is not None:
-                insert_stmt = sge.insert(query, into=initial_table).sql(self.name)
-                cur.execute(insert_stmt).fetchall()
-
-            if overwrite:
-                cur.execute(
-                    sge.Drop(kind="TABLE", this=final_table, exists=True).sql(self.name)
-                )
-                # TODO: This branching should be removed once DuckDB >=0.9.3 is
-                # our lower bound (there's an upstream bug in 0.9.2 that
-                # disallows renaming temp tables)
-                # We should (pending that release) be able to remove the if temp
-                # branch entirely.
-                if temp:
-                    cur.execute(
-                        sge.Create(
-                            kind="TABLE",
-                            this=final_table,
-                            expression=sg.select(STAR).from_(initial_table),
-                            properties=sge.Properties(expressions=properties),
-                        ).sql(self.name)
-                    )
-                    cur.execute(
-                        sge.Drop(kind="TABLE", this=initial_table, exists=True).sql(
-                            self.name
-                        )
-                    )
-                else:
-                    cur.execute(
-                        sge.AlterTable(
-                            this=initial_table,
-                            actions=[sge.RenameTable(this=final_table)],
-                        ).sql(self.name)
-                    )
-
-        return self.table(name, database=database)
-
-    def _load_into_cache(self, name, expr):
-        self.create_table(name, expr, schema=expr.schema(), temp=True)
-
-    def _clean_up_cached_table(self, op):
-        self.drop_table(op.name)
-
-    def table(
-        self, name: str, schema: str | None = None, database: str | None = None
-    ) -> ir.Table:
-        """Construct a table expression.
-
-        Parameters
-        ----------
-        name
-            Table name
-        schema
-            [deprecated] Schema name
-        database
-            Database name
-
-        Returns
-        -------
-        Table
-            Table expression
-
-        """
-        table_loc = self._warn_and_create_table_loc(database, schema)
-
-        catalog, database = None, None
-        if table_loc is not None:
-            catalog = table_loc.catalog or None
-            database = table_loc.db or None
-
-        table_schema = self.get_schema(name, catalog=catalog, database=database)
-        # load geospatial only if geo columns
-        if any(typ.is_geospatial() for typ in table_schema.types):
-            self.load_extension("spatial")
-        return ops.DatabaseTable(
-            name,
-            schema=table_schema,
-            source=self,
-            namespace=ops.Namespace(catalog=catalog, database=database),
-        ).to_expr()
-
-    def get_schema(
-        self,
-        table_name: str,
-        *,
-        catalog: str | None = None,
-        database: str | None = None,
-    ) -> sch.Schema:
-        """Compute the schema of a `table`.
-
-        Parameters
-        ----------
-        table_name
-            May **not** be fully qualified. Use `database` if you want to
-            qualify the identifier.
-        catalog
-            Catalog name
-        database
-            Database name
-
-        Returns
-        -------
-        sch.Schema
-            Ibis schema
-
-        """
-        conditions = [sg.column("table_name").eq(sge.convert(table_name))]
-
-        if catalog is not None:
-            conditions.append(sg.column("table_catalog").eq(sge.convert(catalog)))
-
-        if database is not None:
-            conditions.append(sg.column("table_schema").eq(sge.convert(database)))
-
-        query = (
-            sg.select(
-                "column_name",
-                "data_type",
-                sg.column("is_nullable").eq(sge.convert("YES")).as_("nullable"),
-            )
-            .from_(sg.table("columns", db="information_schema"))
-            .where(sg.and_(*conditions))
-            .order_by("ordinal_position")
-        )
-
-        with self._safe_raw_sql(query) as cur:
-            meta = cur.fetch_arrow_table()
-
-        if not meta:
-            raise exc.IbisError(f"Table not found: {table_name!r}")
-
-        names = meta["column_name"].to_pylist()
-        types = meta["data_type"].to_pylist()
-        nullables = meta["nullable"].to_pylist()
-
-        return sch.Schema(
-            {
-                name: self.compiler.type_mapper.from_string(typ, nullable=nullable)
-                for name, typ, nullable in zip(names, types, nullables)
-            }
-        )
-
-    @contextlib.contextmanager
-    def _safe_raw_sql(self, *args, **kwargs):
-        yield self.raw_sql(*args, **kwargs)
-
-    def list_catalogs(self, like: str | None = None) -> list[str]:
-        col = "catalog_name"
-        query = sg.select(sge.Distinct(expressions=[sg.column(col)])).from_(
-            sg.table("schemata", db="information_schema")
-        )
-        with self._safe_raw_sql(query) as cur:
-            result = cur.fetch_arrow_table()
-        dbs = result[col]
-        return self._filter_with_like(dbs.to_pylist(), like)
-
-    def list_databases(
-        self, like: str | None = None, catalog: str | None = None
-    ) -> list[str]:
-        col = "schema_name"
-        query = sg.select(sge.Distinct(expressions=[sg.column(col)])).from_(
-            sg.table("schemata", db="information_schema")
-        )
-
-        if catalog is not None:
-            query = query.where(sg.column("catalog_name").eq(sge.convert(catalog)))
-
-        with self._safe_raw_sql(query) as cur:
-            out = cur.fetch_arrow_table()
-        return self._filter_with_like(out[col].to_pylist(), like=like)
-
-    @staticmethod
-    def _convert_kwargs(kwargs: MutableMapping) -> None:
-        read_only = str(kwargs.pop("read_only", "False")).capitalize()
+
+class ImpalaQuery(Query):
+
+    def _fetch(self, cursor):
+        batches = cursor.fetchall(columnar=True)
+        names = [x[0] for x in cursor.description]
+        return _column_batches_to_dataframe(names, batches)
+
+    def _db_type_to_dtype(self, db_type):
+        return _HS2_TTypeId_to_dtype[db_type]
+
+
+def _column_batches_to_dataframe(names, batches):
+    from ibis.compat import zip as czip
+    cols = {}
+    for name, chunks in czip(names, czip(*[b.columns for b in batches])):
+        cols[name] = _chunks_to_pandas_array(chunks)
+    return pd.DataFrame(cols, columns=names)
+
+
+def _chunks_to_pandas_array(chunks):
+    total_length = 0
+    have_nulls = False
+    for c in chunks:
+        total_length += len(c)
+        have_nulls = have_nulls or c.nulls.any()
+
+    type_ = chunks[0].data_type
+    numpy_type = _HS2_TTypeId_to_dtype[type_]
+
+    def fill_nonnull(target, chunks):
+        pos = 0
+        for c in chunks:
+            target[pos: pos + len(c)] = c.values
+            pos += len(c.values)
+
+    def fill(target, chunks, na_rep):
+        pos = 0
+        for c in chunks:
+            nulls = c.nulls.copy()
+            nulls.bytereverse()
+            bits = np.frombuffer(nulls.tobytes(), dtype='u1')
+            mask = np.unpackbits(bits).view(np.bool_)
+
+            k = len(c)
+
+            dest = target[pos: pos + k]
+            dest[:] = c.values
+            dest[mask[:k]] = na_rep
+
+            pos += k
+
+    if have_nulls:
+        if numpy_type in ('bool', 'datetime64[ns]'):
+            target = np.empty(total_length, dtype='O')
+            na_rep = np.nan
+        elif numpy_type.startswith('int'):
+            target = np.empty(total_length, dtype='f8')
+            na_rep = np.nan
+        else:
+            target = np.empty(total_length, dtype=numpy_type)
+            na_rep = np.nan
+
+        fill(target, chunks, na_rep)
+    else:
+        target = np.empty(total_length, dtype=numpy_type)
+        fill_nonnull(target, chunks)
+
+    return target
+
+
+class ImpalaAsyncQuery(ImpalaQuery, AsyncQuery):
+
+    def __init__(self, client, ddl):
+        super(ImpalaAsyncQuery, self).__init__(client, ddl)
+        self._cursor = None
+        self._exception = None
+        self._execute_thread = None
+        self._execute_complete = False
+        self._operation_active = False
+
+    def __del__(self):
+        if self._cursor is not None:
+            self._cursor.release()
+
+    def execute(self):
+        if self._operation_active:
+            raise com.IbisError('operation already active')
+        con = self.client.con
+
+        # XXX: there is codegen overhead somewhere which causes execute_async
+        # to block, unfortunately. This threading hack works around it
+        def _async_execute():
+            try:
+                self._cursor = con.execute(self.compiled_ddl, async=True)
+            except Exception as e:
+                self._exception = e
+            self._execute_complete = True
+
+        self._execute_complete = False
+        self._operation_active = True
+        self._execute_thread = threading.Thread(target=_async_execute)
+        self._execute_thread.start()
+        return self
+
+    def _wait_execute(self):
+        if not self._operation_active:
+            raise com.IbisError('No active query')
+        if self._execute_thread.is_alive():
+            self._execute_thread.join()
+        elif self._exception is not None:
+            raise self._exception
+
+    def is_finished(self):
+        """
+        Return True if the operation is finished
+        """
+        from impala.error import ProgrammingError
+        self._wait_execute()
         try:
-            kwargs["read_only"] = ast.literal_eval(read_only)
-        except ValueError as e:
-            raise ValueError(
-                f"invalid value passed to ast.literal_eval: {read_only!r}"
-            ) from e
+            return self._cursor.is_finished()
+        except ProgrammingError as e:
+            if 'state is not available' in e.args[0]:
+                return True
+            raise
+
+    def cancel(self):
+        """
+        Cancel the query (or attempt to)
+        """
+        self._wait_execute()
+        return self._cursor.cancel()
+
+    def status(self):
+        """
+        Retrieve Impala query status
+        """
+        self._wait_execute()
+        return self._cursor.status()
+
+    def wait(self, progress_bar=True):
+        raise NotImplementedError
+
+    def get_result(self):
+        """
+        Presuming the operation is completed, return the cursor result as would
+        be returned by the synchronous query API
+        """
+        self._wait_execute()
+        result = self._fetch(self._cursor)
+        return self._wrap_result(result)
+
+
+_HS2_TTypeId_to_dtype = {
+    'BOOLEAN': 'bool',
+    'TINYINT': 'int8',
+    'SMALLINT': 'int16',
+    'INT': 'int32',
+    'BIGINT': 'int64',
+    'TIMESTAMP': 'datetime64[ns]',
+    'FLOAT': 'float32',
+    'DOUBLE': 'float64',
+    'STRING': 'object',
+    'DECIMAL': 'object',
+    'BINARY': 'object',
+    'VARCHAR': 'object',
+    'CHAR': 'object'
+}
+
+
+class ImpalaClient(SQLClient):
+
+    """
+    An Ibis client interface that uses Impala
+    """
+
+    database_class = ImpalaDatabase
+    sync_query = ImpalaQuery
+    async_query = ImpalaAsyncQuery
+
+    def __init__(self, con, hdfs_client=None, **params):
+        self.con = con
+
+        if isinstance(hdfs_client, hdfs.Client):
+            hdfs_client = WebHDFS(hdfs_client)
+        elif hdfs_client is not None and not isinstance(hdfs_client, HDFS):
+            raise TypeError(hdfs_client)
+
+        self._hdfs = hdfs_client
+
+        self._temp_objects = weakref.WeakValueDictionary()
+
+        self._ensure_temp_db_exists()
+
+    def _build_ast(self, expr):
+        return build_ast(expr)
+
+    def _get_hdfs(self):
+        if self._hdfs is None:
+            raise com.IbisError('No HDFS connection; must pass connection '
+                                'using the hdfs_client argument to '
+                                'ibis.impala.connect')
+        return self._hdfs
+
+    def _set_hdfs(self, hdfs):
+        if not isinstance(hdfs, HDFS):
+            raise TypeError('must be HDFS instance')
+        self._hdfs = hdfs
+
+    hdfs = property(fget=_get_hdfs, fset=_set_hdfs)
 
     @property
-    def version(self) -> str:
-        # TODO: there is a `PRAGMA version` we could use instead
-        import importlib.metadata
-
-        return importlib.metadata.version("duckdb")
-
-    def do_connect(
-        self,
-        database: str | Path = ":memory:",
-        read_only: bool = False,
-        temp_directory: str | Path | None = None,
-        extensions: Sequence[str] | None = None,
-        **config: Any,
-    ) -> None:
-        """Create an Ibis client connected to a DuckDB database.
+    def _table_expr_klass(self):
+        return ImpalaTable
+
+    def close(self):
+        """
+        Close Impala connection and drop any temporary objects
+        """
+        for k, v in self._temp_objects.items():
+            try:
+                v.drop()
+            except HS2Error:
+                pass
+
+        self.con.close()
+
+    def disable_codegen(self, disabled=True):
+        """
+        Turn off or on LLVM codegen in Impala query execution
 
         Parameters
         ----------
-        database
-            Path to a duckdb database.
-        read_only
-            Whether the database is read-only.
-        temp_directory
-            Directory to use for spilling to disk. Only set by default for
-            in-memory connections.
-        extensions
-            A list of duckdb extensions to install/load upon connection.
-        config
-            DuckDB configuration parameters. See the [DuckDB configuration
-            documentation](https://duckdb.org/docs/sql/configuration) for
-            possible configuration values.
+        disabled : boolean, default True
+          To disable codegen, pass with no argument or True. To enable codegen,
+          pass False
+        """
+        self.con.disable_codegen(disabled)
 
-        Examples
-        --------
-        >>> import ibis
-        >>> ibis.duckdb.connect("database.ddb", threads=4, memory_limit="1GB")
-        <ibis.backends.duckdb.Backend object at ...>
-
-        """
-        if (
-            not isinstance(database, Path)
-            and database != ":memory:"
-            and not database.startswith(("md:", "motherduck:"))
-        ):
-            database = Path(database).absolute()
-
-        if temp_directory is None:
-            temp_directory = (
-                Path(os.environ.get("XDG_CACHE_HOME", Path.home() / ".cache"))
-                / "ibis-duckdb"
-                / str(os.getpid())
-            )
+    def log(self, msg):
+        log(msg)
+
+    def _fully_qualified_name(self, name, database):
+        if ddl._is_fully_qualified(name):
+            return name
+
+        database = database or self.current_database
+        return '{0}.`{1}`'.format(database, name)
+
+    def list_tables(self, like=None, database=None):
+        """
+        List tables in the current (or indicated) database. Like the SHOW
+        TABLES command in the impala-shell.
+
+        Parameters
+        ----------
+        like : string, default None
+          e.g. 'foo*' to match all tables starting with 'foo'
+        database : string, default None
+          If not passed, uses the current/default database
+
+        Returns
+        -------
+        tables : list of strings
+        """
+        statement = 'SHOW TABLES'
+        if database:
+            statement += ' IN {0}'.format(database)
+        if like:
+            m = ddl.fully_qualified_re.match(like)
+            if m:
+                database, quoted, unquoted = m.groups()
+                like = quoted or unquoted
+                return self.list_tables(like=like, database=database)
+            statement += " LIKE '{0}'".format(like)
+
+        with self._execute(statement, results=True) as cur:
+            result = self._get_list(cur)
+
+        return result
+
+    def _get_list(self, cur):
+        tuples = cur.fetchall()
+        if len(tuples) > 0:
+            return list(lzip(*tuples)[0])
         else:
-            Path(temp_directory).mkdir(parents=True, exist_ok=True)
-            config["temp_directory"] = str(temp_directory)
+            return []
 
-        self.con = duckdb.connect(str(database), config=config, read_only=read_only)
+    def set_database(self, name):
+        """
+        Set the default database scope for client
+        """
+        self.con.set_database(name)
 
-        # Load any pre-specified extensions
-        if extensions is not None:
-            self._load_extensions(extensions)
+    def exists_database(self, name):
+        """
+        Checks if a given database exists
 
-        # Default timezone
-        with self._safe_raw_sql("SET TimeZone = 'UTC'"):
-            pass
+        Parameters
+        ----------
+        name : string
+          Database name
 
-        self._record_batch_readers_consumed = {}
+        Returns
+        -------
+        if_exists : boolean
+        """
+        return len(self.list_databases(like=name)) > 0
 
-    def _load_extensions(
-        self, extensions: list[str], force_install: bool = False
-    ) -> None:
-        f = self.compiler.f
-        query = (
-            sg.select(f.unnest(f.list_append(C.aliases, C.extension_name)))
-            .from_(f.duckdb_extensions())
-            .where(sg.and_(C.installed, C.loaded))
-        )
-        with self._safe_raw_sql(query) as cur:
-            installed = map(itemgetter(0), cur.fetchall())
-            # Install and load all other extensions
-            todo = frozenset(extensions).difference(installed)
-            for extension in todo:
-                cur.install_extension(extension, force_install=force_install)
-                cur.load_extension(extension)
-
-    def load_extension(self, extension: str, force_install: bool = False) -> None:
-        """Install and load a duckdb extension by name or path.
+    def create_database(self, name, path=None, force=False):
+        """
+        Create a new Impala database
 
         Parameters
         ----------
-        extension
-            The extension name or path.
-        force_install
-            Force reinstallation of the extension.
-
-        """
-        self._load_extensions([extension], force_install=force_install)
-
-    def create_database(
-        self, name: str, catalog: str | None = None, force: bool = False
-    ) -> None:
-        if catalog is not None:
-            raise exc.UnsupportedOperationError(
-                "DuckDB cannot create a database in another catalog."
-            )
+        name : string
+          Database name
+        path : string, default None
+          HDFS path where to store the database data; otherwise uses Impala
+          default
+        """
+        if path:
+            # explicit mkdir ensures the user own the dir rather than impala,
+            # which is easier for manual cleanup, if necessary
+            self.hdfs.mkdir(path)
+        statement = ddl.CreateDatabase(name, path=path, can_exist=force)
+        return self._execute(statement)
 
-        name = sg.table(name, catalog=catalog, quoted=self.compiler.quoted)
-        with self._safe_raw_sql(sge.Create(this=name, kind="SCHEMA", replace=force)):
-            pass
+    def drop_database(self, name, force=False):
+        """
+        Drop an Impala database
 
-    def drop_database(
-        self, name: str, catalog: str | None = None, force: bool = False
-    ) -> None:
-        if catalog is not None:
-            raise exc.UnsupportedOperationError(
-                "DuckDB cannot drop a database in another catalog."
-            )
+        Parameters
+        ----------
+        name : string
+          Database name
+        force : boolean, default False
+          If False and there are any tables in this database, raises an
+          IntegrityError
+        """
+        if not force or self.exists_database(name):
+            tables = self.list_tables(database=name)
+            udfs = self.list_udfs(database=name)
+            udas = self.list_udas(database=name)
+        else:
+            tables = []
+            udfs = []
+            udas = []
+        if force:
+            for table in tables:
+                self.log('Dropping {0}'.format('{0}.{1}'.format(name, table)))
+                self.drop_table_or_view(table, database=name)
+            for func in udfs:
+                self.log('Dropping function {0}({1})'.format(func.name,
+                                                             func.inputs))
+                self.drop_udf(func.name, input_types=func.inputs,
+                              database=name, force=True)
+            for func in udas:
+                self.log('Dropping aggregate function {0}({1})'
+                         .format(func.name, func.inputs))
+                self.drop_uda(func.name, input_types=func.inputs,
+                              database=name, force=True)
+        else:
+            if len(tables) > 0 or len(udfs) > 0 or len(udas) > 0:
+                raise com.IntegrityError('Database {0} must be empty before '
+                                         'being dropped, or set '
+                                         'force=True'.format(name))
+        statement = ddl.DropDatabase(name, must_exist=not force)
+        return self._execute(statement)
 
-        name = sg.table(name, catalog=catalog, quoted=self.compiler.quoted)
-        with self._safe_raw_sql(sge.Drop(this=name, kind="SCHEMA", replace=force)):
-            pass
+    def list_databases(self, like=None):
+        """
+        List databases in the Impala cluster. Like the SHOW DATABASES command
+        in the impala-shell.
+
+        Parameters
+        ----------
+        like : string, default None
+          e.g. 'foo*' to match all tables starting with 'foo'
+
+        Returns
+        -------
+        databases : list of strings
+        """
+        statement = 'SHOW DATABASES'
+        if like:
+            statement += " LIKE '{0}'".format(like)
 
-    def register(
-        self,
-        source: str | Path | Any,
-        table_name: str | None = None,
-        **kwargs: Any,
-    ) -> ir.Table:
-        """Register a data source as a table in the current database.
+        with self._execute(statement, results=True) as cur:
+            results = self._get_list(cur)
+
+        return results
+
+    def get_schema(self, table_name, database=None):
+        """
+        Return a Schema object for the indicated table and database
 
         Parameters
         ----------
-        source
-            The data source(s). May be a path to a file or directory of
-            parquet/csv files, an iterable of parquet or CSV files, a pandas
-            dataframe, a pyarrow table or dataset, or a postgres URI.
-        table_name
-            An optional name to use for the created table. This defaults to a
-            sequentially generated name.
-        **kwargs
-            Additional keyword arguments passed to DuckDB loading functions for
-            CSV or parquet.  See https://duckdb.org/docs/data/csv and
-            https://duckdb.org/docs/data/parquet for more information.
+        table_name : string
+          May be fully qualified
+        database : string, default None
 
         Returns
         -------
-        ir.Table
-            The just-registered table
+        schema : ibis Schema
+        """
+        qualified_name = self._fully_qualified_name(table_name, database)
+        query = 'DESCRIBE {0}'.format(qualified_name)
+        tuples = self.con.fetchall(query)
+
+        names, types, comments = zip(*tuples)
+
+        ibis_types = []
+        for t in types:
+            t = t.lower()
+            t = udf._impala_to_ibis_type.get(t, t)
+            ibis_types.append(t)
+
+        names = [x.lower() for x in names]
+
+        return dt.Schema(names, ibis_types)
+
+    @property
+    def client_options(self):
+        return self.con.options
 
+    def get_options(self):
+        """
+        Return current query options for the Impala session
         """
+        query = 'SET'
+        tuples = self.con.fetchall(query)
+        return dict(tuples)
 
-        if isinstance(source, (str, Path)):
-            first = str(source)
-        elif isinstance(source, (list, tuple)):
-            first = source[0]
-        else:
-            try:
-                return self.read_in_memory(source, table_name=table_name, **kwargs)
-            except (duckdb.InvalidInputException, NameError):
-                self._register_failure()
-
-        if first.startswith(("parquet://", "parq://")) or first.endswith(
-            ("parq", "parquet")
-        ):
-            return self.read_parquet(source, table_name=table_name, **kwargs)
-        elif first.startswith(
-            ("csv://", "csv.gz://", "txt://", "txt.gz://")
-        ) or first.endswith(("csv", "csv.gz", "tsv", "tsv.gz", "txt", "txt.gz")):
-            return self.read_csv(source, table_name=table_name, **kwargs)
-        elif first.startswith(("postgres://", "postgresql://")):
-            return self.read_postgres(source, table_name=table_name, **kwargs)
-        elif first.startswith("sqlite://"):
-            return self.read_sqlite(
-                first[len("sqlite://") :], table_name=table_name, **kwargs
-            )
+    def set_options(self, options):
+        self.con.set_options(options)
+
+    def reset_options(self):
+        # Must nuke all cursors
+        raise NotImplementedError
+
+    def set_compression_codec(self, codec):
+        """
+        Parameters
+        """
+        if codec is None:
+            codec = 'none'
         else:
-            self._register_failure()  # noqa: RET503
+            codec = codec.lower()
 
-    def _register_failure(self):
-        import inspect
+        if codec not in ('none', 'gzip', 'snappy'):
+            raise ValueError('Unknown codec: {0}'.format(codec))
 
-        msg = ", ".join(
-            name for name, _ in inspect.getmembers(self) if name.startswith("read_")
-        )
-        raise ValueError(
-            f"Cannot infer appropriate read function for input, "
-            f"please call one of {msg} directly"
-        )
-
-    @util.experimental
-    def read_json(
-        self,
-        source_list: str | list[str] | tuple[str],
-        table_name: str | None = None,
-        **kwargs,
-    ) -> ir.Table:
-        """Read newline-delimited JSON into an ibis table.
-
-        ::: {.callout-note}
-        ## This feature requires duckdb>=0.7.0
-        :::
-
-        Parameters
-        ----------
-        source_list
-            File or list of files
-        table_name
-            Optional table name
-        **kwargs
-            Additional keyword arguments passed to DuckDB's `read_json_auto` function
-
-        Returns
-        -------
-        Table
-            An ibis table expression
-
-        """
-        if not table_name:
-            table_name = util.gen_name("read_json")
-
-        options = [
-            sg.to_identifier(key).eq(sge.convert(val)) for key, val in kwargs.items()
-        ]
-
-        self._create_temp_view(
-            table_name,
-            sg.select(STAR).from_(
-                self.compiler.f.read_json_auto(
-                    normalize_filenames(source_list), *options
-                )
-            ),
-        )
-
-        return self.table(table_name)
-
-    def read_csv(
-        self,
-        source_list: str | list[str] | tuple[str],
-        table_name: str | None = None,
-        **kwargs: Any,
-    ) -> ir.Table:
-        """Register a CSV file as a table in the current database.
-
-        Parameters
-        ----------
-        source_list
-            The data source(s). May be a path to a file or directory of CSV files, or an
-            iterable of CSV files.
-        table_name
-            An optional name to use for the created table. This defaults to
-            a sequentially generated name.
-        **kwargs
-            Additional keyword arguments passed to DuckDB loading function.
-            See https://duckdb.org/docs/data/csv for more information.
-
-        Returns
-        -------
-        ir.Table
-            The just-registered table
-
-        """
-        source_list = normalize_filenames(source_list)
-
-        if not table_name:
-            table_name = util.gen_name("read_csv")
-
-        # auto_detect and columns collide, so we set auto_detect=True
-        # unless COLUMNS has been specified
-        if any(
-            source.startswith(("http://", "https://", "s3://"))
-            for source in source_list
-        ):
-            self._load_extensions(["httpfs"])
-
-        kwargs.setdefault("header", True)
-        kwargs["auto_detect"] = kwargs.pop("auto_detect", "columns" not in kwargs)
-        # TODO: clean this up
-        # We want to _usually_ quote arguments but if we quote `columns` it messes
-        # up DuckDB's struct parsing.
-        options = [
-            sg.to_identifier(key).eq(sge.convert(val)) for key, val in kwargs.items()
-        ]
-
-        if (columns := kwargs.pop("columns", None)) is not None:
-            options.append(
-                sg.to_identifier("columns").eq(
-                    sge.Struct(
-                        expressions=[
-                            sge.PropertyEQ(
-                                this=sge.convert(key), expression=sge.convert(value)
-                            )
-                            for key, value in columns.items()
-                        ]
-                    )
-                )
-            )
-
-        self._create_temp_view(
-            table_name,
-            sg.select(STAR).from_(self.compiler.f.read_csv(source_list, *options)),
-        )
-
-        return self.table(table_name)
-
-    def read_geo(
-        self,
-        source: str,
-        table_name: str | None = None,
-        **kwargs: Any,
-    ) -> ir.Table:
-        """Register a GEO file as a table in the current database.
-
-        Parameters
-        ----------
-        source
-            The data source(s). Path to a file of geospatial files supported
-            by duckdb.
-            See https://duckdb.org/docs/extensions/spatial.html#st_read---read-spatial-data-from-files
-        table_name
-            An optional name to use for the created table. This defaults to
-            a sequentially generated name.
-        **kwargs
-            Additional keyword arguments passed to DuckDB loading function.
-            See https://duckdb.org/docs/extensions/spatial.html#st_read---read-spatial-data-from-files
-            for more information.
-
-        Returns
-        -------
-        ir.Table
-            The just-registered table
-
-        """
-
-        if not table_name:
-            table_name = util.gen_name("read_geo")
-
-        # load geospatial extension
-        self.load_extension("spatial")
-
-        source = util.normalize_filename(source)
-        if source.startswith(("http://", "https://", "s3://")):
-            self._load_extensions(["httpfs"])
-
-        source_expr = sg.select(STAR).from_(
-            self.compiler.f.st_read(
-                source,
-                *(sg.to_identifier(key).eq(val) for key, val in kwargs.items()),
-            )
-        )
-
-        view = sge.Create(
-            kind="VIEW",
-            this=sg.table(table_name, quoted=self.compiler.quoted),
-            properties=sge.Properties(expressions=[sge.TemporaryProperty()]),
-            expression=source_expr,
-        )
-        with self._safe_raw_sql(view):
-            pass
-        return self.table(table_name)
+        self.set_options({'COMPRESSION_CODEC': codec})
 
-    def read_parquet(
-        self,
-        source_list: str | Iterable[str],
-        table_name: str | None = None,
-        **kwargs: Any,
-    ) -> ir.Table:
-        """Register a parquet file as a table in the current database.
+    def exists_table(self, name, database=None):
+        """
+        Determine if the indicated table or view exists
 
         Parameters
         ----------
-        source_list
-            The data source(s). May be a path to a file, an iterable of files,
-            or directory of parquet files.
-        table_name
-            An optional name to use for the created table. This defaults to
-            a sequentially generated name.
-        **kwargs
-            Additional keyword arguments passed to DuckDB loading function.
-            See https://duckdb.org/docs/data/parquet for more information.
+        name : string
+        database : string, default None
 
         Returns
         -------
-        ir.Table
-            The just-registered table
+        if_exists : boolean
+        """
+        return len(self.list_tables(like=name, database=database)) > 0
 
+    def create_view(self, name, expr, database=None):
         """
-        source_list = normalize_filenames(source_list)
+        Create an Impala view from a table expression
 
-        table_name = table_name or util.gen_name("read_parquet")
+        Parameters
+        ----------
+        name : string
+        expr : ibis TableExpr
+        database : string, default None
+        """
+        ast = self._build_ast(expr)
+        select = ast.queries[0]
+        statement = ddl.CreateView(name, select, database=database)
+        return self._execute(statement)
 
-        # Default to using the native duckdb parquet reader
-        # If that fails because of auth issues, fall back to ingesting via
-        # pyarrow dataset
-        try:
-            self._read_parquet_duckdb_native(source_list, table_name, **kwargs)
-        except duckdb.IOException:
-            self._read_parquet_pyarrow_dataset(source_list, table_name, **kwargs)
+    def drop_view(self, name, database=None, force=False):
+        """
+        Drop an Impala view
 
-        return self.table(table_name)
+        Parameters
+        ----------
+        name : string
+        database : string, default None
+        force : boolean, default False
+          Database may throw exception if table does not exist
+        """
+        statement = ddl.DropView(name, database=database,
+                                 must_exist=not force)
+        return self._execute(statement)
+
+    def create_table(self, table_name, obj=None, schema=None, database=None,
+                     format='parquet', force=False, external=False,
+                     location=None, partition=None, like_parquet=None,
+                     path=None):
+        """
+        Create a new table in Impala using an Ibis table expression
+
+        Parameters
+        ----------
+        table_name : string
+        obj : TableExpr or pandas.DataFrame, optional
+          If passed, creates table from select statement results
+        schema : ibis.Schema, optional
+          Mutually exclusive with expr, creates an empty table with a
+          particular schema
+        database : string, default None (optional)
+        format : {'parquet'}
+        force : boolean, default False
+          Do not create table if table with indicated name already exists
+        external : boolean, default False
+          Create an external table; Impala will not delete the underlying data
+          when the table is dropped
+        location : string, default None
+          Specify the directory location where Impala reads and writes files
+          for the table
+        partition : list of strings
+          Must pass a schema to use this. Cannot partition from an expression
+          (create-table-as-select)
+        like_parquet : string (HDFS path), optional
+          Can specify in lieu of a schema
+
+        Examples
+        --------
+        con.create_table('new_table_name', table_expr)
+        """
+        if like_parquet is not None:
+            raise NotImplementedError
+
+        # TODO: deprecation warning
+        if path is not None:
+            location = path
 
-    def _read_parquet_duckdb_native(
-        self, source_list: str | Iterable[str], table_name: str, **kwargs: Any
-    ) -> None:
-        if any(
-            source.startswith(("http://", "https://", "s3://"))
-            for source in source_list
-        ):
-            self._load_extensions(["httpfs"])
+        if obj is not None:
+            if isinstance(obj, pd.DataFrame):
+                writer, to_insert = _write_temp_dataframe(self, obj)
+            else:
+                to_insert = obj
+            ast = self._build_ast(to_insert)
+            select = ast.queries[0]
+
+            if partition is not None:
+                # Fairly certain this is currently the case
+                raise ValueError('partition not supported with '
+                                 'create-table-as-select. Create an '
+                                 'empty partitioned table instead '
+                                 'and insert into those partitions.')
+
+            statement = ddl.CTAS(table_name, select,
+                                 database=database,
+                                 can_exist=force,
+                                 format=format,
+                                 external=external,
+                                 path=location)
+        elif schema is not None:
+            statement = ddl.CreateTableWithSchema(
+                table_name, schema, ddl.NoFormat(),
+                database=database,
+                format=format,
+                can_exist=force,
+                external=external,
+                path=location, partition=partition)
+        else:
+            raise com.IbisError('Must pass expr or schema')
 
-        options = [
-            sg.to_identifier(key).eq(sge.convert(val)) for key, val in kwargs.items()
-        ]
-        self._create_temp_view(
-            table_name,
-            sg.select(STAR).from_(self.compiler.f.read_parquet(source_list, *options)),
-        )
+        return self._execute(statement)
 
-    def _read_parquet_pyarrow_dataset(
-        self, source_list: str | Iterable[str], table_name: str, **kwargs: Any
-    ) -> None:
-        import pyarrow.dataset as ds
+    def avro_file(self, hdfs_dir, avro_schema, name=None, database=None,
+                  external=True, persist=False):
+        """
+        Create a (possibly temporary) table to read a collection of Avro data.
+
+        Parameters
+        ----------
+        hdfs_dir : string
+          Absolute HDFS path to directory containing avro files
+        avro_schema : dict
+          The Avro schema for the data as a Python dict
+        name : string, default None
+        database : string, default None
+        external : boolean, default True
+        persist : boolean, default False
 
-        dataset = ds.dataset(list(map(ds.dataset, source_list)), **kwargs)
-        self._load_extensions(["httpfs"])
-        # We don't create a view since DuckDB special cases Arrow Datasets
-        # so if we also create a view we end up with both a "lazy table"
-        # and a view with the same name
-        self.con.register(table_name, dataset)
-        # DuckDB normally auto-detects Arrow Datasets that are defined
-        # in local variables but the `dataset` variable won't be local
-        # by the time we execute against this so we register it
-        # explicitly.
+        Returns
+        -------
+        avro_table : ImpalaTable
+        """
+        name, database = self._get_concrete_table_path(name, database,
+                                                       persist=persist)
 
-    def read_in_memory(
-        self,
-        source: pd.DataFrame | pa.Table | pa.ipc.RecordBatchReader,
-        table_name: str | None = None,
-    ) -> ir.Table:
-        """Register a Pandas DataFrame or pyarrow object as a table in the current database.
+        stmt = ddl.CreateTableAvro(name, hdfs_dir, avro_schema,
+                                   database=database,
+                                   external=external)
+        self._execute(stmt)
+        return self._wrap_new_table(name, database, persist)
+
+    def delimited_file(self, hdfs_dir, schema, name=None, database=None,
+                       delimiter=',',
+                       na_rep=None, escapechar=None, lineterminator=None,
+                       external=True, persist=False):
+        """
+        Interpret delimited text files (CSV / TSV / etc.) as an Ibis table. See
+        `parquet_file` for more exposition on what happens under the hood.
 
         Parameters
         ----------
-        source
-            The data source.
-        table_name
-            An optional name to use for the created table. This defaults to
-            a sequentially generated name.
+        hdfs_dir : string
+          HDFS directory name containing delimited text files
+        schema : ibis Schema
+        name : string, default None
+          Name for temporary or persistent table; otherwise random one
+          generated
+        database : string
+          Database to create the (possibly temporary) table in
+        delimiter : length-1 string, default ','
+          Pass None if there is no delimiter
+        escapechar : length-1 string
+          Character used to escape special characters
+        lineterminator : length-1 string
+          Character used to delimit lines
+        external : boolean, default True
+          Create table as EXTERNAL (data will not be deleted on drop). Not that
+          if persist=False and external=False, whatever data you reference will
+          be deleted
+        persist : boolean, default False
+          If True, do not delete the table upon garbage collection of ibis
+          table object
 
         Returns
         -------
-        ir.Table
-            The just-registered table
+        delimited_table : ImpalaTable
+        """
+        name, database = self._get_concrete_table_path(name, database,
+                                                       persist=persist)
+
+        stmt = ddl.CreateTableDelimited(name, hdfs_dir, schema,
+                                        database=database,
+                                        delimiter=delimiter,
+                                        external=external,
+                                        na_rep=na_rep,
+                                        lineterminator=lineterminator,
+                                        escapechar=escapechar)
+        self._execute(stmt)
+        return self._wrap_new_table(name, database, persist)
+
+    def parquet_file(self, hdfs_dir, schema=None, name=None, database=None,
+                     external=True, like_file=None, like_table=None,
+                     persist=False):
+        """
+        Make indicated parquet file in HDFS available as an Ibis table.
+
+        The table created can be optionally named and persisted, otherwise a
+        unique name will be generated. Temporarily, for any non-persistent
+        external table created by Ibis we will attempt to drop it when the
+        underlying object is garbage collected (or the Python interpreter shuts
+        down normally).
 
+        Parameters
+        ----------
+        hdfs_dir : string
+          Path in HDFS
+        schema : ibis Schema
+          If no schema provided, and neither of the like_* argument is passed,
+          one will be inferred from one of the parquet files in the directory.
+        like_file : string
+          Absolute path to Parquet file in HDFS to use for schema
+          definitions. An alternative to having to supply an explicit schema
+        like_table : string
+          Fully scoped and escaped string to an Impala table whose schema we
+          will use for the newly created table.
+        name : string, optional
+          random unique name generated otherwise
+        database : string, optional
+          Database to create the (possibly temporary) table in
+        external : boolean, default True
+          If a table is external, the referenced data will not be deleted when
+          the table is dropped in Impala. Otherwise (external=False) Impala
+          takes ownership of the Parquet file.
+        persist : boolean, default False
+          Do not drop the table upon Ibis garbage collection / interpreter
+          shutdown
+
+        Returns
+        -------
+        parquet_table : ImpalaTable
         """
-        table_name = table_name or util.gen_name("read_in_memory")
-        self.con.register(table_name, source)
+        name, database = self._get_concrete_table_path(name, database,
+                                                       persist=persist)
+
+        # If no schema provided, need to find some absolute path to a file in
+        # the HDFS directory
+        if like_file is None and like_table is None and schema is None:
+            file_name = self.hdfs._find_any_file(hdfs_dir)
+            like_file = pjoin(hdfs_dir, file_name)
+
+        stmt = ddl.CreateTableParquet(name, hdfs_dir,
+                                      schema=schema,
+                                      database=database,
+                                      example_file=like_file,
+                                      example_table=like_table,
+                                      external=external,
+                                      can_exist=False)
+        self._execute(stmt)
+        return self._wrap_new_table(name, database, persist)
+
+    def _get_concrete_table_path(self, name, database, persist=False):
+        if not persist:
+            if name is None:
+                name = util.guid()
+
+            if database is None:
+                self._ensure_temp_db_exists()
+                database = options.impala.temp_db
+            return name, database
+        else:
+            if name is None:
+                raise com.IbisError('Must pass table name if persist=True')
+            return name, database
+
+    def _ensure_temp_db_exists(self):
+        # TODO: session memoize to avoid unnecessary `SHOW DATABASES` calls
+        name, path = options.impala.temp_db, options.impala.temp_hdfs_path
+        if not self.exists_database(name):
+            self.create_database(name, path=path, force=True)
+
+    def _wrap_new_table(self, name, database, persist):
+        qualified_name = self._fully_qualified_name(name, database)
+
+        if persist:
+            t = self.table(qualified_name)
+        else:
+            schema = self._get_table_schema(qualified_name)
+            node = ImpalaTemporaryTable(qualified_name, schema, self)
+            t = self._table_expr_klass(node)
+
+        # Compute number of rows in table for better default query planning
+        cardinality = t.count().execute()
+        set_card = ("alter table {0} set tblproperties('numRows'='{1}', "
+                    "'STATS_GENERATED_VIA_STATS_TASK' = 'true')"
+                    .format(qualified_name, cardinality))
+        self._execute(set_card)
 
-        if isinstance(source, pa.ipc.RecordBatchReader):
-            # Ensure the reader isn't marked as started, in case the name is
-            # being overwritten.
-            self._record_batch_readers_consumed[table_name] = False
+        self._temp_objects[id(t)] = t
 
-        return self.table(table_name)
+        return t
 
-    def read_delta(
-        self,
-        source_table: str,
-        table_name: str | None = None,
-        **kwargs: Any,
-    ) -> ir.Table:
-        """Register a Delta Lake table as a table in the current database.
+    def text_file(self, hdfs_path, column_name='value'):
+        """
+        Interpret text data as a table with a single string column.
 
         Parameters
         ----------
-        source_table
-            The data source. Must be a directory
-            containing a Delta Lake table.
-        table_name
-            An optional name to use for the created table. This defaults to
-            a sequentially generated name.
-        **kwargs
-            Additional keyword arguments passed to deltalake.DeltaTable.
 
         Returns
         -------
-        ir.Table
-            The just-registered table.
+        text_table : TableExpr
+        """
+        pass
 
+    def insert(self, table_name, obj=None, database=None, overwrite=False,
+               partition=None, values=None, validate=True):
         """
-        source_table = normalize_filenames(source_table)[0]
+        Insert into existing table.
 
-        table_name = table_name or util.gen_name("read_delta")
+        See ImpalaTable.insert for other parameters.
 
-        try:
-            from deltalake import DeltaTable
-        except ImportError:
-            raise ImportError(
-                "The deltalake extra is required to use the "
-                "read_delta method. You can install it using pip:\n\n"
-                "pip install 'ibis-framework[deltalake]'\n"
-            )
+        Parameters
+        ----------
+        table_name : string
+        database : string, default None
+
+        Examples
+        --------
+        con.insert('my_table', table_expr)
+
+        # Completely overwrite contents
+        con.insert('my_table', table_expr, overwrite=True)
+        """
+        table = self.table(table_name, database=database)
+        return table.insert(obj=obj, overwrite=overwrite, partition=partition,
+                            values=values, validate=validate)
 
-        delta_table = DeltaTable(source_table, **kwargs)
+    def load_data(self, table_name, path, database=None, overwrite=False,
+                  partition=None):
+        """
+        Wraps the LOAD DATA DDL statement. Loads data into an Impala table by
+        physically moving data files.
 
-        return self.read_in_memory(
-            delta_table.to_pyarrow_dataset(), table_name=table_name
-        )
+        Parameters
+        ----------
+        table_name : string
+        database : string, default None (optional)
+        """
+        table = self.table(table_name, database=database)
+        return table.load_data(path, overwrite=overwrite,
+                               partition=partition)
 
-    def list_tables(
-        self,
-        like: str | None = None,
-        database: tuple[str, str] | str | None = None,
-        schema: str | None = None,
-    ) -> list[str]:
-        """List tables and views.
+    def drop_table(self, table_name, database=None, force=False):
+        """
+        Drop an Impala table
 
         Parameters
         ----------
-        like
-            Regex to filter by table/view name.
-        database
-            Database location. If not passed, uses the current database.
+        table_name : string
+        database : string, default None (optional)
+        force : boolean, default False
+          Database may throw exception if table does not exist
 
-            By default uses the current `database` (`self.current_database`) and
-            `catalog` (`self.current_catalog`).
+        Examples
+        --------
+        con.drop_table('my_table', database='operations', force=True)
+        """
+        statement = ddl.DropTable(table_name, database=database,
+                                  must_exist=not force)
+        self._execute(statement)
 
-            To specify a table in a separate catalog, you can pass in the
-            catalog and database as a string `"catalog.database"`, or as a tuple of
-            strings `("catalog", "database")`.
+    def truncate_table(self, table_name, database=None):
+        """
+        Delete all rows from, but do not drop, an existing table
 
-            ::: {.callout-note}
-            ## Ibis does not use the word `schema` to refer to database hierarchy.
+        Parameters
+        ----------
+        table_name : string
+        database : string, default None (optional)
+        """
+        statement = ddl.TruncateTable(table_name, database=database)
+        self._execute(statement)
 
-            A collection of tables is referred to as a `database`.
-            A collection of `database` is referred to as a `catalog`.
+    def drop_table_or_view(self, name, database=None, force=False):
+        """
+        Attempt to drop a relation that may be a view or table
+        """
+        try:
+            self.drop_table(name, database=database)
+        except Exception as e:
+            try:
+                self.drop_view(name, database=database)
+            except:
+                raise e
 
-            These terms are mapped onto the corresponding features in each
-            backend (where available), regardless of whether the backend itself
-            uses the same terminology.
-            :::
-        schema
-            [deprecated] Schema name. If not passed, uses the current schema.
+    def cache_table(self, table_name, database=None, pool='default'):
+        """
+        Caches a table in cluster memory in the given pool.
 
-        Returns
-        -------
-        list[str]
-            List of table and view names.
+        Parameters
+        ----------
+        table_name : string
+        database : string default None (optional)
+        pool : string, default 'default'
+           The name of the pool in which to cache the table
 
         Examples
         --------
-        >>> import ibis
-        >>> con = ibis.duckdb.connect()
-        >>> foo = con.create_table("foo", schema=ibis.schema(dict(a="int")))
-        >>> con.list_tables()
-        ['foo']
-        >>> bar = con.create_view("bar", foo)
-        >>> con.list_tables()
-        ['bar', 'foo']
-        >>> con.create_database("my_database")
-        >>> con.list_tables(database="my_database")
-        []
-        >>> with con.begin() as c:
-        ...     c.exec_driver_sql("CREATE TABLE my_database.baz (a INTEGER)")  # doctest: +ELLIPSIS
-        <...>
-        >>> con.list_tables(database="my_database")
-        ['baz']
+        con.cache_table('my_table', database='operations', pool='op_4GB_pool')
+        """
+        statement = ddl.CacheTable(table_name, database=database, pool=pool)
+        self._execute(statement)
+
+    def _get_table_schema(self, tname):
+        return self.get_schema(tname)
+
+    def _get_schema_using_query(self, query):
+        with self._execute(query, results=True) as cur:
+            # resets the state of the cursor and closes operation
+            cur.fetchall()
+            names, ibis_types = self._adapt_types(cur.description)
+
+        # per #321; most Impala tables will be lower case already, but Avro
+        # data, depending on the version of Impala, might have field names in
+        # the metastore cased according to the explicit case in the declared
+        # avro schema. This is very annoying, so it's easier to just conform on
+        # all lowercase fields from Impala.
+        names = [x.lower() for x in names]
+
+        return dt.Schema(names, ibis_types)
+
+    def create_function(self, func, name=None, database=None):
+        """
+        Creates a function within Impala
+
+        Parameters
+        ----------
+        func : ImpalaUDF or ImpalaUDA
+          Created with wrap_udf or wrap_uda
+        name : string (optional)
+        database : string (optional)
+        """
+        if name is None:
+            name = func.name
+        database = database or self.current_database
+
+        if isinstance(func, udf.ImpalaUDF):
+            stmt = ddl.CreateFunction(func.lib_path, func.so_symbol,
+                                      func.input_type,
+                                      func.output,
+                                      name, database)
+        elif isinstance(func, udf.ImpalaUDA):
+            stmt = ddl.CreateAggregateFunction(func.lib_path,
+                                               func.input_type,
+                                               func.output,
+                                               func.update_fn,
+                                               func.init_fn,
+                                               func.merge_fn,
+                                               func.serialize_fn,
+                                               func.finalize_fn,
+                                               name, database)
+        else:
+            raise TypeError(func)
+        self._execute(stmt)
 
+    def drop_udf(self, name, input_types=None, database=None, force=False,
+                 aggregate=False):
         """
-        table_loc = self._warn_and_create_table_loc(database, schema)
+        Drops a UDF
+        If only name is given, this will search
+        for the relevant UDF and drop it.
+        To delete an overloaded UDF, give only a name and force=True
 
-        catalog = F.current_database()
-        database = F.current_schema()
-        if table_loc is not None:
-            catalog = table_loc.catalog or catalog
-            database = table_loc.db or database
+        Parameters
+        ----------
+        name : string
+        input_types : list of strings (optional)
+        force : boolean, default False Must be set to true to
+                drop overloaded UDFs
+        database : string, default None
+        aggregate : boolean, default False
+        """
+        if not input_types:
+            if not database:
+                database = self.current_database
+            result = self.list_udfs(database=database, like=name)
+            if len(result) > 1:
+                if force:
+                    for func in result:
+                        self._drop_single_function(func.name, func.inputs,
+                                                   database=database,
+                                                   aggregate=aggregate)
+                    return
+                else:
+                    raise Exception("More than one function " +
+                                    "with {0} found.".format(name) +
+                                    "Please specify force=True")
+            elif len(result) == 1:
+                func = result.pop()
+                self._drop_single_function(func.name, func.inputs,
+                                           database=database,
+                                           aggregate=aggregate)
+                return
+            else:
+                raise Exception("No function found with name {0}"
+                                .format(name))
+        self._drop_single_function(name, input_types, database=database,
+                                   aggregate=aggregate)
+
+    def drop_uda(self, name, input_types=None, database=None, force=False):
+        """
+        Drop aggregate function. See drop_udf for more information on the
+        parameters.
+        """
+        return self.drop_udf(name, input_types=input_types, database=database,
+                             force=force)
+
+    def _drop_single_function(self, name, input_types, database=None,
+                              aggregate=False):
+        stmt = ddl.DropFunction(name, input_types, must_exist=False,
+                                aggregate=aggregate, database=database)
+        self._execute(stmt)
+
+    def _drop_all_functions(self, database):
+        udfs = self.list_udfs(database=database)
+        for fnct in udfs:
+            stmt = ddl.DropFunction(fnct.name, fnct.inputs, must_exist=False,
+                                    aggregate=False, database=database)
+            self._execute(stmt)
+        udafs = self.list_udas(database=database)
+        for udaf in udafs:
+            stmt = ddl.DropFunction(udaf.name, udaf.inputs, must_exist=False,
+                                    aggregate=True, database=database)
+            self._execute(stmt)
+
+    def list_udfs(self, database=None, like=None):
+        """
+        Lists all UDFs associated with given database
+
+        Parameters
+        ----------
+        database : string
+        like : string for searching (optional)
+        """
+        if not database:
+            database = self.current_database
+        statement = ddl.ListFunction(database, like=like, aggregate=False)
+        with self._execute(statement, results=True) as cur:
+            result = self._get_udfs(cur, udf.ImpalaUDF)
+        return result
+
+    def list_udas(self, database=None, like=None):
+        """
+        Lists all UDAFs associated with a given database
+
+        Parameters
+        ----------
+        database : string
+        like : string for searching (optional)
+        """
+        if not database:
+            database = self.current_database
+        statement = ddl.ListFunction(database, like=like, aggregate=True)
+        with self._execute(statement, results=True) as cur:
+            result = self._get_udfs(cur, udf.ImpalaUDA)
+
+        return result
+
+    def _get_udfs(self, cur, klass):
+        from ibis.expr.rules import varargs
+        from ibis.expr.datatypes import validate_type
+
+        def _to_type(x):
+            ibis_type = udf._impala_type_to_ibis(x.lower())
+            return validate_type(ibis_type)
+
+        tuples = cur.fetchall()
+        if len(tuples) > 0:
+            result = []
+            for out_type, sig in tuples:
+                name, types = _split_signature(sig)
+                types = _type_parser(types).types
+
+                inputs = []
+                for arg in types:
+                    argm = _arg_type.match(arg)
+                    var, simple = argm.groups()
+                    if simple:
+                        t = _to_type(simple)
+                        inputs.append(t)
+                    else:
+                        t = _to_type(var)
+                        inputs = varargs(t)
+                        # TODO
+                        # inputs.append(varargs(t))
+                        break
+
+                output = udf._impala_type_to_ibis(out_type.lower())
+                result.append(klass(inputs, output, name=name))
+            return result
+        else:
+            return []
+
+    def exists_udf(self, name, database=None):
+        """
+        Checks if a given UDF exists within a specified database
 
-        col = "table_name"
-        sql = (
-            sg.select(col)
-            .from_(sg.table("tables", db="information_schema"))
-            .distinct()
-            .where(
-                C.table_catalog.eq(catalog).or_(
-                    C.table_catalog.eq(sge.convert("temp"))
-                ),
-                C.table_schema.eq(database),
-            )
-            .sql(self.name, pretty=True)
-        )
-        out = self.con.execute(sql).fetch_arrow_table()
+        Parameters
+        ----------
+        name : string, UDF name
+        database : string, database name
 
-        return self._filter_with_like(out[col].to_pylist(), like)
+        Returns
+        -------
+        if_exists : boolean
+        """
+        return len(self.list_udfs(database=database, like=name)) > 0
 
-    def read_postgres(
-        self, uri: str, *, table_name: str | None = None, database: str = "public"
-    ) -> ir.Table:
-        """Register a table from a postgres instance into a DuckDB table.
+    def exists_uda(self, name, database=None):
+        """
+        Checks if a given UDAF exists within a specified database
 
         Parameters
         ----------
-        uri
-            A postgres URI of the form `postgres://user:password@host:port`
-        table_name
-            The table to read
-        database
-            PostgreSQL database (schema) where `table_name` resides
+        name : string, UDAF name
+        database : string, database name
 
         Returns
         -------
-        ir.Table
-            The just-registered table.
+        if_exists : boolean
+        """
+        return len(self.list_udas(database=database, like=name)) > 0
+
+    def compute_stats(self, name, database=None, incremental=False,
+                      async=False):
+        """
+        Issue COMPUTE STATS command for a given table
 
+        Parameters
+        ----------
+        name : string
+          Can be fully qualified (with database name)
+        database : string, optional
+        incremental : boolean, default False
+          If True, issue COMPUTE INCREMENTAL STATS
         """
-        if table_name is None:
-            raise ValueError(
-                "`table_name` is required when registering a postgres table"
-            )
-        self._load_extensions(["postgres_scanner"])
+        # TODO async + cancellation
+        if async:
+            raise NotImplementedError
 
-        self._create_temp_view(
-            table_name,
-            sg.select(STAR).from_(
-                self.compiler.f.postgres_scan_pushdown(uri, database, table_name)
-            ),
-        )
+        maybe_inc = 'INCREMENTAL ' if incremental else ''
+        cmd = 'COMPUTE {0}STATS'.format(maybe_inc)
 
-        return self.table(table_name)
+        stmt = self._table_command(cmd, name, database=database)
+        self._execute(stmt)
 
-    def read_mysql(
-        self,
-        uri: str,
-        *,
-        catalog: str,
-        table_name: str | None = None,
-    ) -> ir.Table:
-        """Register a table from a MySQL instance into a DuckDB table.
+    def invalidate_metadata(self, name=None, database=None):
+        """
+        Issue INVALIDATE METADATA command, optionally only applying to a
+        particular table. See Impala documentation.
 
         Parameters
         ----------
-        uri
-            A mysql URI of the form `mysql://user:password@host:port/database`
-        catalog
-            User-defined alias given to the MySQL database that is being attached
-            to DuckDB
-        table_name
-            The table to read
+        name : string, optional
+          Table name. Can be fully qualified (with database)
+        database : string, optional
+        """
+        stmt = 'INVALIDATE METADATA'
+        if name is not None:
+            stmt = self._table_command(stmt, name, database=database)
+        self._execute(stmt)
 
-        Returns
-        -------
-        ir.Table
-            The just-registered table.
+    def refresh(self, name, database=None):
         """
+        Reload HDFS block location metadata for a table, for example after
+        ingesting data as part of an ETL pipeline. Related to INVALIDATE
+        METADATA. See Impala documentation for more.
 
-        parsed = urllib.parse.urlparse(uri)
+        Parameters
+        ----------
+        name : string
+          Table name. Can be fully qualified (with database)
+        database : string, optional
+        """
+        # TODO(wesm): can this statement be cancelled?
+        stmt = self._table_command('REFRESH', name, database=database)
+        self._execute(stmt)
+
+    def describe_formatted(self, name, database=None):
+        """
+        Retrieve results of DESCRIBE FORMATTED command. See Impala
+        documentation for more.
+
+        Parameters
+        ----------
+        name : string
+          Table name. Can be fully qualified (with database)
+        database : string, optional
+        """
+        from ibis.impala.metadata import parse_metadata
 
-        if table_name is None:
-            raise ValueError("`table_name` is required when registering a mysql table")
+        stmt = self._table_command('DESCRIBE FORMATTED',
+                                   name, database=database)
+        query = ImpalaQuery(self, stmt)
+        result = query.execute()
 
-        self._load_extensions(["mysql"])
+        # Leave formatting to pandas
+        for c in result.columns:
+            result[c] = result[c].str.strip()
 
-        database = parsed.path.strip("/")
+        return parse_metadata(result)
 
-        query_con = f"""ATTACH 'host={parsed.hostname} user={parsed.username} password={parsed.password} port={parsed.port} database={database}' AS {catalog} (TYPE mysql)"""
+    def show_files(self, name, database=None):
+        """
+        Retrieve results of SHOW FILES command for a table. See Impala
+        documentation for more.
 
-        with self._safe_raw_sql(query_con):
-            pass
+        Parameters
+        ----------
+        name : string
+          Table name. Can be fully qualified (with database)
+        database : string, optional
+        """
+        stmt = self._table_command('SHOW FILES IN', name, database=database)
+        return self._exec_statement(stmt)
+
+    def list_partitions(self, name, database=None):
+        stmt = self._table_command('SHOW PARTITIONS', name, database=database)
+        return self._exec_statement(stmt)
+
+    def table_stats(self, name, database=None):
+        """
+        Return results of SHOW TABLE STATS for indicated table. See also
+        ImpalaTable.stats
+        """
+        stmt = self._table_command('SHOW TABLE STATS', name, database=database)
+        return self._exec_statement(stmt)
+
+    def column_stats(self, name, database=None):
+        """
+        Return results of SHOW COLUMN STATS for indicated table. See also
+        ImpalaTable.column_stats
+        """
+        stmt = self._table_command('SHOW COLUMN STATS', name,
+                                   database=database)
+        return self._exec_statement(stmt)
+
+    def _exec_statement(self, stmt, adapter=None):
+        query = ImpalaQuery(self, stmt)
+        result = query.execute()
+        if adapter is not None:
+            result = adapter(result)
+        return result
 
-        return self.table(table_name, database=(catalog, database))
+    def _table_command(self, cmd, name, database=None):
+        qualified_name = self._fully_qualified_name(name, database)
+        return '{0} {1}'.format(cmd, qualified_name)
+
+    def _adapt_types(self, descr):
+        names = []
+        adapted_types = []
+        for col in descr:
+            names.append(col[0])
+            impala_typename = col[1]
+            typename = udf._impala_to_ibis_type[impala_typename.lower()]
+
+            if typename == 'decimal':
+                precision, scale = col[4:6]
+                adapted_types.append(dt.Decimal(precision, scale))
+            else:
+                adapted_types.append(typename)
+        return names, adapted_types
 
-    def read_sqlite(
-        self, path: str | Path, *, table_name: str | None = None
-    ) -> ir.Table:
-        """Register a table from a SQLite database into a DuckDB table.
+    def write_dataframe(self, df, path, format='csv', async=False):
+        """
+        Write a pandas DataFrame to indicated file path (default: HDFS) in the
+        indicated format
 
         Parameters
         ----------
-        path
-            The path to the SQLite database
-        table_name
-            The table to read
+        df : DataFrame
+        path : string
+          Absolute output path
+        format : {'csv'}, default 'csv'
+        async : boolean, default False
+          Not yet supported
+
+        Returns
+        -------
+        None (for now)
+        """
+        from ibis.impala.pandas_interop import DataFrameWriter
+
+        if async:
+            raise NotImplementedError
+
+        writer = DataFrameWriter(self, df)
+        return writer.write_csv(path)
+
+
+# ----------------------------------------------------------------------
+# ORM-ish usability layer
+
+
+class ScalarFunction(DatabaseEntity):
+
+    def drop(self):
+        pass
+
+
+class AggregateFunction(DatabaseEntity):
+
+    def drop(self):
+        pass
+
+
+class ImpalaTable(ir.TableExpr, DatabaseEntity):
+
+    """
+    References a physical table in the Impala-Hive metastore
+    """
+
+    @property
+    def _qualified_name(self):
+        return self.op().args[0]
+
+    @property
+    def _unqualified_name(self):
+        return self._match_name()[1]
+
+    @property
+    def _client(self):
+        return self.op().args[2]
+
+    def _match_name(self):
+        m = ddl.fully_qualified_re.match(self._qualified_name)
+        if not m:
+            raise com.IbisError('Cannot determine database name from {0}'
+                                .format(self._qualified_name))
+        db, quoted, unquoted = m.groups()
+        return db, quoted or unquoted
+
+    @property
+    def _database(self):
+        return self._match_name()[0]
+
+    def compute_stats(self, incremental=False, async=False):
+        """
+        Invoke Impala COMPUTE STATS command to compute column, table, and
+        partition statistics.
+
+        See also ImpalaClient.compute_stats
+        """
+        return self._client.compute_stats(self._qualified_name,
+                                          incremental=incremental,
+                                          async=async)
+
+    def invalidate_metadata(self):
+        self._client.invalidate_metadata(self._qualified_name)
+
+    def refresh(self):
+        self._client.refresh(self._qualified_name)
+
+    def metadata(self):
+        """
+        Return parsed results of DESCRIBE FORMATTED statement
 
         Returns
         -------
-        ir.Table
-            The just-registered table.
+        meta : TableMetadata
+        """
+        return self._client.describe_formatted(self._qualified_name)
+
+    describe_formatted = metadata
+
+    def files(self):
+        """
+        Return results of SHOW FILES statement
+        """
+        return self._client.show_files(self._qualified_name)
+
+    def drop(self):
+        """
+        Drop the table from the database
+        """
+        self._client.drop_table_or_view(self._qualified_name)
+
+    def insert(self, obj=None, overwrite=False, partition=None,
+               values=None, validate=True):
+        """
+        Insert into Impala table. Wraps ImpalaClient.insert
+
+        Parameters
+        ----------
+        obj : TableExpr or pandas DataFrame
+        overwrite : boolean, default False
+          If True, will replace existing contents of table
+        partition : list or dict, optional
+          For partitioned tables, indicate the partition that's being inserted
+          into, either with an ordered list of partition keys or a dict of
+          partition field name to value. For example for the partition
+          (year=2007, month=7), this can be either (2007, 7) or {'year': 2007,
+          'month': 7}.
+        validate : boolean, default True
+          If True, do more rigorous validation that schema of table being
+          inserted is compatible with the existing table
 
         Examples
         --------
-        >>> import ibis
-        >>> import sqlite3
-        >>> ibis.options.interactive = True
-        >>> with sqlite3.connect("/tmp/sqlite.db") as con:
-        ...     con.execute("DROP TABLE IF EXISTS t")  # doctest: +ELLIPSIS
-        ...     con.execute("CREATE TABLE t (a INT, b TEXT)")  # doctest: +ELLIPSIS
-        ...     con.execute(
-        ...         "INSERT INTO t VALUES (1, 'a'), (2, 'b'), (3, 'c')"
-        ...     )  # doctest: +ELLIPSIS
-        <...>
-        >>> con = ibis.connect("duckdb://")
-        >>> t = con.read_sqlite(path="/tmp/sqlite.db", table_name="t")
-        >>> t
-        ┏━━━━━━━┳━━━━━━━━┓
-        ┃ a     ┃ b      ┃
-        ┡━━━━━━━╇━━━━━━━━┩
-        │ int64 │ string │
-        ├───────┼────────┤
-        │     1 │ a      │
-        │     2 │ b      │
-        │     3 │ c      │
-        └───────┴────────┘
+        t.insert(table_expr)
 
+        # Completely overwrite contents
+        t.insert(table_expr, overwrite=True)
         """
+        if isinstance(obj, pd.DataFrame):
+            writer, expr = _write_temp_dataframe(self._client, obj)
+        else:
+            expr = obj
 
-        if table_name is None:
-            raise ValueError("`table_name` is required when registering a sqlite table")
-        self._load_extensions(["sqlite"])
-
-        self._create_temp_view(
-            table_name,
-            sg.select(STAR).from_(
-                self.compiler.f.sqlite_scan(
-                    sg.to_identifier(str(path), quoted=True), table_name
-                )
-            ),
-        )
-
-        return self.table(table_name)
-
-    def attach(
-        self, path: str | Path, name: str | None = None, read_only: bool = False
-    ) -> None:
-        """Attach another DuckDB database to the current DuckDB session.
+        if values is not None:
+            raise NotImplementedError
+
+        if validate:
+            existing_schema = self.schema()
+            insert_schema = expr.schema()
+            if not insert_schema.equals(existing_schema):
+                _validate_compatible(insert_schema, existing_schema)
+
+        if partition is not None:
+            partition_schema = self.partition_schema()
+            expr = expr.drop(partition_schema.names)
+        else:
+            partition_schema = None
+
+        ast = build_ast(expr)
+        select = ast.queries[0]
+        statement = ddl.InsertSelect(self._qualified_name,
+                                     select,
+                                     partition=partition,
+                                     partition_schema=partition_schema,
+                                     overwrite=overwrite)
+        return self._execute(statement)
+
+    def load_data(self, path, overwrite=False, partition=None):
+        """
+        Wraps the LOAD DATA DDL statement. Loads data into an Impala table by
+        physically moving data files.
 
         Parameters
         ----------
-        path
-            Path to the database to attach.
-        name
-            Name to attach the database as. Defaults to the basename of `path`.
-        read_only
-            Whether to attach the database as read-only.
+        path : string
+        overwrite : boolean, default False
+          Overwrite the existing data in the entire table or indicated
+          partition
+        partition : dict, optional
+          If specified, the partition must already exist
 
+        Returns
+        -------
+        query : ImpalaQuery
         """
-        code = f"ATTACH '{path}'"
+        if partition is not None:
+            partition_schema = self.partition_schema()
+        else:
+            partition_schema = None
 
-        if name is not None:
-            name = sg.to_identifier(name).sql(self.name)
-            code += f" AS {name}"
+        stmt = ddl.LoadData(self._qualified_name, path,
+                            partition=partition,
+                            partition_schema=partition_schema)
 
-        if read_only:
-            code += " (READ_ONLY)"
+        return self._execute(stmt)
 
-        self.con.execute(code).fetchall()
+    @property
+    def name(self):
+        return self.op().name
 
-    def detach(self, name: str) -> None:
-        """Detach a database from the current DuckDB session.
+    def rename(self, new_name, database=None):
+        """
+        Rename table inside Impala. References to the old table are no longer
+        valid.
 
         Parameters
         ----------
-        name
-            The name of the database to detach.
+        new_name : string
+        database : string
+
+        Returns
+        -------
+        renamed : ImpalaTable
+        """
+        m = ddl.fully_qualified_re.match(new_name)
+        if not m and database is None:
+            database = self._database
+        statement = ddl.RenameTable(self._qualified_name, new_name,
+                                    new_database=database)
+        self._client._execute(statement)
+
+        op = self.op().change_name(statement.new_qualified_name)
+        return ImpalaTable(op)
+
+    def _execute(self, stmt):
+        return self._client._execute(stmt)
+
+    @property
+    def is_partitioned(self):
+        """
+        True if the table is partitioned
+        """
+        return self.metadata().is_partitioned
+
+    def partition_schema(self):
+        """
+        For partitioned tables, return the schema (names and types) for the
+        partition columns
 
+        Returns
+        -------
+        partition_schema : ibis Schema
         """
-        name = sg.to_identifier(name).sql(self.name)
-        self.con.execute(f"DETACH {name}").fetchall()
+        schema = self.schema()
+        name_to_type = dict(zip(schema.names, schema.types))
+
+        result = self.partitions()
 
-    def attach_sqlite(
-        self, path: str | Path, overwrite: bool = False, all_varchar: bool = False
-    ) -> None:
-        """Attach a SQLite database to the current DuckDB session.
+        partition_fields = []
+        for x in result.columns:
+            if x not in name_to_type:
+                break
+            partition_fields.append((x, name_to_type[x]))
+
+        pnames, ptypes = zip(*partition_fields)
+        return dt.Schema(pnames, ptypes)
+
+    def add_partition(self, spec, location=None):
+        """
+        Add a new table partition, creating any new directories in HDFS if
+        necessary.
+
+        Partition parameters can be set in a single DDL statement, or you can
+        use alter_partition to set them after the fact.
+
+        Returns
+        -------
+        None (for now)
+        """
+        part_schema = self.partition_schema()
+        stmt = ddl.AddPartition(self._qualified_name, spec, part_schema,
+                                location=location)
+        return self._execute(stmt)
+
+    def alter(self, location=None, format=None, tbl_properties=None,
+              serde_properties=None):
+        """
+        Change setting and parameters of the table.
 
         Parameters
         ----------
-        path
-            The path to the SQLite database.
-        overwrite
-            Allow overwriting any tables or views that already exist in your current
-            session with the contents of the SQLite database.
-        all_varchar
-            Set all SQLite columns to type `VARCHAR` to avoid type errors on ingestion.
+        location : string, optional
+          For partitioned tables, you may want the alter_partition function
+        format : string, optional
+        tbl_properties : dict, optional
+        serde_properties : dict, optional
 
-        Examples
-        --------
-        >>> import ibis
-        >>> import sqlite3
-        >>> with sqlite3.connect("/tmp/attach_sqlite.db") as con:
-        ...     con.execute("DROP TABLE IF EXISTS t")  # doctest: +ELLIPSIS
-        ...     con.execute("CREATE TABLE t (a INT, b TEXT)")  # doctest: +ELLIPSIS
-        ...     con.execute(
-        ...         "INSERT INTO t VALUES (1, 'a'), (2, 'b'), (3, 'c')"
-        ...     )  # doctest: +ELLIPSIS
-        <...>
-        >>> con = ibis.connect("duckdb://")
-        >>> con.list_tables()
-        []
-        >>> con.attach_sqlite("/tmp/attach_sqlite.db")
-        >>> con.list_tables()
-        ['t']
-
-        """
-        self.load_extension("sqlite")
-        with self._safe_raw_sql(f"SET GLOBAL sqlite_all_varchar={all_varchar}") as cur:
-            cur.execute(
-                f"CALL sqlite_attach('{path}', overwrite={overwrite})"
-            ).fetchall()
-
-    def register_filesystem(self, filesystem: AbstractFileSystem):
-        """Register an `fsspec` filesystem object with DuckDB.
-
-        This allow a user to read from any `fsspec` compatible filesystem using
-        `read_csv`, `read_parquet`, `read_json`, etc.
-
-
-        ::: {.callout-note}
-        Creating an `fsspec` filesystem requires that the corresponding
-        backend-specific `fsspec` helper library is installed.
-
-        e.g. to connect to Google Cloud Storage, `gcsfs` must be installed.
-        :::
+        Returns
+        -------
+        None (for now)
+        """
+        def _run_ddl(**kwds):
+            stmt = ddl.AlterTable(self._qualified_name, **kwds)
+            return self._execute(stmt)
+
+        return self._alter_table_helper(_run_ddl, location=location,
+                                        format=format,
+                                        tbl_properties=tbl_properties,
+                                        serde_properties=serde_properties)
+
+    def alter_partition(self, spec, location=None, format=None,
+                        tbl_properties=None,
+                        serde_properties=None):
+        """
+        Change setting and parameters of an existing partition
 
         Parameters
         ----------
-        filesystem
-            The fsspec filesystem object to register with DuckDB.
-            See https://duckdb.org/docs/guides/python/filesystems for details.
+        spec : dict or list
+          The partition keys for the partition being modified
+        location : string, optional
+        format : string, optional
+        tbl_properties : dict, optional
+        serde_properties : dict, optional
 
-        Examples
-        --------
-        >>> import ibis
-        >>> import fsspec
-        >>> gcs = fsspec.filesystem("gcs")
-        >>> con = ibis.duckdb.connect()
-        >>> con.register_filesystem(gcs)
-        >>> t = con.read_csv(
-        ...     "gcs://ibis-examples/data/band_members.csv.gz",
-        ...     table_name="band_members",
-        ... )
-        DatabaseTable: band_members
-          name string
-          band string
-
-        """
-        self.con.register_filesystem(filesystem)
-
-    def _run_pre_execute_hooks(self, expr: ir.Expr) -> None:
-        # Warn for any tables depending on RecordBatchReaders that have already
-        # started being consumed.
-        for t in expr.op().find(ops.PhysicalTable):
-            started = self._record_batch_readers_consumed.get(t.name)
-            if started is True:
-                warnings.warn(
-                    f"Table {t.name!r} is backed by a `pyarrow.RecordBatchReader` "
-                    "that has already been partially consumed. This may lead to "
-                    "unexpected results. Either recreate the table from a new "
-                    "`pyarrow.RecordBatchReader`, or use `Table.cache()`/"
-                    "`con.create_table()` to consume and store the results in "
-                    "the backend to reuse later."
-                )
-            elif started is False:
-                self._record_batch_readers_consumed[t.name] = True
-
-        if expr.op().find((ops.GeoSpatialUnOp, ops.GeoSpatialBinOp)):
-            self.load_extension("spatial")
-
-        super()._run_pre_execute_hooks(expr)
-
-    def _to_duckdb_relation(
-        self,
-        expr: ir.Expr,
-        *,
-        params: Mapping[ir.Scalar, Any] | None = None,
-        limit: int | str | None = None,
-    ):
-        """Preprocess the expr, and return a ``duckdb.DuckDBPyRelation`` object.
-
-        When retrieving in-memory results, it's faster to use `duckdb_con.sql`
-        than `duckdb_con.execute`, as the query planner can take advantage of
-        knowing the output type. Since the relation objects aren't compatible
-        with the dbapi, we choose to only use them in select internal methods
-        where performance might matter, and use the standard
-        `duckdb_con.execute` everywhere else.
-        """
-        self._run_pre_execute_hooks(expr)
-        table_expr = expr.as_table()
-        sql = self.compile(table_expr, limit=limit, params=params)
-        return self.con.sql(sql)
-
-    def to_pyarrow_batches(
-        self,
-        expr: ir.Expr,
-        *,
-        params: Mapping[ir.Scalar, Any] | None = None,
-        limit: int | str | None = None,
-        chunk_size: int = 1_000_000,
-        **_: Any,
-    ) -> pa.ipc.RecordBatchReader:
-        """Return a stream of record batches.
-
-        The returned `RecordBatchReader` contains a cursor with an unbounded lifetime.
-
-        For analytics use cases this is usually nothing to fret about. In some cases you
-        may need to explicit release the cursor.
-
-        Parameters
-        ----------
-        expr
-            Ibis expression
-        params
-            Bound parameters
-        limit
-            Limit the result to this number of rows
-        chunk_size
-            ::: {.callout-warning}
-            ## DuckDB returns 1024 size batches regardless of what argument is passed.
-            :::
-
-        """
-        self._run_pre_execute_hooks(expr)
-        table = expr.as_table()
-        sql = self.compile(table, limit=limit, params=params)
-
-        def batch_producer(cur):
-            yield from cur.fetch_record_batch(rows_per_batch=chunk_size)
-
-        result = self.raw_sql(sql)
-        return pa.ipc.RecordBatchReader.from_batches(
-            expr.as_table().schema().to_pyarrow(), batch_producer(result)
-        )
-
-    def to_pyarrow(
-        self,
-        expr: ir.Expr,
-        *,
-        params: Mapping[ir.Scalar, Any] | None = None,
-        limit: int | str | None = None,
-        **_: Any,
-    ) -> pa.Table:
-        table = self._to_duckdb_relation(expr, params=params, limit=limit).arrow()
-        return expr.__pyarrow_result__(table)
-
-    def execute(
-        self,
-        expr: ir.Expr,
-        params: Mapping | None = None,
-        limit: str | None = "default",
-        **_: Any,
-    ) -> Any:
-        """Execute an expression."""
-        import pandas as pd
-        import pyarrow.types as pat
-
-        table = self._to_duckdb_relation(expr, params=params, limit=limit).arrow()
-
-        df = pd.DataFrame(
-            {
-                name: (
-                    col.to_pylist()
-                    if (
-                        pat.is_nested(col.type)
-                        or
-                        # pyarrow / duckdb type null literals columns as int32?
-                        # but calling `to_pylist()` will render it as None
-                        col.null_count
-                    )
-                    else col.to_pandas(timestamp_as_object=True)
-                )
-                for name, col in zip(table.column_names, table.columns)
-            }
-        )
-        df = DuckDBPandasData.convert_table(df, expr.as_table().schema())
-        return expr.__pandas_result__(df)
-
-    @util.experimental
-    def to_torch(
-        self,
-        expr: ir.Expr,
-        *,
-        params: Mapping[ir.Scalar, Any] | None = None,
-        limit: int | str | None = None,
-        **kwargs: Any,
-    ) -> dict[str, torch.Tensor]:
-        """Execute an expression and return results as a dictionary of torch tensors.
-
-        Parameters
-        ----------
-        expr
-            Ibis expression to execute.
-        params
-            Parameters to substitute into the expression.
-        limit
-            An integer to effect a specific row limit. A value of `None` means no limit.
-        kwargs
-            Keyword arguments passed into the backend's `to_torch` implementation.
-
-        Returns
-        -------
-        dict[str, torch.Tensor]
-            A dictionary of torch tensors, keyed by column name.
-
-        """
-        return self._to_duckdb_relation(expr, params=params, limit=limit).torch()
-
-    @util.experimental
-    def to_parquet(
-        self,
-        expr: ir.Table,
-        path: str | Path,
-        *,
-        params: Mapping[ir.Scalar, Any] | None = None,
-        **kwargs: Any,
-    ) -> None:
-        """Write the results of executing the given expression to a parquet file.
-
-        This method is eager and will execute the associated expression
-        immediately.
-
-        Parameters
-        ----------
-        expr
-            The ibis expression to execute and persist to parquet.
-        path
-            The data source. A string or Path to the parquet file.
-        params
-            Mapping of scalar parameter expressions to value.
-        **kwargs
-            DuckDB Parquet writer arguments. See
-            https://duckdb.org/docs/data/parquet#writing-to-parquet-files for
-            details
+        Returns
+        -------
+        None (for now)
+        """
+        part_schema = self.partition_schema()
 
-        Examples
-        --------
-        Write out an expression to a single parquet file.
+        def _run_ddl(**kwds):
+            stmt = ddl.AlterPartition(self._qualified_name, spec,
+                                      part_schema, **kwds)
+            return self._execute(stmt)
 
-        >>> import ibis
-        >>> penguins = ibis.examples.penguins.fetch()
-        >>> con = ibis.get_backend(penguins)
-        >>> con.to_parquet(penguins, "/tmp/penguins.parquet")
+        return self._alter_table_helper(_run_ddl, location=location,
+                                        format=format,
+                                        tbl_properties=tbl_properties,
+                                        serde_properties=serde_properties)
 
-        Write out an expression to a hive-partitioned parquet file.
+    def _alter_table_helper(self, f, **alterations):
+        results = []
+        for k, v in alterations.items():
+            if v is None:
+                continue
+            result = f(**{k: v})
+            results.append(result)
+        return results
 
-        >>> import tempfile
-        >>> penguins = ibis.examples.penguins.fetch()
-        >>> con = ibis.get_backend(penguins)
+    def drop_partition(self, spec):
+        """
+        Drop an existing table partition
+        """
+        part_schema = self.partition_schema()
+        stmt = ddl.DropPartition(self._qualified_name, spec, part_schema)
+        return self._execute(stmt)
 
-        Partition on a single column.
+    def partitions(self):
+        """
+        Return a pandas.DataFrame giving information about this table's
+        partitions. Raises an exception if the table is not partitioned.
 
-        >>> con.to_parquet(penguins, tempfile.mkdtemp(), partition_by="year")
+        Returns
+        -------
+        partitions : pandas.DataFrame
+        """
+        return self._client.list_partitions(self._qualified_name)
 
-        Partition on multiple columns.
+    def stats(self):
+        """
+        Return results of SHOW TABLE STATS as a DataFrame. If not partitioned,
+        contains only one row
 
-        >>> con.to_parquet(penguins, tempfile.mkdtemp(), partition_by=("year", "island"))
+        Returns
+        -------
+        stats : pandas.DataFrame
+        """
+        return self._client.table_stats(self._qualified_name)
 
+    def column_stats(self):
         """
-        self._run_pre_execute_hooks(expr)
-        query = self.compile(expr, params=params)
-        args = ["FORMAT 'parquet'", *(f"{k.upper()} {v!r}" for k, v in kwargs.items())]
-        copy_cmd = f"COPY ({query}) TO {str(path)!r} ({', '.join(args)})"
-        with self._safe_raw_sql(copy_cmd):
-            pass
+        Return results of SHOW COLUMN STATS as a pandas DataFrame
 
-    @util.experimental
-    def to_csv(
-        self,
-        expr: ir.Table,
-        path: str | Path,
-        *,
-        params: Mapping[ir.Scalar, Any] | None = None,
-        header: bool = True,
-        **kwargs: Any,
-    ) -> None:
-        """Write the results of executing the given expression to a CSV file.
-
-        This method is eager and will execute the associated expression
-        immediately.
+        Returns
+        -------
+        column_stats : pandas.DataFrame
+        """
+        return self._client.column_stats(self._qualified_name)
 
-        Parameters
-        ----------
-        expr
-            The ibis expression to execute and persist to CSV.
-        path
-            The data source. A string or Path to the CSV file.
-        params
-            Mapping of scalar parameter expressions to value.
-        header
-            Whether to write the column names as the first line of the CSV file.
-        **kwargs
-            DuckDB CSV writer arguments. https://duckdb.org/docs/data/csv/overview.html#parameters
-
-        """
-        self._run_pre_execute_hooks(expr)
-        query = self.compile(expr, params=params)
-        args = [
-            "FORMAT 'csv'",
-            f"HEADER {int(header)}",
-            *(f"{k.upper()} {v!r}" for k, v in kwargs.items()),
-        ]
-        copy_cmd = f"COPY ({query}) TO {str(path)!r} ({', '.join(args)})"
-        with self._safe_raw_sql(copy_cmd):
-            pass
 
-    def _get_schema_using_query(self, query: str) -> sch.Schema:
-        with self._safe_raw_sql(f"DESCRIBE {query}") as cur:
-            rows = cur.fetch_arrow_table()
-
-        rows = rows.to_pydict()
-
-        type_mapper = self.compiler.type_mapper
-        return sch.Schema(
-            {
-                name: type_mapper.from_string(typ, nullable=null == "YES")
-                for name, typ, null in zip(
-                    rows["column_name"], rows["column_type"], rows["null"]
-                )
-            }
-        )
-
-    def _register_in_memory_tables(self, expr: ir.Expr) -> None:
-        for memtable in expr.op().find(ops.InMemoryTable):
-            self._register_in_memory_table(memtable)
-
-    def _register_in_memory_table(self, op: ops.InMemoryTable) -> None:
-        # only register if we haven't already done so
-        if (name := op.name) not in self.list_tables():
-            self.con.register(name, op.data.to_pyarrow(op.schema))
-
-    def _register_udfs(self, expr: ir.Expr) -> None:
-        con = self.con
-
-        for udf_node in expr.op().find(ops.ScalarUDF):
-            compile_func = getattr(
-                self, f"_compile_{udf_node.__input_type__.name.lower()}_udf"
-            )
-            with contextlib.suppress(duckdb.InvalidInputException):
-                con.remove_function(udf_node.__class__.__name__)
-
-            registration_func = compile_func(udf_node)
-            if registration_func is not None:
-                registration_func(con)
-
-    def _compile_udf(self, udf_node: ops.ScalarUDF):
-        func = udf_node.__func__
-        name = type(udf_node).__name__
-        type_mapper = self.compiler.type_mapper
-        input_types = [
-            type_mapper.to_string(param.annotation.pattern.dtype)
-            for param in udf_node.__signature__.parameters.values()
-        ]
-        output_type = type_mapper.to_string(udf_node.dtype)
-
-        def register_udf(con):
-            return con.create_function(
-                name,
-                func,
-                input_types,
-                output_type,
-                type=_UDF_INPUT_TYPE_MAPPING[udf_node.__input_type__],
-            )
-
-        return register_udf
-
-    _compile_python_udf = _compile_udf
-    _compile_pyarrow_udf = _compile_udf
-
-    def _get_temp_view_definition(self, name: str, definition: str) -> str:
-        return sge.Create(
-            this=sg.to_identifier(name, quoted=self.compiler.quoted),
-            kind="VIEW",
-            expression=definition,
-            replace=True,
-            properties=sge.Properties(expressions=[sge.TemporaryProperty()]),
-        )
+class ImpalaTemporaryTable(ops.DatabaseTable):
 
-    def _create_temp_view(self, table_name, source):
-        with self._safe_raw_sql(self._get_temp_view_definition(table_name, source)):
+    def __del__(self):
+        try:
+            self.drop()
+        except com.IbisError:
+            pass
+
+    def drop(self):
+        try:
+            self.source.drop_table(self.name)
+        except ImpylaError:
+            # database might have been dropped
             pass
+
+
+def _write_temp_dataframe(client, df):
+    from ibis.impala.pandas_interop import DataFrameWriter
+    writer = DataFrameWriter(client, df)
+    path = writer.write_temp_csv()
+    return writer, writer.delimited_table(path)
+
+
+def _validate_compatible(from_schema, to_schema):
+    if set(from_schema.names) != set(to_schema.names):
+        raise com.IbisInputError('Schemas have different names')
+
+    for name in from_schema:
+        lt = from_schema[name]
+        rt = to_schema[name]
+        if not rt.can_implicit_cast(lt):
+            raise com.IbisInputError('Cannot safely cast {0!r} to {1!r}'
+                                     .format(lt, rt))
+
+
+def _split_signature(x):
+    name, rest = x.split('(', 1)
+    return name, rest[:-1]
+
+_arg_type = re.compile('(.*)\.\.\.|([^\.]*)')
+
+
+class _type_parser(object):
+
+    NORMAL, IN_PAREN = 0, 1
+
+    def __init__(self, value):
+        self.value = value
+        self.state = self.NORMAL
+        self.buf = six.StringIO()
+        self.types = []
+        for c in value:
+            self._step(c)
+        self._push()
+
+    def _push(self):
+        val = self.buf.getvalue().strip()
+        if val:
+            self.types.append(val)
+        self.buf = six.StringIO()
+
+    def _step(self, c):
+        if self.state == self.NORMAL:
+            if c == '(':
+                self.state = self.IN_PAREN
+            elif c == ',':
+                self._push()
+                return
+        elif self.state == self.IN_PAREN:
+            if c == ')':
+                self.state = self.NORMAL
+        self.buf.write(c)
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `ibis_framework-9.0.0.dev686/ibis/backends/flink/__init__.py` & `ibis-framework-v0.6.0/ibis/impala/tests/test_ddl.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,1033 +1,1105 @@
-from __future__ import annotations
+# Copyright 2014 Cloudera Inc.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+# http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from copy import copy
+import gc
+
+import ibis
+import pandas as pd
+
+from posixpath import join as pjoin
+import pytest
+
+from ibis.expr.tests.mocks import MockConnection
+from ibis.compat import unittest, mock
+from ibis.impala import ddl
+from ibis.impala.compat import HS2Error, ImpylaError
+from ibis.impala.client import build_ast
+from ibis.impala.tests.common import IbisTestEnv, ImpalaE2E, connect_test
+from ibis.tests.util import assert_equal
+import ibis.common as com
+import ibis.util as util
+
+
+ENV = IbisTestEnv()
+
+
+class TestDropTable(unittest.TestCase):
+
+    def test_must_exist(self):
+        statement = ddl.DropTable('foo', database='bar', must_exist=True)
+        query = statement.compile()
+        expected = "DROP TABLE bar.`foo`"
+        assert query == expected
+
+        statement = ddl.DropTable('foo', database='bar', must_exist=False)
+        query = statement.compile()
+        expected = "DROP TABLE IF EXISTS bar.`foo`"
+        assert query == expected
+
+
+class TestInsertLoadData(unittest.TestCase):
+
+    def setUp(self):
+        self.con = MockConnection()
+        self.t = self.con.table('functional_alltypes')
+
+    def test_select_basics(self):
+        name = 'testing123456'
+
+        expr = self.t.limit(10)
+        select, _ = _get_select(expr)
+
+        stmt = ddl.InsertSelect(name, select, database='foo')
+        result = stmt.compile()
+
+        expected = """\
+INSERT INTO foo.`testing123456`
+SELECT *
+FROM functional_alltypes
+LIMIT 10"""
+        assert result == expected
+
+        stmt = ddl.InsertSelect(name, select, database='foo', overwrite=True)
+        result = stmt.compile()
+
+        expected = """\
+INSERT OVERWRITE foo.`testing123456`
+SELECT *
+FROM functional_alltypes
+LIMIT 10"""
+        assert result == expected
+
+    def test_load_data_unpartitioned(self):
+        path = '/path/to/data'
+        stmt = ddl.LoadData('functional_alltypes', path, database='foo')
+
+        result = stmt.compile()
+        expected = ("LOAD DATA INPATH '/path/to/data' "
+                    "INTO TABLE foo.`functional_alltypes`")
+        assert result == expected
+
+        stmt.overwrite = True
+        result = stmt.compile()
+        expected = ("LOAD DATA INPATH '/path/to/data' "
+                    "OVERWRITE INTO TABLE foo.`functional_alltypes`")
+        assert result == expected
+
+    def test_load_data_partitioned(self):
+        path = '/path/to/data'
+        part = {'year': 2007, 'month': 7}
+        part_schema = ibis.schema([('year', 'int32'), ('month', 'int32')])
+        stmt = ddl.LoadData('functional_alltypes', path,
+                            database='foo',
+                            partition=part,
+                            partition_schema=part_schema)
+
+        result = stmt.compile()
+        expected = """\
+LOAD DATA INPATH '/path/to/data' INTO TABLE foo.`functional_alltypes`
+PARTITION (year=2007, month=7)"""
+        assert result == expected
+
+        stmt.overwrite = True
+        result = stmt.compile()
+        expected = """\
+LOAD DATA INPATH '/path/to/data' OVERWRITE INTO TABLE foo.`functional_alltypes`
+PARTITION (year=2007, month=7)"""
+        assert result == expected
+
+    def test_select_overwrite(self):
+        pass
 
-import itertools
-from typing import TYPE_CHECKING, Any
 
-import sqlglot as sg
-import sqlglot.expressions as sge
+class TestCacheTable(unittest.TestCase):
+
+    def test_pool_name(self):
+        statement = ddl.CacheTable('foo', database='bar')
+        query = statement.compile()
+        expected = "ALTER TABLE bar.`foo` SET CACHED IN 'default'"
+        assert query == expected
+
+        statement = ddl.CacheTable('foo', database='bar', pool='my_pool')
+        query = statement.compile()
+        expected = "ALTER TABLE bar.`foo` SET CACHED IN 'my_pool'"
+        assert query == expected
+
+
+class TestAlterTablePartition(unittest.TestCase):
+
+    def setUp(self):
+        self.part_schema = ibis.schema([('year', 'int32'),
+                                        ('month', 'int32')])
+        self.table_name = 'tbl'
+
+    def test_add_partition(self):
+        stmt = ddl.AddPartition(self.table_name,
+                                {'year': 2007, 'month': 4},
+                                self.part_schema)
+
+        result = stmt.compile()
+        expected = 'ALTER TABLE tbl ADD PARTITION (year=2007, month=4)'
+        assert result == expected
+
+    def test_drop_partition(self):
+        stmt = ddl.DropPartition(self.table_name,
+                                 {'year': 2007, 'month': 4},
+                                 self.part_schema)
+
+        result = stmt.compile()
+        expected = 'ALTER TABLE tbl DROP PARTITION (year=2007, month=4)'
+        assert result == expected
+
+    def test_add_partition_with_props(self):
+        props = dict(
+            location='/users/foo/my-data'
+        )
+        stmt = ddl.AddPartition(self.table_name,
+                                {'year': 2007, 'month': 4},
+                                self.part_schema, **props)
+
+        result = stmt.compile()
+        expected = """\
+ALTER TABLE tbl ADD PARTITION (year=2007, month=4)
+LOCATION '/users/foo/my-data'"""
+        assert result == expected
+
+    def test_alter_partition_properties(self):
+        part = {'year': 2007, 'month': 4}
+
+        def _get_ddl_string(props):
+            stmt = ddl.AlterPartition(self.table_name, part,
+                                      self.part_schema,
+                                      **props)
+            return stmt.compile()
+
+        result = _get_ddl_string({'location': '/users/foo/my-data'})
+        expected = """\
+ALTER TABLE tbl PARTITION (year=2007, month=4)
+SET LOCATION '/users/foo/my-data'"""
+        assert result == expected
+
+        result = _get_ddl_string({'format': 'avro'})
+        expected = """\
+ALTER TABLE tbl PARTITION (year=2007, month=4)
+SET FILEFORMAT AVRO"""
+        assert result == expected
+
+        result = _get_ddl_string({'tbl_properties': {
+            'bar': 2, 'foo': '1'
+        }})
+        expected = """\
+ALTER TABLE tbl PARTITION (year=2007, month=4)
+SET TBLPROPERTIES ('bar'='2', 'foo'='1')"""
+        assert result == expected
+
+        result = _get_ddl_string({'serde_properties': {'baz': 3}})
+        expected = """\
+ALTER TABLE tbl PARTITION (year=2007, month=4)
+SET SERDEPROPERTIES ('baz'='3')"""
+        assert result == expected
+
+    def test_alter_table_properties(self):
+        part = {'year': 2007, 'month': 4}
+
+        def _get_ddl_string(props):
+            stmt = ddl.AlterPartition(self.table_name, part,
+                                      self.part_schema,
+                                      **props)
+            return stmt.compile()
+
+        result = _get_ddl_string({'location': '/users/foo/my-data'})
+        expected = """\
+ALTER TABLE tbl PARTITION (year=2007, month=4)
+SET LOCATION '/users/foo/my-data'"""
+        assert result == expected
+
+        result = _get_ddl_string({'format': 'avro'})
+        expected = """\
+ALTER TABLE tbl PARTITION (year=2007, month=4)
+SET FILEFORMAT AVRO"""
+        assert result == expected
+
+        result = _get_ddl_string({'tbl_properties': {
+            'bar': 2, 'foo': '1'
+        }})
+        expected = """\
+ALTER TABLE tbl PARTITION (year=2007, month=4)
+SET TBLPROPERTIES ('bar'='2', 'foo'='1')"""
+        assert result == expected
+
+        result = _get_ddl_string({'serde_properties': {'baz': 3}})
+        expected = """\
+ALTER TABLE tbl PARTITION (year=2007, month=4)
+SET SERDEPROPERTIES ('baz'='3')"""
+        assert result == expected
+
+
+class TestCreateTable(unittest.TestCase):
+
+    def setUp(self):
+        self.con = MockConnection()
+
+        self.t = t = self.con.table('functional_alltypes')
+        self.expr = t[t.bigint_col > 0]
+
+    def test_create_external_table_as(self):
+        path = '/path/to/table'
+        select = build_ast(self.con.table('test1')).queries[0]
+        statement = ddl.CTAS('another_table',
+                             select,
+                             external=True,
+                             can_exist=False,
+                             path=path,
+                             database='foo')
+        result = statement.compile()
+
+        expected = """\
+CREATE EXTERNAL TABLE foo.`another_table`
+STORED AS PARQUET
+LOCATION '{0}'
+AS
+SELECT *
+FROM test1""".format(path)
+        assert result == expected
+
+    def test_create_table_with_location(self):
+        path = '/path/to/table'
+        schema = ibis.schema([('foo', 'string'),
+                              ('bar', 'int8'),
+                              ('baz', 'int16')])
+        statement = ddl.CreateTableWithSchema('another_table', schema,
+                                              ddl.NoFormat(),
+                                              can_exist=False,
+                                              path=path, database='foo')
+        result = statement.compile()
+
+        expected = """\
+CREATE TABLE foo.`another_table`
+(`foo` string,
+ `bar` tinyint,
+ `baz` smallint)
+LOCATION '{0}'""".format(path)
+        assert result == expected
+
+    def test_create_table_like_parquet(self):
+        directory = '/path/to/'
+        path = '/path/to/parquetfile'
+        statement = ddl.CreateTableParquet('new_table',
+                                           directory,
+                                           example_file=path,
+                                           can_exist=True,
+                                           database='foo')
+
+        result = statement.compile()
+        expected = """\
+CREATE EXTERNAL TABLE IF NOT EXISTS foo.`new_table`
+LIKE PARQUET '{0}'
+STORED AS PARQUET
+LOCATION '{1}'""".format(path, directory)
+
+        assert result == expected
+
+    def test_create_table_parquet_like_other(self):
+        # alternative to "LIKE PARQUET"
+        directory = '/path/to/'
+        example_table = 'db.other'
+
+        statement = ddl.CreateTableParquet('new_table',
+                                           directory,
+                                           example_table=example_table,
+                                           can_exist=True,
+                                           database='foo')
+
+        result = statement.compile()
+        expected = """\
+CREATE EXTERNAL TABLE IF NOT EXISTS foo.`new_table`
+LIKE {0}
+STORED AS PARQUET
+LOCATION '{1}'""".format(example_table, directory)
+
+        assert result == expected
+
+    def test_create_table_parquet_with_schema(self):
+        directory = '/path/to/'
+
+        schema = ibis.schema([('foo', 'string'),
+                              ('bar', 'int8'),
+                              ('baz', 'int16')])
+
+        statement = ddl.CreateTableParquet('new_table',
+                                           directory,
+                                           schema=schema,
+                                           external=True,
+                                           can_exist=True,
+                                           database='foo')
+
+        result = statement.compile()
+        expected = """\
+CREATE EXTERNAL TABLE IF NOT EXISTS foo.`new_table`
+(`foo` string,
+ `bar` tinyint,
+ `baz` smallint)
+STORED AS PARQUET
+LOCATION '{0}'""".format(directory)
+
+        assert result == expected
+
+    def test_create_table_delimited(self):
+        path = '/path/to/files/'
+        schema = ibis.schema([('a', 'string'),
+                              ('b', 'int32'),
+                              ('c', 'double'),
+                              ('d', 'decimal(12,2)')])
+
+        stmt = ddl.CreateTableDelimited('new_table', path, schema,
+                                        delimiter='|',
+                                        escapechar='\\',
+                                        lineterminator='\0',
+                                        database='foo',
+                                        can_exist=True)
+
+        result = stmt.compile()
+        expected = """\
+CREATE EXTERNAL TABLE IF NOT EXISTS foo.`new_table`
+(`a` string,
+ `b` int,
+ `c` double,
+ `d` decimal(12,2))
+ROW FORMAT DELIMITED
+FIELDS TERMINATED BY '|'
+ESCAPED BY '\\'
+LINES TERMINATED BY '\0'
+LOCATION '{0}'""".format(path)
+        assert result == expected
+
+    def test_create_external_table_avro(self):
+        path = '/path/to/files/'
+
+        avro_schema = {
+            'fields': [
+                {'name': 'a', 'type': 'string'},
+                {'name': 'b', 'type': 'int'},
+                {'name': 'c', 'type': 'double'},
+                {"type": "bytes",
+                 "logicalType": "decimal",
+                 "precision": 4,
+                 "scale": 2,
+                 'name': 'd'}
+            ],
+            'name': 'my_record',
+            'type': 'record'
+        }
 
-import ibis.common.exceptions as exc
-import ibis.expr.operations as ops
-import ibis.expr.schema as sch
-import ibis.expr.types as ir
-from ibis.backends import CanCreateDatabase, NoUrl
-from ibis.backends.flink.compiler import FlinkCompiler
-from ibis.backends.flink.ddl import (
-    CreateDatabase,
-    CreateTableWithSchema,
-    DropDatabase,
-    DropTable,
-    DropView,
-    InsertSelect,
-    RenameTable,
-)
-from ibis.backends.sql import SQLBackend
-from ibis.backends.tests.errors import Py4JJavaError
-from ibis.expr.operations.udf import InputType
-from ibis.util import gen_name
-
-if TYPE_CHECKING:
-    from collections.abc import Mapping
-    from pathlib import Path
-
-    import pandas as pd
-    import pyarrow as pa
-    from pyflink.table import Table, TableEnvironment
-    from pyflink.table.table_result import TableResult
-
-    from ibis.expr.api import Watermark
-
-_INPUT_TYPE_TO_FUNC_TYPE = {InputType.PYTHON: "general", InputType.PANDAS: "pandas"}
-
-
-class Backend(SQLBackend, CanCreateDatabase, NoUrl):
-    name = "flink"
-    compiler = FlinkCompiler()
-    supports_temporary_tables = True
-    supports_python_udfs = True
-
-    @property
-    def dialect(self):
-        # TODO: remove when ported to sqlglot
-        return self.compiler.dialect
-
-    def do_connect(self, table_env: TableEnvironment) -> None:
-        """Create a Flink `Backend` for use with Ibis.
-
-        Parameters
-        ----------
-        table_env
-            A table environment.
-
-        Examples
-        --------
-        >>> import ibis
-        >>> from pyflink.table import EnvironmentSettings, TableEnvironment
-        >>> table_env = TableEnvironment.create(EnvironmentSettings.in_streaming_mode())
-        >>> ibis.flink.connect(table_env)
-        <ibis.backends.flink.Backend at 0x...>
+        stmt = ddl.CreateTableAvro('new_table', path, avro_schema,
+                                   database='foo', can_exist=True)
 
-        """
-        self._table_env = table_env
+        result = stmt.compile()
+        expected = """\
+CREATE EXTERNAL TABLE IF NOT EXISTS foo.`new_table`
+STORED AS AVRO
+LOCATION '%s'
+TBLPROPERTIES ('avro.schema.literal'='{
+  "fields": [
+    {
+      "name": "a",
+      "type": "string"
+    },
+    {
+      "name": "b",
+      "type": "int"
+    },
+    {
+      "name": "c",
+      "type": "double"
+    },
+    {
+      "logicalType": "decimal",
+      "name": "d",
+      "precision": 4,
+      "scale": 2,
+      "type": "bytes"
+    }
+  ],
+  "name": "my_record",
+  "type": "record"
+}')""" % path
+        assert result == expected
+
+    def test_create_table_parquet(self):
+        statement = _create_table('some_table', self.expr,
+                                  database='bar',
+                                  can_exist=False)
+        result = statement.compile()
+
+        expected = """\
+CREATE TABLE bar.`some_table`
+STORED AS PARQUET
+AS
+SELECT *
+FROM functional_alltypes
+WHERE `bigint_col` > 0"""
+        assert result == expected
+
+    def test_no_overwrite(self):
+        statement = _create_table('tname', self.expr, can_exist=True)
+        result = statement.compile()
+
+        expected = """\
+CREATE TABLE IF NOT EXISTS `tname`
+STORED AS PARQUET
+AS
+SELECT *
+FROM functional_alltypes
+WHERE `bigint_col` > 0"""
+        assert result == expected
+
+    def test_avro_other_formats(self):
+        statement = _create_table('tname', self.t, format='avro',
+                                  can_exist=True)
+        result = statement.compile()
+        expected = """\
+CREATE TABLE IF NOT EXISTS `tname`
+STORED AS AVRO
+AS
+SELECT *
+FROM functional_alltypes"""
+        assert result == expected
 
-    def disconnect(self) -> None:
+        self.assertRaises(ValueError, _create_table, 'tname', self.t,
+                          format='foo')
+
+    def test_partition_by(self):
         pass
 
-    def raw_sql(self, query: str) -> TableResult:
-        return self._table_env.execute_sql(query)
 
-    def _get_schema_using_query(self, query: str) -> sch.Schema:
-        from pyflink.table.types import create_arrow_schema
+class TestDDLE2E(ImpalaE2E, unittest.TestCase):
 
-        table = self._table_env.sql_query(query)
-        schema = table.get_schema()
-        pa_schema = create_arrow_schema(
-            schema.get_field_names(), schema.get_field_data_types()
-        )
-        return sch.Schema.from_pyarrow(pa_schema)
+    @classmethod
+    def setUpClass(cls):
+        ImpalaE2E.setup_e2e(cls)
 
-    def list_databases(self, like: str | None = None) -> list[str]:
-        databases = self._table_env.list_databases()
-        return self._filter_with_like(databases, like)
-
-    @property
-    def current_catalog(self) -> str:
-        return self._table_env.get_current_catalog()
-
-    @property
-    def current_database(self) -> str:
-        return self._table_env.get_current_database()
-
-    def create_database(
-        self,
-        name: str,
-        db_properties: dict | None = None,
-        catalog: str | None = None,
-        force: bool = False,
-    ) -> None:
-        """Create a new database.
-
-        Parameters
-        ----------
-        name : str
-            Name of the new database.
-        db_properties : dict, optional
-            Properties of the database. Accepts dictionary of key-value pairs
-            (key1=val1, key2=val2, ...).
-        catalog : str, optional
-            Name of the catalog in which the new database will be created.
-        force : bool, optional
-            If `False`, an exception is raised if the database already exists.
-
-        """
-        statement = CreateDatabase(
-            name=name, db_properties=db_properties, catalog=catalog, can_exist=force
-        )
-        self.raw_sql(statement.compile())
+        cls.path_uuid = 'change-location-{0}'.format(util.guid())
+        fake_path = pjoin(cls.tmp_dir, cls.path_uuid)
 
-    def drop_database(
-        self, name: str, catalog: str | None = None, force: bool = False
-    ) -> None:
-        """Drop a database with name `name`.
-
-        Parameters
-        ----------
-        name : str
-            Database to drop.
-        catalog : str, optional
-            Name of the catalog from which the database will be dropped.
-        force : bool, optional
-            If `False`, an exception is raised if the database does not exist.
-
-        """
-        statement = DropDatabase(name=name, catalog=catalog, must_exist=not force)
-        self.raw_sql(statement.compile())
-
-    def list_tables(
-        self,
-        like: str | None = None,
-        *,
-        database: str | None = None,
-        catalog: str | None = None,
-        temp: bool = False,
-    ) -> list[str]:
-        """Return the list of table/view names.
-
-        Return the list of table/view names in the `database` and `catalog`. If
-        `database`/`catalog` are not specified, their default values will be
-        used. Temporary tables can only be listed for the default database and
-        catalog, hence `database` and `catalog` are ignored if `temp` is True.
-
-        Parameters
-        ----------
-        like : str, optional
-            A pattern in Python's regex format.
-        temp : bool, optional
-            Whether to list temporary tables or permanent tables.
-        database : str, optional
-            The database to list tables of, if not the current one.
-        catalog : str, optional
-            The catalog to list tables of, if not the current one.
-
-        Returns
-        -------
-        list[str]
-            The list of the table/view names that match the pattern `like`.
-
-        """
-        catalog = catalog or self.current_catalog
-        database = database or self.current_database
-
-        # The following is equivalent to the SQL query string `SHOW TABLES FROM|IN`,
-        # but executing the SQL string directly yields a `TableResult` object
-        if temp:
-            # Note (mehmet): TableEnvironment does not provide a function to list
-            # the temporary tables in a given catalog and database.
-            # Ref: https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/table/api/TableEnvironment.html
-            tables = self._table_env.list_temporary_tables()
-        else:
-            # Note (mehmet): `listTables` returns both tables and views.
-            # Ref: Docstring for pyflink/table/table_environment.py:list_tables()
-            tables = self._table_env._j_tenv.listTables(catalog, database)
-
-        return self._filter_with_like(tables, like)
-
-    def list_views(
-        self,
-        like: str | None = None,
-        temp: bool = False,
-    ) -> list[str]:
-        """Return the list of view names.
-
-        Return the list of view names.
-
-        Parameters
-        ----------
-        like : str, optional
-            A pattern in Python's regex format.
-        temp : bool, optional
-            Whether to list temporary views or permanent views.
-
-        Returns
-        -------
-        list[str]
-            The list of the view names that match the pattern `like`.
-
-        """
-
-        if temp:
-            views = self._table_env.list_temporary_views()
-        else:
-            views = self._table_env.list_views()
-
-        return self._filter_with_like(views, like)
-
-    def table(
-        self,
-        name: str,
-        database: str | None = None,
-        catalog: str | None = None,
-    ) -> ir.Table:
-        """Return a table expression from a table or view in the database.
-
-        Parameters
-        ----------
-        name
-            Table name.
-        database
-            Database in which the table resides.
-        catalog
-            Catalog in which the table resides.
-
-        Returns
-        -------
-        Table
-            Table named `name` from `database`
-
-        """
-        if database is not None and not isinstance(database, str):
-            raise exc.IbisTypeError(
-                f"`database` must be a string; got {type(database)}"
-            )
-        schema = self.get_schema(name, catalog=catalog, database=database)
-        node = ops.DatabaseTable(
-            name,
-            schema=schema,
-            source=self,
-            namespace=ops.Namespace(catalog=catalog, database=database),
-        )
-        return node.to_expr()
+        cls.table_name = 'table_{0}'.format(util.guid())
 
-    def get_schema(
-        self,
-        table_name: str,
-        *,
-        catalog: str | None = None,
-        database: str | None = None,
-    ) -> sch.Schema:
-        """Return a Schema object for the indicated table and database.
-
-        Parameters
-        ----------
-        table_name : str
-            Table name.
-        catalog : str, optional
-            Catalog name.
-        database : str, optional
-            Database name.
-
-        Returns
-        -------
-        sch.Schema
-            Ibis schema
+        schema = ibis.schema([('foo', 'string'), ('bar', 'int64')])
 
-        """
-        from pyflink.table.types import create_arrow_schema
+        cls.con.create_table(cls.table_name,
+                             database=cls.tmp_db,
+                             schema=schema,
+                             format='parquet',
+                             external=True,
+                             location=fake_path)
+        cls.table = cls.con.table(cls.table_name, database=cls.tmp_db)
 
-        from ibis.backends.flink.datatypes import get_field_data_types
+    @classmethod
+    def tearDownClass(cls):
+        cls.con.drop_table(cls.table_name, database=cls.tmp_db)
+        ImpalaE2E.teardown_e2e(cls)
 
-        qualified_name = sg.table(table_name, db=catalog, catalog=database).sql(
-            self.name
-        )
-        table = self._table_env.from_path(qualified_name)
-        pyflink_schema = table.get_schema()
+    def test_list_databases(self):
+        assert len(self.con.list_databases()) > 0
 
-        return sch.Schema.from_pyarrow(
-            create_arrow_schema(
-                pyflink_schema.get_field_names(), get_field_data_types(pyflink_schema)
-            )
-        )
+    def test_list_tables(self):
+        assert len(self.con.list_tables(database=self.test_data_db)) > 0
+        assert len(self.con.list_tables(like='*nat*',
+                                        database=self.test_data_db)) > 0
 
-    @property
-    def version(self) -> str:
-        import pyflink.version
-
-        return pyflink.version.__version__
-
-    def _register_udfs(self, expr: ir.Expr) -> None:
-        for udf_node in expr.op().find(ops.ScalarUDF):
-            register_func = getattr(
-                self, f"_compile_{udf_node.__input_type__.name.lower()}_udf"
-            )
-            register_func(udf_node)
-
-    def _register_udf(self, udf_node: ops.ScalarUDF):
-        import pyflink.table.udf
-
-        from ibis.backends.flink.datatypes import FlinkType
-
-        name = type(udf_node).__name__
-        self._table_env.drop_temporary_function(name)
-        udf = pyflink.table.udf.udf(
-            udf_node.__func__,
-            result_type=FlinkType.from_ibis(udf_node.dtype),
-            func_type=_INPUT_TYPE_TO_FUNC_TYPE[udf_node.__input_type__],
-        )
-        self._table_env.create_temporary_function(name, udf)
+    def test_set_database(self):
+        # create new connection with no default db set
+        env = copy(ENV)
+        env.test_data_db = None
+        con = connect_test(env)
+        self.assertRaises(Exception, con.table, 'functional_alltypes')
+        con.set_database(self.test_data_db)
+        con.table('functional_alltypes')
 
-    _compile_pandas_udf = _register_udf
-    _compile_python_udf = _register_udf
+    def test_tables_robust_to_set_database(self):
+        db_name = '__ibis_test_{0}'.format(util.guid())
 
-    def compile(
-        self,
-        expr: ir.Expr,
-        params: Mapping[ir.Expr, Any] | None = None,
-        pretty: bool = False,
-        **_: Any,
-    ) -> Any:
-        """Compile an Ibis expression to Flink."""
-        return super().compile(
-            expr, params=params, pretty=pretty
-        )  # Discard `limit` and other kwargs.
-
-    def _to_sqlglot(
-        self, expr: ir.Expr, params: Mapping[ir.Expr, Any] | None = None, **_: Any
-    ) -> str:
-        return super()._to_sqlglot(expr, params=params)
-
-    def execute(self, expr: ir.Expr, **kwargs: Any) -> Any:
-        """Execute an expression."""
-        self._register_udfs(expr)
-
-        table_expr = expr.as_table()
-        sql = self.compile(table_expr, **kwargs)
-        df = self._table_env.sql_query(sql).to_pandas()
-
-        return expr.__pandas_result__(df)
-
-    def create_table(
-        self,
-        name: str,
-        obj: pd.DataFrame | pa.Table | ir.Table | None = None,
-        *,
-        schema: sch.Schema | None = None,
-        database: str | None = None,
-        catalog: str | None = None,
-        tbl_properties: dict | None = None,
-        watermark: Watermark | None = None,
-        primary_key: str | list[str] | None = None,
-        temp: bool = False,
-        overwrite: bool = False,
-    ) -> ir.Table:
-        """Create a new table in Flink.
-
-        In Flink, tables can be either virtual (VIEWS) or regular (TABLES).
-        VIEWS can be created from an existing Table object, usually the result
-        of a Table API or SQL query. TABLES describe external data, such as a
-        file, database table, or message queue. In other words, TABLES refer
-        explicitly to tables constructed directly from source/sink connectors.
-
-        When `obj` is in-memory (e.g., Dataframe), currently this function can
-        create only a TEMPORARY VIEW. If `obj` is in-memory and `temp` is False,
-        it will raise an error.
-
-        Parameters
-        ----------
-        name
-            Name of the new table.
-        obj
-            An Ibis table expression, pandas DataFrame, or PyArrow Table that will
-            be used to extract the schema and the data of the new table. An
-            optional `schema` can be used to override the schema.
-        schema
-            The schema for the new table. Required if `obj` is not provided.
-        database
-            Name of the database where the table will be created, if not the
-            default.
-        catalog
-            Name of the catalog where the table will be created, if not the
-            default.
-        tbl_properties
-            Table properties used to create a table source/sink. The properties
-            are usually used to find and create the underlying connector. Accepts
-            dictionary of key-value pairs (key1=val1, key2=val2, ...).
-        watermark
-            Watermark strategy for the table, only applicable on sources.
-        primary_key
-            A single column or a list of columns to be marked as primary. Raises
-            an error if the column(s) in `primary_key` is NOT a subset of the
-            columns in `schema`. Primary keys must be non-nullable in Flink and
-            the columns indicated as primary key will be designated as non-nullable.
-        temp
-            Whether a table is temporary or not.
-        overwrite
-            Whether to clobber existing data.
-
-        Returns
-        -------
-        Table
-            The table that was created.
-
-        """
-        import pandas as pd
-        import pyarrow as pa
-        import pyarrow_hotfix  # noqa: F401
-
-        import ibis.expr.types as ir
-
-        if obj is None and schema is None:
-            raise exc.IbisError("`schema` or `obj` is required")
-        if isinstance(obj, (pd.DataFrame, pa.Table)) and not temp:
-            raise exc.IbisError(
-                "`temp` cannot be False when `obj` is in-memory. "
-                "Currently can create only TEMPORARY VIEW for in-memory data."
-            )
-
-        if overwrite:
-            if self.list_tables(like=name, temp=temp):
-                self.drop_table(
-                    name=name,
-                    catalog=catalog,
-                    database=database,
-                    temp=temp,
-                    force=True,
-                )
-
-        # In-memory data is created as views in `pyflink`
-        if obj is not None:
-            if isinstance(obj, pd.DataFrame):
-                dataframe = obj
-
-            elif isinstance(obj, pa.Table):
-                dataframe = obj.to_pandas()
-
-            elif isinstance(obj, ir.Table):
-                # Note (mehmet): If obj points to in-memory data, we create a view.
-                # Other cases are unsupported for now, e.g., obj is of UnboundTable.
-                # See TODO right below for more context on how we handle in-memory data.
-                op = obj.op()
-                if isinstance(op, ops.InMemoryTable):
-                    dataframe = op.data.to_frame()
-                else:
-                    raise exc.IbisError(
-                        "`obj` is of type ibis.expr.types.Table but it is not in-memory. "
-                        "Currently, only in-memory tables are supported. "
-                        "See ibis.memtable() for info on creating in-memory table."
-                    )
-            else:
-                raise exc.IbisError(f"Unsupported `obj` type: {type(obj)}")
-
-            # TODO (mehmet): Flink requires a source connector to create regular tables.
-            # In-memory data can only be created as a view (virtual table). So we decided
-            # to create views for in-memory data. Ideally, this function should only create
-            # tables. However, for that, we would need the notion of a "default" table,
-            # which may not be ideal. We plan to get back to this later.
-            # Ref: https://github.com/ibis-project/ibis/pull/7479#discussion_r1416237088
-            return self.create_view(
-                name=name,
-                obj=dataframe,
-                schema=schema,
-                database=database,
-                catalog=catalog,
-                temp=temp,
-                overwrite=overwrite,
-            )
-
-        # External data is created as tables in `pyflink`
-        else:  # obj is None, schema is not None
-            if not tbl_properties:
-                raise exc.IbisError(
-                    "`tbl_properties` is required when creating table with schema"
-                )
-            elif (
-                "connector" not in tbl_properties or tbl_properties["connector"] is None
-            ):
-                raise exc.IbisError("connector must be defined in `tbl_properties`")
-
-            # TODO (mehmet): Given that we rely on default catalog if one is not specified,
-            # is there any point to support temporary tables?
-            statement = CreateTableWithSchema(
-                table_name=name,
-                schema=schema,
-                tbl_properties=tbl_properties,
-                watermark=watermark,
-                primary_key=primary_key,
-                temporary=temp,
-                database=database,
-                catalog=catalog,
-            )
-            sql = statement.compile()
-            self.raw_sql(sql)
-
-            return self.table(name, database=database, catalog=catalog)
-
-    def drop_table(
-        self,
-        name: str,
-        *,
-        database: str | None = None,
-        catalog: str | None = None,
-        temp: bool = False,
-        force: bool = False,
-    ) -> None:
-        """Drop a table.
-
-        Parameters
-        ----------
-        name
-            Name of the table to drop.
-        database
-            Name of the database where the table exists, if not the default.
-        catalog
-            Name of the catalog where the table exists, if not the default.
-        temp
-            Whether the table is temporary or not.
-        force
-            If `False`, an exception is raised if the table does not exist.
-
-        """
-        statement = DropTable(
-            table_name=name,
-            database=database,
-            catalog=catalog,
-            must_exist=not force,
-            temporary=temp,
-        )
-        self.raw_sql(statement.compile())
+        self.con.create_database(db_name)
+        self.temp_databases.append(db_name)
 
-    def rename_table(
-        self,
-        old_name: str,
-        new_name: str,
-        force: bool = True,
-    ) -> None:
-        """Rename an existing table.
-
-        Parameters
-        ----------
-        old_name
-            The old name of the table.
-        new_name
-            The new name of the table.
-        force
-            If `False`, an exception is raised if the table does not exist.
-
-        """
-        statement = RenameTable(
-            old_name=old_name,
-            new_name=new_name,
-            must_exist=not force,
-        )
-        sql = statement.compile()
-        self.raw_sql(sql)
+        table = self.con.table('functional_alltypes')
 
-    def create_view(
-        self,
-        name: str,
-        obj: pd.DataFrame | ir.Table,
-        *,
-        schema: sch.Schema | None = None,
-        database: str | None = None,
-        catalog: str | None = None,
-        force: bool = False,
-        temp: bool = False,
-        overwrite: bool = False,
-    ) -> ir.Table:
-        """Create a new view from a dataframe or table.
-
-        When `obj` is in-memory (e.g., Dataframe), currently this function can
-        create only a TEMPORARY VIEW. If `obj` is in-memory and `temp` is False,
-        it will raise an error.
-
-        Parameters
-        ----------
-        name
-            Name of the new view.
-        obj
-            An Ibis table expression that will be used to create the view.
-        schema
-            The schema for the new view.
-        database
-            Name of the database where the view will be created, if not
-            provided the database's default is used.
-        catalog
-            Name of the catalog where the table exists, if not the default.
-        force
-            If `False`, an exception is raised if the table is already present.
-        temp
-            Whether the table is temporary or not.
-        overwrite
-            If `True`, remove the existing view, and create a new one.
-
-        Returns
-        -------
-        Table
-            The view that was created.
-
-        """
-        import pandas as pd
-
-        from ibis.backends.flink.datatypes import FlinkRowSchema
-
-        if isinstance(obj, pd.DataFrame) and not temp:
-            raise exc.IbisError(
-                "`temp` cannot be False when `obj` is in-memory. "
-                "Currently supports creating only temporary view for in-memory data."
-            )
-
-        if overwrite and self.list_views(like=name, temp=temp):
-            self.drop_view(
-                name=name,
-                database=database,
-                catalog=catalog,
-                temp=temp,
-                force=True,
-            )
-
-        if isinstance(obj, pd.DataFrame):
-            qualified_name = sg.table(
-                name, db=database, catalog=catalog, quoted=self.compiler.quoted
-            ).sql(self.name)
-            if schema:
-                table = self._table_env.from_pandas(
-                    obj, FlinkRowSchema.from_ibis(schema)
-                )
-            else:
-                table = self._table_env.from_pandas(obj)
-            # Note (mehmet): We use `create_temporary_view` here instead of `register_table`
-            # as suggested in PyFlink source code due to the deprecation of `register_table`.
-            self._table_env.create_temporary_view(qualified_name, table)
-
-        elif isinstance(obj, ir.Table):
-            query_expression = self.compile(obj)
-            stmt = sge.Create(
-                kind="VIEW",
-                this=sg.table(
-                    name, db=database, catalog=catalog, quoted=self.compiler.quoted
-                ),
-                expression=query_expression,
-                exists=force,
-                properties=sge.Properties(expressions=[sge.TemporaryProperty()])
-                if temp
-                else None,
-            )
-            self.raw_sql(stmt.sql(self.name))
-
-        else:
-            raise exc.IbisError(f"Unsupported `obj` type: {type(obj)}")
-
-        return self.table(name=name, database=database, catalog=catalog)
-
-    def drop_view(
-        self,
-        name: str,
-        *,
-        database: str | None = None,
-        catalog: str | None = None,
-        temp: bool = False,
-        force: bool = False,
-    ) -> None:
-        """Drop a view.
-
-        Parameters
-        ----------
-        name
-            Name of the view to drop.
-        database
-            Name of the database where the view exists, if not the default.
-        catalog
-            Name of the catalog where the view exists, if not the default.
-        temp
-            Whether the view is temporary or not.
-        force
-            If `False`, an exception is raised if the view does not exist.
-
-        """
-        # TODO(deepyaman): Support (and differentiate) permanent views.
-
-        statement = DropView(
-            name=name,
-            database=database,
-            catalog=catalog,
-            must_exist=(not force),
-            temporary=temp,
-        )
-        sql = statement.compile()
-        self.raw_sql(sql)
+        self.con.set_database(db_name)
 
-    def _read_file(
-        self,
-        file_type: str,
-        path: str | Path,
-        schema: sch.Schema | None = None,
-        table_name: str | None = None,
-    ) -> ir.Table:
-        """Register a file as a table in the current database.
-
-        Parameters
-        ----------
-        file_type
-            File type, e.g., parquet, csv, json.
-        path
-            The data source.
-        schema
-            The schema for the new table.
-        table_name
-            An optional name to use for the created table. This defaults to
-            a sequentially generated name.
-
-        Returns
-        -------
-        ir.Table
-            The just-registered table
-
-        Raises
-        ------
-        ValueError
-            If `schema` is None.
-
-        """
-        if schema is None:
-            raise ValueError(
-                f"`schema` must be explicitly provided when calling `read_{file_type}`"
-            )
-
-        table_name = table_name or gen_name(f"read_{file_type}")
-        tbl_properties = {
-            "connector": "filesystem",
-            "path": path,
-            "format": file_type,
-        }
+        # it still works!
+        table.limit(10).execute()
 
-        return self.create_table(
-            name=table_name,
-            schema=schema,
-            tbl_properties=tbl_properties,
-        )
+    def test_create_exists_drop_database(self):
+        tmp_name = '__ibis_test_{0}'.format(util.guid())
 
-    def read_parquet(
-        self,
-        path: str | Path,
-        schema: sch.Schema | None = None,
-        table_name: str | None = None,
-    ) -> ir.Table:
-        """Register a parquet file as a table in the current database.
-
-        Parameters
-        ----------
-        path
-            The data source.
-        schema
-            The schema for the new table.
-        table_name
-            An optional name to use for the created table. This defaults to
-            a sequentially generated name.
-
-        Returns
-        -------
-        ir.Table
-            The just-registered table
-
-        """
-        return self._read_file(
-            file_type="parquet", path=path, schema=schema, table_name=table_name
-        )
+        assert not self.con.exists_database(tmp_name)
 
-    def read_csv(
-        self,
-        path: str | Path,
-        schema: sch.Schema | None = None,
-        table_name: str | None = None,
-    ) -> ir.Table:
-        """Register a csv file as a table in the current database.
-
-        Parameters
-        ----------
-        path
-            The data source.
-        schema
-            The schema for the new table.
-        table_name
-            An optional name to use for the created table. This defaults to
-            a sequentially generated name.
-
-        Returns
-        -------
-        ir.Table
-            The just-registered table
-
-        """
-        return self._read_file(
-            file_type="csv", path=path, schema=schema, table_name=table_name
-        )
+        self.con.create_database(tmp_name)
+        assert self.con.exists_database(tmp_name)
 
-    def read_json(
-        self,
-        path: str | Path,
-        schema: sch.Schema | None = None,
-        table_name: str | None = None,
-    ) -> ir.Table:
-        """Register a json file as a table in the current database.
-
-        Parameters
-        ----------
-        path
-            The data source.
-        schema
-            The schema for the new table.
-        table_name
-            An optional name to use for the created table. This defaults to
-            a sequentially generated name.
-
-        Returns
-        -------
-        ir.Table
-            The just-registered table
-
-        """
-        return self._read_file(
-            file_type="json", path=path, schema=schema, table_name=table_name
-        )
+        self.con.drop_database(tmp_name)
+        assert not self.con.exists_database(tmp_name)
 
-    def insert(
-        self,
-        table_name: str,
-        obj: pa.Table | pd.DataFrame | ir.Table | list | dict,
-        database: str | None = None,
-        catalog: str | None = None,
-        overwrite: bool = False,
-    ) -> TableResult:
-        """Insert data into a table.
-
-        Parameters
-        ----------
-        table_name
-            The name of the table to insert data into.
-        obj
-            The source data or expression to insert.
-        database
-            Name of the attached database that the table is located in.
-        catalog
-            Name of the attached catalog that the table is located in.
-        overwrite
-            If `True` then replace existing contents of table.
-
-        Returns
-        -------
-        TableResult
-            The table result.
-
-        Raises
-        ------
-        ValueError
-            If the type of `obj` isn't supported
-
-        """
-        import pandas as pd
-        import pyarrow as pa
-        import pyarrow_hotfix  # noqa: F401
-
-        if isinstance(obj, ir.Table):
-            statement = InsertSelect(
-                table_name,
-                self.compile(obj),
-                database=database,
-                catalog=catalog,
-                overwrite=overwrite,
-            )
-            return self.raw_sql(statement.compile())
-
-        if isinstance(obj, pa.Table):
-            obj = obj.to_pandas()
-        if isinstance(obj, dict):
-            obj = pd.DataFrame.from_dict(obj)
-        if isinstance(obj, pd.DataFrame):
-            table = self._table_env.from_pandas(obj)
-            return table.execute_insert(table_name, overwrite=overwrite)
-
-        if isinstance(obj, list):
-            # pyflink infers datatypes, which may sometimes result in incompatible types
-            table = self._table_env.from_elements(obj)
-            return table.execute_insert(table_name, overwrite=overwrite)
-
-        raise ValueError(
-            "No operation is being performed. Either the obj parameter "
-            "is not a pandas DataFrame or is not a ibis Table."
-            f"The given obj is of type {type(obj).__name__} ."
-        )
+    def test_exists_table(self):
+        assert self.con.exists_table('functional_alltypes')
+        assert not self.con.exists_table(util.guid())
 
-    def to_pyarrow(
-        self,
-        expr: ir.Expr,
-        *,
-        params: Mapping[ir.Scalar, Any] | None = None,
-        limit: int | str | None = None,
-        **kwargs: Any,
-    ) -> pa.Table:
-        import pyarrow as pa
-        import pyarrow_hotfix  # noqa: F401
+    def text_exists_table_with_database(self):
+        table_name = _random_table_name()
+        tmp_db = self.test_data_db
+        self.con.create_table(table_name, self.alltypes, database=tmp_db)
 
-        pyarrow_batches = iter(
-            self.to_pyarrow_batches(expr, params=params, limit=limit, **kwargs)
-        )
+        assert self.con.exists_table(table_name, database=tmp_db)
+
+        tmp_name = '__ibis_test_{0}'.format(util.guid())
+        self.con.create_database(tmp_name)
+        self.temp_databases.append(tmp_name)
+        assert not self.con.exists_table(table_name, database=tmp_name)
+
+    def test_create_exists_drop_view(self):
+        tmp_name = util.guid()
+
+        assert not self.con.exists_table(tmp_name)
+
+        expr = (self.con.table('functional_alltypes')
+                .group_by('string_col')
+                .size())
 
-        first_batch = next(pyarrow_batches, None)
+        self.con.create_view(tmp_name, expr)
+        self.temp_views.append(tmp_name)
+        assert self.con.exists_table(tmp_name)
 
-        if first_batch is None:
-            pa_table = expr.as_table().schema().to_pyarrow().empty_table()
-        else:
-            pa_table = pa.Table.from_batches(
-                itertools.chain((first_batch,), pyarrow_batches)
-            )
-        return expr.__pyarrow_result__(pa_table)
-
-    def to_pyarrow_batches(
-        self,
-        expr: ir.Table,
-        *,
-        params: Mapping[ir.Scalar, Any] | None = None,
-        chunk_size: int | None = None,
-        limit: int | str | None = None,
-        **kwargs: Any,
-    ):
-        import pyarrow as pa
-        import pyarrow_hotfix  # noqa: F401
-
-        ibis_table = expr.as_table()
-
-        if params is None and limit is None:
-            # Note (mehmet): `_from_pyflink_table_to_pyarrow_batches()` does not
-            # support args `params` and `limit`.
-            pyflink_table = self._from_ibis_table_to_pyflink_table(ibis_table)
-            if pyflink_table:
-                # Note (mehmet): `_from_pyflink_table_to_pyarrow_batches()` supports
-                # only expressions that are registered as tables in Flink.
-                return self._from_pyflink_table_to_pyarrow_batches(
-                    table=pyflink_table,
-                    chunk_size=chunk_size,
-                )
-
-        # Note (mehmet): In the following, the entire result is fetched
-        # into a dataframe before converting it to arrow batches.
-        df = self.execute(ibis_table, limit=limit, **kwargs)
-        # TODO (mehmet): `limit` is discarded in `execute()`. Is this intentional?
-        df = df.head(limit)
-
-        ibis_schema = ibis_table.schema()
-        arrow_schema = ibis_schema.to_pyarrow()
-        arrow_table = pa.Table.from_pandas(df, schema=arrow_schema)
-        return arrow_table.to_reader()
+        # just check it works for now
+        expr2 = self.con.table(tmp_name)
+        expr2.execute()
+
+        self.con.drop_view(tmp_name)
+        assert not self.con.exists_table(tmp_name)
+
+    def test_drop_non_empty_database(self):
+        tmp_db = '__ibis_test_{0}'.format(util.guid())
+
+        self.con.create_database(tmp_db)
+
+        self.con.create_table(util.guid(), self.alltypes, database=tmp_db)
+
+        # Has a view, too
+        self.con.create_view(util.guid(), self.alltypes,
+                             database=tmp_db)
+
+        self.assertRaises(com.IntegrityError, self.con.drop_database, tmp_db)
+
+        self.con.drop_database(tmp_db, force=True)
+        assert not self.con.exists_database(tmp_db)
+
+    def test_create_database_with_location(self):
+        base = pjoin(self.tmp_dir, util.guid())
+        name = '__ibis_test_{0}'.format(util.guid())
+        tmp_path = pjoin(base, name)
+
+        self.con.create_database(name, path=tmp_path)
+        assert self.hdfs.exists(base)
+        self.con.drop_database(name)
+        self.hdfs.rmdir(base)
+
+    @pytest.mark.superuser
+    def test_create_table_with_location(self):
+        base = pjoin(self.tmp_dir, util.guid())
+        name = 'test_{0}'.format(util.guid())
+        tmp_path = pjoin(base, name)
+
+        # impala user has trouble writing to jenkins-owned dir so here we give
+        # the tmp dir 777
+        superuser_hdfs = ibis.hdfs_connect(host=ENV.nn_host,
+                                           port=ENV.webhdfs_port,
+                                           auth_mechanism=ENV.auth_mechanism,
+                                           verify=(ENV.auth_mechanism
+                                                   not in ['GSSAPI', 'LDAP']),
+                                           user=ENV.hdfs_superuser)
+        superuser_hdfs.mkdir(base)
+        superuser_hdfs.chmod(base, '777')
+
+        expr = self.alltypes
+        table_name = _random_table_name()
+
+        self.con.create_table(table_name, obj=expr, location=tmp_path,
+                              database=self.test_data_db)
+        self.temp_tables.append('.'.join([self.test_data_db, table_name]))
+        assert self.hdfs.exists(tmp_path)
+
+    def test_drop_table_not_exist(self):
+        random_name = util.guid()
+        self.assertRaises(Exception, self.con.drop_table, random_name)
+
+        self.con.drop_table(random_name, force=True)
+
+    def test_truncate_table(self):
+        expr = self.alltypes.limit(50)
+
+        table_name = util.guid()
+        self.con.create_table(table_name, obj=expr)
+        self.temp_tables.append(table_name)
 
-    def _from_ibis_table_to_pyflink_table(self, table: ir.Table) -> Table | None:
         try:
-            table_name = table.get_name()
-        except AttributeError:
-            # `table` is not a registered table in Flink.
-            return None
+            self.con.truncate_table(table_name)
+        except HS2Error as e:
+            if 'AnalysisException' in e.args[0]:
+                pytest.skip('TRUNCATE not available in this '
+                            'version of Impala')
+
+        result = self.con.table(table_name).execute()
+        assert len(result) == 0
+
+    def test_ctas_from_table_expr(self):
+        expr = self.alltypes
+        table_name = _random_table_name()
+        db = self.test_data_db
+
+        self.con.create_table(table_name, expr, database=db)
+        self.temp_tables.append('.'.join((db, table_name)))
+
+    def test_create_empty_table(self):
+        schema = ibis.schema([('a', 'string'),
+                              ('b', 'timestamp'),
+                              ('c', 'decimal(12,8)'),
+                              ('d', 'double')])
+
+        table_name = util.guid()
+        self.con.create_table(table_name, schema=schema)
+        self.temp_tables.append(table_name)
+
+        result_schema = self.con.get_schema(table_name)
+        assert_equal(result_schema, schema)
+
+        assert len(self.con.table(table_name).execute()) == 0
+
+    def test_insert_table(self):
+        expr = self.alltypes
+        table_name = _random_table_name()
+        db = self.test_data_db
+
+        self.con.create_table(table_name, expr.limit(0), database=db)
+        self.temp_tables.append('.'.join((db, table_name)))
+
+        self.con.insert(table_name, expr.limit(10), database=db)
+
+        # check using ImpalaTable.insert
+        t = self.con.table(table_name, database=db)
+        t.insert(expr.limit(10))
+
+        sz = t.count()
+        assert sz.execute() == 20
+
+        # Overwrite and verify only 10 rows now
+        t.insert(expr.limit(10), overwrite=True)
+        assert sz.execute() == 10
+
+    def test_insert_validate_types(self):
+        # GH #235
+        table_name = _random_table_name()
+        db = self.test_data_db
+
+        expr = self.alltypes
+        self.con.create_table(table_name,
+                              schema=expr['tinyint_col', 'int_col',
+                                          'string_col'].schema(),
+                              database=db)
+        self.temp_tables.append('.'.join((db, table_name)))
+
+        t = self.con.table(table_name, database=db)
+
+        to_insert = expr[expr.tinyint_col, expr.smallint_col.name('int_col'),
+                         expr.string_col]
+        t.insert(to_insert.limit(10))
+
+        to_insert = expr[expr.tinyint_col,
+                         expr.smallint_col.cast('int32').name('int_col'),
+                         expr.string_col]
+        t.insert(to_insert.limit(10))
+
+        to_insert = expr[expr.tinyint_col,
+                         expr.bigint_col.name('int_col'),
+                         expr.string_col]
+        with self.assertRaises(com.IbisError):
+            t.insert(to_insert.limit(10))
+
+    def test_compute_stats(self):
+        t = self.con.table('functional_alltypes')
+
+        t.compute_stats()
+        t.compute_stats(incremental=True)
+
+        self.con.compute_stats('functional_alltypes')
+
+    def test_invalidate_metadata(self):
+        with self._patch_execute() as ex_mock:
+            self.con.invalidate_metadata()
+            ex_mock.assert_called_with('INVALIDATE METADATA')
+
+        self.con.invalidate_metadata('functional_alltypes')
+        t = self.con.table('functional_alltypes')
+        t.invalidate_metadata()
+
+        with self._patch_execute() as ex_mock:
+            self.con.invalidate_metadata('functional_alltypes',
+                                         database=self.test_data_db)
+            ex_mock.assert_called_with('INVALIDATE METADATA '
+                                       '{0}.`{1}`'
+                                       .format(self.test_data_db,
+                                               'functional_alltypes'))
+
+    def test_refresh(self):
+        tname = 'functional_alltypes'
+        with self._patch_execute() as ex_mock:
+            self.con.refresh(tname)
+            ex_cmd = 'REFRESH {0}.`{1}`'.format(self.test_data_db,
+                                                tname)
+            ex_mock.assert_called_with(ex_cmd)
+
+        t = self.con.table(tname)
+        with self._patch_execute() as ex_mock:
+            t.refresh()
+            ex_cmd = 'REFRESH {0}.`{1}`'.format(self.test_data_db,
+                                                tname)
+            ex_mock.assert_called_with(ex_cmd)
+
+    def _patch_execute(self):
+        return mock.patch.object(self.con, '_execute',
+                                 wraps=self.con._execute)
+
+    def test_describe_formatted(self):
+        from ibis.impala.metadata import TableMetadata
+
+        t = self.con.table('functional_alltypes')
+        with self._patch_execute() as ex_mock:
+            desc = t.describe_formatted()
+            ex_mock.assert_called_with('DESCRIBE FORMATTED '
+                                       '{0}.`{1}`'
+                                       .format(self.test_data_db,
+                                               'functional_alltypes'),
+                                       results=True)
+            assert isinstance(desc, TableMetadata)
+
+    def test_show_files(self):
+        t = self.con.table('functional_alltypes')
+        qualified_name = '{0}.`{1}`'.format(self.test_data_db,
+                                            'functional_alltypes')
+        with self._patch_execute() as ex_mock:
+            desc = t.files()
+            ex_mock.assert_called_with('SHOW FILES IN {0}'
+                                       .format(qualified_name),
+                                       results=True)
+            assert isinstance(desc, pd.DataFrame)
+
+    def test_table_column_stats(self):
+        t = self.con.table('functional_alltypes')
+
+        qualified_name = '{0}.`{1}`'.format(self.test_data_db,
+                                            'functional_alltypes')
+        with self._patch_execute() as ex_mock:
+            desc = t.stats()
+            ex_mock.assert_called_with('SHOW TABLE STATS {0}'
+                                       .format(qualified_name),
+                                       results=True)
+            assert isinstance(desc, pd.DataFrame)
+
+        with self._patch_execute() as ex_mock:
+            desc = t.column_stats()
+            ex_mock.assert_called_with('SHOW COLUMN STATS {0}'
+                                       .format(qualified_name),
+                                       results=True)
+            assert isinstance(desc, pd.DataFrame)
+
+    def test_drop_table_or_view(self):
+        t = self.db.functional_alltypes
+
+        tname = util.guid()
+        self.con.create_table(tname, t.limit(10))
+        self.temp_tables.append(tname)
+
+        vname = util.guid()
+        self.con.create_view(vname, t.limit(10))
+        self.temp_views.append(vname)
+
+        t2 = self.db[tname]
+        t2.drop()
+        assert tname not in self.db
+
+        t3 = self.db[vname]
+        t3.drop()
+        assert vname not in self.db
+
+    def test_rename_table(self):
+        tmp_db = '__ibis_tmp_{0}'.format(util.guid()[:4])
+        self.con.create_database(tmp_db)
+        self.temp_databases.append(tmp_db)
+
+        orig_name = 'tmp_rename_test'
+        self.con.create_table(orig_name,
+                              self.con.table('tpch_region'))
+        table = self.con.table(orig_name)
+
+        old_name = table.name
+
+        new_name = 'rename_test'
+        renamed = table.rename(new_name, database=tmp_db)
+        renamed.execute()
+
+        t = self.con.table(new_name, database=tmp_db)
+        assert_equal(renamed, t)
+
+        assert table.name == old_name
+
+    def test_change_location(self):
+        old_loc = self.table.metadata().location
+
+        new_path = pjoin(self.tmp_dir, 'new-path')
+        self.table.alter(location=new_path)
+
+        new_loc = self.table.metadata().location
+        assert new_loc == old_loc.replace(self.path_uuid, 'new-path')
+
+    def test_change_properties(self):
+        props = {'foo': '1', 'bar': '2'}
+
+        self.table.alter(tbl_properties=props)
+        tbl_props = self.table.metadata().tbl_properties
+        for k, v in props.iteritems():
+            assert v == tbl_props[k]
+
+        self.table.alter(serde_properties=props)
+        serde_props = self.table.metadata().serde_properties
+        for k, v in props.iteritems():
+            assert v == serde_props[k]
+
+    def test_change_format(self):
+        self.table.alter(format='avro')
+
+        meta = self.table.metadata()
+        assert 'Avro' in meta.hive_format
+
+    def test_cleanup_tmp_table_on_gc(self):
+        hdfs_path = pjoin(self.test_data_dir, 'parquet/tpch_region')
+        table = self.con.parquet_file(hdfs_path)
+        name = table.op().name
+        table = None
+        gc.collect()
+        _assert_table_not_exists(self.con, name)
+
+    def test_persist_parquet_file_with_name(self):
+        hdfs_path = pjoin(self.test_data_dir, 'parquet/tpch_region')
+
+        name = _random_table_name()
+        schema = ibis.schema([('r_regionkey', 'int16'),
+                              ('r_name', 'string'),
+                              ('r_comment', 'string')])
+        self.con.parquet_file(hdfs_path, schema=schema,
+                              name=name,
+                              database=self.tmp_db,
+                              persist=True)
+        gc.collect()
+
+        # table still exists
+        self.con.table(name, database=self.tmp_db)
+
+        _ensure_drop(self.con, name, database=self.tmp_db)
+
+    def test_query_avro(self):
+        hdfs_path = pjoin(self.test_data_dir, 'avro/tpch_region_avro')
+
+        avro_schema = {
+            "fields": [
+                {"type": ["int", "null"], "name": "R_REGIONKEY"},
+                {"type": ["string", "null"], "name": "R_NAME"},
+                {"type": ["string", "null"], "name": "R_COMMENT"}],
+            "type": "record",
+            "name": "a"
+        }
 
-        qualified_name = sg.table(table_name, quoted=self.compiler.quoted).sql(
-            self.name
-        )
+        table = self.con.avro_file(hdfs_path, avro_schema,
+                                   database=self.tmp_db)
+
+        name = table.op().name
+        assert name.startswith('{0}.'.format(self.tmp_db))
+
+        # table exists
+        self.con.table(name)
+
+        expr = table.r_name.value_counts()
+        expr.execute()
+
+        assert table.count().execute() == 5
+
+        df = table.execute()
+        assert len(df) == 5
+
+    def test_query_parquet_file_with_schema(self):
+        hdfs_path = pjoin(self.test_data_dir, 'parquet/tpch_region')
+
+        schema = ibis.schema([('r_regionkey', 'int16'),
+                              ('r_name', 'string'),
+                              ('r_comment', 'string')])
+
+        table = self.con.parquet_file(hdfs_path, schema=schema)
+
+        name = table.op().name
+
+        # table exists
+        self.con.table(name)
+
+        expr = table.r_name.value_counts()
+        expr.execute()
+
+        assert table.count().execute() == 5
+
+    def test_query_parquet_file_like_table(self):
+        hdfs_path = pjoin(self.test_data_dir, 'parquet/tpch_region')
+
+        ex_schema = ibis.schema([('r_regionkey', 'int16'),
+                                 ('r_name', 'string'),
+                                 ('r_comment', 'string')])
+
+        table = self.con.parquet_file(hdfs_path, like_table='tpch_region')
+
+        assert_equal(table.schema(), ex_schema)
+
+    def test_query_parquet_infer_schema(self):
+        hdfs_path = pjoin(self.test_data_dir, 'parquet/tpch_region')
+        table = self.con.parquet_file(hdfs_path)
+
+        # NOTE: the actual schema should have an int16, but bc this is being
+        # inferred from a parquet file, which has no notion of int16, the
+        # inferred schema will have an int32 instead.
+        ex_schema = ibis.schema([('r_regionkey', 'int32'),
+                                 ('r_name', 'string'),
+                                 ('r_comment', 'string')])
+
+        assert_equal(table.schema(), ex_schema)
+
+    def test_create_table_persist_fails_if_called_twice(self):
+        tname = util.guid()
+
+        hdfs_path = pjoin(self.test_data_dir, 'parquet/tpch_region')
+        self.con.parquet_file(hdfs_path, name=tname, persist=True)
+        self.temp_tables.append(tname)
+
+        with self.assertRaises(HS2Error):
+            self.con.parquet_file(hdfs_path, name=tname, persist=True)
+
+    def test_create_table_reserved_identifier(self):
+        table_name = 'distinct'
+        expr = self.con.table('functional_alltypes')
+        self.con.create_table(table_name, expr)
+        self.temp_tables.append(table_name)
+
+        t = self.con.table(table_name)
+        t.limit(10).execute()
+
+    def test_query_text_file_regex(self):
+        pass
+
+    def test_query_delimited_file_directory(self):
+        hdfs_path = pjoin(self.test_data_dir, 'csv')
+
+        schema = ibis.schema([('foo', 'string'),
+                              ('bar', 'double'),
+                              ('baz', 'int8')])
+        name = 'delimited_table_test1'
+        table = self.con.delimited_file(hdfs_path, schema, name=name,
+                                        database=self.tmp_db,
+                                        delimiter=',')
         try:
-            return self._table_env.from_path(qualified_name)
-        except Py4JJavaError:
-            # `table` is not a registered table in Flink.
-            return None
-
-    def _from_pyflink_table_to_pyarrow_batches(
-        self,
-        table: Table,
-        *,
-        chunk_size: int | None = None,
-    ):
-        import pyarrow as pa
-        import pyarrow_hotfix  # noqa: F401
-        import pytz
-        from pyflink.java_gateway import get_gateway
-        from pyflink.table.serializers import ArrowSerializer
-        from pyflink.table.types import create_arrow_schema
-
-        from ibis.backends.flink.datatypes import get_field_data_types
-        # Note (mehmet): Implementation of this is based on
-        # pyflink/table/table.py: to_pandas().
-
-        gateway = get_gateway()
-        if chunk_size:
-            max_arrow_batch_size = chunk_size
-        else:
-            max_arrow_batch_size = (
-                table._j_table.getTableEnvironment()
-                .getConfig()
-                .get(
-                    gateway.jvm.org.apache.flink.python.PythonOptions.MAX_ARROW_BATCH_SIZE
-                )
-            )
-        batches_iterator = gateway.jvm.org.apache.flink.table.runtime.arrow.ArrowUtils.collectAsPandasDataFrame(
-            table._j_table, max_arrow_batch_size
-        )
+            expr = (table
+                    [table.bar > 0]
+                    .group_by('foo')
+                    .aggregate([table.bar.sum().name('sum(bar)'),
+                                table.baz.sum().name('mean(baz)')]))
+            expr.execute()
+        finally:
+            self.con.drop_table(name, database=self.tmp_db)
 
-        pyflink_schema = table.get_schema()
-        arrow_schema = create_arrow_schema(
-            pyflink_schema.get_field_names(), get_field_data_types(pyflink_schema)
-        )
+    def test_temp_table_concurrency(self):
+        pytest.skip('Cannot get this test to run under pytest')
 
-        timezone = pytz.timezone(
-            table._j_table.getTableEnvironment().getConfig().getLocalTimeZone().getId()
-        )
-        serializer = ArrowSerializer(
-            arrow_schema, pyflink_schema.to_row_data_type(), timezone
-        )
+        from threading import Thread, Lock
+        import gc
+        nthreads = 4
 
-        return pa.ipc.RecordBatchReader.from_batches(
-            arrow_schema, serializer.load_from_iterator(batches_iterator)
-        )
+        hdfs_path = pjoin(self.test_data_dir, 'parquet/tpch_region')
+
+        lock = Lock()
+
+        results = []
+
+        def do_something():
+            t = self.con.parquet_file(hdfs_path)
+
+            with lock:
+                t.limit(10).execute()
+                t = None
+                gc.collect()
+                results.append(True)
+
+        threads = []
+        for i in range(nthreads):
+            t = Thread(target=do_something)
+            t.start()
+            threads.append(t)
+
+        [x.join() for x in threads]
+
+        assert results == [True] * nthreads
+
+
+def _create_table(table_name, expr, database=None, can_exist=False,
+                  format='parquet'):
+    ast = build_ast(expr)
+    select = ast.queries[0]
+    statement = ddl.CTAS(table_name, select,
+                         database=database,
+                         format=format,
+                         can_exist=can_exist)
+    return statement
+
+
+def _get_select(expr):
+    ast = build_ast(expr)
+    select = ast.queries[0]
+    context = ast.context
+
+    return select, context
+
+
+def _random_table_name():
+    table_name = '__ibis_test_' + util.guid()
+    return table_name
+
+
+def _assert_table_not_exists(con, table_name, database=None):
+    if database is not None:
+        tname = '.'.join((database, table_name))
+    else:
+        tname = table_name
+
+    try:
+        con.table(tname)
+    except ImpylaError:
+        pass
+    except:
+        raise
+
+
+def _ensure_drop(con, table_name, database=None):
+    con.drop_table(table_name, database=database, force=True)
+    _assert_table_not_exists(con, table_name, database=database)
```

### Comparing `ibis_framework-9.0.0.dev686/ibis/backends/flink/compiler.py` & `ibis-framework-v0.6.0/ibis/sql/alchemy.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,570 +1,875 @@
-"""Flink Ibis expression to SQL compiler."""
+# Copyright 2015 Cloudera Inc.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+# http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import operator
+import six
+
+import sqlalchemy as sa
+import sqlalchemy.sql as sql
+
+from ibis.client import SQLClient, AsyncQuery, Query
+from ibis.sql.compiler import Select, Union, TableSetFormatter
+import ibis.common as com
+import ibis.expr.datatypes as dt
+import ibis.expr.operations as ops
+import ibis.expr.types as ir
+import ibis.sql.compiler as comp
+import ibis.sql.transforms as transforms
+import ibis.util as util
+import ibis
+
+
+_ibis_type_to_sqla = {
+    dt.Int8: sa.types.SmallInteger,
+    dt.Int16: sa.types.SmallInteger,
+    dt.Int32: sa.types.Integer,
+    dt.Int64: sa.types.BigInteger,
+
+    # Mantissa-based
+    dt.Float: sa.types.Float(precision=24),
+    dt.Double: sa.types.Float(precision=53),
+
+    dt.Boolean: sa.types.Boolean,
+
+    dt.String: sa.types.String,
+
+    dt.Timestamp: sa.types.DateTime,
+
+    dt.Decimal: sa.types.NUMERIC,
+}
+
+_sqla_type_mapping = {
+    sa.types.SmallInteger: dt.Int16,
+    sa.types.INTEGER: dt.Int64,
+    sa.types.BOOLEAN: dt.Boolean,
+    sa.types.BIGINT: dt.Int64,
+    sa.types.FLOAT: dt.Double,
+    sa.types.REAL: dt.Double,
+
+    sa.types.TEXT: dt.String,
+    sa.types.NullType: dt.String,
+    sa.types.Text: dt.String,
+}
+
+_sqla_type_to_ibis = dict((v, k) for k, v in
+                          _ibis_type_to_sqla.items())
+_sqla_type_to_ibis.update(_sqla_type_mapping)
+
+
+def schema_from_table(table):
+    # Convert SQLA table to Ibis schema
+    names = table.columns.keys()
+
+    types = []
+    for c in table.columns.values():
+        type_class = type(c.type)
+
+        if isinstance(c.type, sa.types.NUMERIC):
+            t = dt.Decimal(c.type.precision,
+                           c.type.scale,
+                           nullable=c.nullable)
+        else:
+            if c.type in _sqla_type_to_ibis:
+                ibis_class = _sqla_type_to_ibis[c.type]
+            elif type_class in _sqla_type_to_ibis:
+                ibis_class = _sqla_type_to_ibis[type_class]
+            else:
+                raise NotImplementedError(c.type)
+            t = ibis_class(c.nullable)
 
-from __future__ import annotations
+        types.append(t)
 
-import sqlglot as sg
-import sqlglot.expressions as sge
+    return dt.Schema(names, types)
 
-import ibis.common.exceptions as com
-import ibis.expr.datatypes as dt
-import ibis.expr.operations as ops
-from ibis.backends.sql.compiler import NULL, STAR, SQLGlotCompiler
-from ibis.backends.sql.datatypes import FlinkType
-from ibis.backends.sql.dialects import Flink
-from ibis.backends.sql.rewrites import (
-    exclude_unsupported_window_frame_from_ops,
-    exclude_unsupported_window_frame_from_rank,
-    exclude_unsupported_window_frame_from_row_number,
-    rewrite_sample_as_filter,
-)
-from ibis.expr.rewrites import rewrite_stringslice
-
-
-class FlinkCompiler(SQLGlotCompiler):
-    quoted = True
-    dialect = Flink
-    type_mapper = FlinkType
-    rewrites = (
-        rewrite_sample_as_filter,
-        exclude_unsupported_window_frame_from_row_number,
-        exclude_unsupported_window_frame_from_ops,
-        exclude_unsupported_window_frame_from_rank,
-        rewrite_stringslice,
-        *SQLGlotCompiler.rewrites,
-    )
-
-    UNSUPPORTED_OPERATIONS = frozenset(
-        (
-            ops.AnalyticVectorizedUDF,
-            ops.ApproxMedian,
-            ops.ArgMax,
-            ops.ArgMin,
-            ops.ArrayCollect,
-            ops.ArrayFlatten,
-            ops.ArraySort,
-            ops.ArrayStringJoin,
-            ops.Correlation,
-            ops.CountDistinctStar,
-            ops.Covariance,
-            ops.DateDiff,
-            ops.ExtractURLField,
-            ops.FindInSet,
-            ops.IsInf,
-            ops.IsNan,
-            ops.Levenshtein,
-            ops.Median,
-            ops.MultiQuantile,
-            ops.NthValue,
-            ops.Quantile,
-            ops.ReductionVectorizedUDF,
-            ops.RegexSplit,
-            ops.RowID,
-            ops.StringSplit,
-            ops.Translate,
-        )
-    )
-
-    SIMPLE_OPS = {
-        ops.All: "min",
-        ops.Any: "max",
-        ops.ApproxCountDistinct: "approx_count_distinct",
-        ops.ArrayDistinct: "array_distinct",
-        ops.ArrayLength: "cardinality",
-        ops.ArrayPosition: "array_position",
-        ops.ArrayRemove: "array_remove",
-        ops.ArrayUnion: "array_union",
-        ops.ExtractDayOfYear: "dayofyear",
-        ops.First: "first_value",
-        ops.Last: "last_value",
-        ops.MapKeys: "map_keys",
-        ops.MapValues: "map_values",
-        ops.Power: "power",
-        ops.RegexSearch: "regexp",
-        ops.StrRight: "right",
-        ops.StringLength: "char_length",
-        ops.StringToDate: "to_date",
-        ops.StringToTimestamp: "to_timestamp",
-        ops.Strip: "trim",
-        ops.TypeOf: "typeof",
-    }
+
+def table_from_schema(name, meta, schema):
+    # Convert Ibis schema to SQLA table
+    sqla_cols = []
+
+    for cname, itype in zip(schema.names, schema.types):
+        ctype = _to_sqla_type(itype)
+
+        col = sa.Column(cname, ctype, nullable=itype.nullable)
+        sqla_cols.append(col)
+
+    return sa.Table(name, meta, *sqla_cols)
+
+
+def _to_sqla_type(itype):
+    if isinstance(itype, dt.Decimal):
+        return sa.types.NUMERIC(itype.precision, itype.scale)
+    else:
+        return _ibis_type_to_sqla[type(itype)]
+
+
+def fixed_arity(sa_func, arity):
+    if isinstance(sa_func, six.string_types):
+        sa_func = getattr(sa.func, sa_func)
+
+    def formatter(t, expr):
+        if arity != len(expr.op().args):
+            raise com.IbisError('incorrect number of args')
+
+        return _varargs_call(sa_func, t, expr)
+
+    return formatter
+
+
+def varargs(sa_func):
+    def formatter(t, expr):
+        op = expr.op()
+        trans_args = [t.translate(arg) for arg in op.args]
+        return sa_func(*trans_args)
+    return formatter
+
+
+def _varargs_call(sa_func, t, expr):
+    op = expr.op()
+    trans_args = [t.translate(arg) for arg in op.args]
+    return sa_func(*trans_args)
+
+
+def _table_column(t, expr):
+    op = expr.op()
+    ctx = t.context
+    table = op.table
+
+    sa_table = _get_sqla_table(ctx, table)
+    out_expr = getattr(sa_table.c, op.name)
+
+    # If the column does not originate from the table set in the current SELECT
+    # context, we should format as a subquery
+    if t.permit_subquery and ctx.is_foreign_expr(table):
+        return sa.select([out_expr])
+
+    return out_expr
+
+
+def _get_sqla_table(ctx, table):
+    if ctx.has_ref(table):
+        ctx_level = ctx
+        sa_table = ctx_level.get_table(table)
+        while sa_table is None and ctx_level.parent is not ctx_level:
+            ctx_level = ctx_level.parent
+            sa_table = ctx_level.get_table(table)
+    else:
+        sa_table = table.op().sqla_table
+
+    return sa_table
+
+
+def _table_array_view(t, expr):
+    ctx = t.context
+    table = ctx.get_compiled_expr(expr.op().table)
+    return table
+
+
+def _exists_subquery(t, expr):
+    op = expr.op()
+    ctx = t.context
+
+    filtered = (op.foreign_table.filter(op.predicates)
+                .projection([ir.literal(1).name(ir.unnamed)]))
+
+    sub_ctx = ctx.subcontext()
+    clause = to_sqlalchemy(filtered, context=sub_ctx, exists=True)
+
+    if isinstance(op, transforms.NotExistsSubquery):
+        clause = sa.not_(clause)
+
+    return clause
+
+
+def _cast(t, expr):
+    op = expr.op()
+    arg, target_type = op.args
+    sa_arg = t.translate(arg)
+    sa_type = t.get_sqla_type(target_type)
+
+    if isinstance(arg, ir.CategoryValue) and target_type == 'int32':
+        return sa_arg
+    else:
+        return sa.cast(sa_arg, sa_type)
+
+
+def _contains(t, expr):
+    op = expr.op()
+
+    left, right = [t.translate(arg) for arg in op.args]
+    return left.in_(right)
+
+
+def _reduction(sa_func):
+    def formatter(t, expr):
+        op = expr.op()
+
+        # HACK: support trailing arguments
+        arg, where = op.args[:2]
+
+        return _reduction_format(t, sa_func, arg, where)
+    return formatter
+
+
+def _reduction_format(t, sa_func, arg, where):
+    if where is not None:
+        case = where.ifelse(arg, ibis.NA)
+        arg = t.translate(case)
+    else:
+        arg = t.translate(arg)
+
+    return sa_func(arg)
+
+
+def _literal(t, expr):
+    return sa.literal(expr.op().value)
+
+
+def _value_list(t, expr):
+    return [t.translate(x) for x in expr.op().values]
+
+
+def _is_null(t, expr):
+    arg = t.translate(expr.op().args[0])
+    return arg.is_(sa.null())
+
+
+def _not_null(t, expr):
+    arg = t.translate(expr.op().args[0])
+    return arg.isnot(sa.null())
+
+
+def _round(t, expr):
+    op = expr.op()
+    arg, digits = op.args
+    sa_arg = t.translate(arg)
+
+    f = sa.func.round
+
+    if digits is not None:
+        sa_digits = t.translate(digits)
+        return f(sa_arg, sa_digits)
+    else:
+        return f(sa_arg)
+
+
+def _count_distinct(t, expr):
+    arg, = expr.op().args
+    sa_arg = t.translate(arg)
+    return sa.func.count(sa_arg.distinct())
+
+
+def _simple_case(t, expr):
+    op = expr.op()
+
+    cases = [op.base == case for case in op.cases]
+    return _translate_case(t, cases, op.results, op.default)
+
+
+def _searched_case(t, expr):
+    op = expr.op()
+    return _translate_case(t, op.cases, op.results, op.default)
+
+
+def _translate_case(t, cases, results, default):
+    case_args = [t.translate(arg) for arg in cases]
+    result_args = [t.translate(arg) for arg in results]
+
+    whens = zip(case_args, result_args)
+    default = t.translate(default)
+
+    return sa.case(whens, else_=default)
+
+
+def unary(sa_func):
+    return fixed_arity(sa_func, 1)
+
+
+_operation_registry = {
+    ops.And: fixed_arity(sql.and_, 2),
+    ops.Or: fixed_arity(sql.or_, 2),
+
+    ops.Abs: unary(sa.func.abs),
+
+    ops.Cast: _cast,
+
+    ops.Coalesce: varargs(sa.func.coalesce),
+
+    ops.NullIf: fixed_arity(sa.func.nullif, 2),
+
+    ops.Contains: _contains,
+
+    ops.Count: _reduction(sa.func.count),
+    ops.Sum: _reduction(sa.func.sum),
+    ops.Mean: _reduction(sa.func.avg),
+    ops.Min: _reduction(sa.func.min),
+    ops.Max: _reduction(sa.func.max),
+
+    ops.CountDistinct: _count_distinct,
+
+    ops.GroupConcat: fixed_arity(sa.func.group_concat, 2),
+
+    ops.Between: fixed_arity(sa.between, 3),
+
+    ops.IsNull: _is_null,
+    ops.NotNull: _not_null,
+    ops.Negate: unary(sa.not_),
+
+    ops.Round: _round,
+
+    ops.TypeOf: unary(sa.func.typeof),
+
+    ir.Literal: _literal,
+    ir.ValueList: _value_list,
+    ir.NullLiteral: lambda *args: sa.null(),
+
+    ops.SimpleCase: _simple_case,
+    ops.SearchedCase: _searched_case,
+
+    ops.TableColumn: _table_column,
+    ops.TableArrayView: _table_array_view,
+
+    transforms.ExistsSubquery: _exists_subquery,
+    transforms.NotExistsSubquery: _exists_subquery,
+}
+
+
+# TODO: unit tests for each of these
+_binary_ops = {
+    # Binary arithmetic
+    ops.Add: operator.add,
+    ops.Subtract: operator.sub,
+    ops.Multiply: operator.mul,
+    ops.Divide: operator.truediv,
+    ops.Power: operator.pow,
+    ops.Modulus: operator.mod,
+
+    # Comparisons
+    ops.Equals: operator.eq,
+    ops.NotEquals: operator.ne,
+    ops.Less: operator.lt,
+    ops.LessEqual: operator.le,
+    ops.Greater: operator.gt,
+    ops.GreaterEqual: operator.ge,
+
+    # Boolean comparisons
+
+    # TODO
+}
+
+for _k, _v in _binary_ops.items():
+    _operation_registry[_k] = fixed_arity(_v, 2)
+
+
+class AlchemySelectBuilder(comp.SelectBuilder):
 
     @property
-    def NAN(self):
-        raise NotImplementedError("Flink does not support NaN")
+    def _select_class(self):
+        return AlchemySelect
+
+    def _convert_group_by(self, exprs):
+        return exprs
+
+
+class AlchemyContext(comp.QueryContext):
+
+    def __init__(self, *args, **kwargs):
+        self._table_objects = {}
+        self.dialect = kwargs.pop('dialect', AlchemyDialect)
+        comp.QueryContext.__init__(self, *args, **kwargs)
+
+    def subcontext(self):
+        return type(self)(dialect=self.dialect, parent=self)
+
+    def _to_sql(self, expr, ctx):
+        return to_sqlalchemy(expr, context=ctx)
+
+    def _compile_subquery(self, expr):
+        sub_ctx = self.subcontext()
+        return self._to_sql(expr, sub_ctx)
+
+    def has_table(self, expr, parent_contexts=False):
+        key = self._get_table_key(expr)
+        return self._key_in(key, '_table_objects',
+                            parent_contexts=parent_contexts)
+
+    def set_table(self, expr, obj):
+        key = self._get_table_key(expr)
+        self._table_objects[key] = obj
+
+    def get_table(self, expr):
+        """
+        Get the memoized SQLAlchemy expression object
+        """
+        return self._get_table_item('_table_objects', expr)
+
+
+class AlchemyQueryBuilder(comp.QueryBuilder):
+
+    select_builder = AlchemySelectBuilder
+
+    def __init__(self, expr, context=None, dialect=None):
+        if dialect is None:
+            dialect = AlchemyDialect
+
+        self.dialect = dialect
+        comp.QueryBuilder.__init__(self, expr, context=context)
+
+    def _make_context(self):
+        return AlchemyContext(dialect=self.dialect)
 
     @property
-    def POS_INF(self):
-        raise NotImplementedError("Flink does not support Infinity")
+    def _union_class(self):
+        return AlchemyUnion
 
-    NEG_INF = POS_INF
 
-    @staticmethod
-    def _generate_groups(groups):
-        return groups
-
-    def _aggregate(self, funcname: str, *args, where):
-        func = self.f[funcname]
-        if where is not None:
-            # FILTER (WHERE ) is broken for one or both of:
-            #
-            # 1. certain aggregates: std/var doesn't return the right result
-            # 2. certain kinds of predicates: x IN y doesn't filter the right
-            #    values out
-            # 3. certain aggregates AND predicates STD(w) FILTER (WHERE x IN Y)
-            #    returns an incorrect result
-            #
-            # One solution is to try `IF(predicate, arg, NULL)`.
-            #
-            # Unfortunately that won't work without casting the NULL to a
-            # specific type.
-            #
-            # At this point in the Ibis compiler we don't have any of the Ibis
-            # operation's type information because we thrown it away. In every
-            # other engine Ibis supports the type of a NULL literal is inferred
-            # by the engine.
-            #
-            # Using a CASE statement and leaving out the explicit NULL does the
-            # trick for Flink.
-            #
-            # Le sigh.
-            args = tuple(sge.Case(ifs=[sge.If(this=where, true=arg)]) for arg in args)
-        return func(*args)
-
-    @staticmethod
-    def _minimize_spec(start, end, spec):
-        if (
-            start is None
-            and isinstance(getattr(end, "value", None), ops.Literal)
-            and end.value.value == 0
-            and end.following
-        ):
-            return None
-        elif (
-            isinstance(getattr(end, "value", None), ops.Cast)
-            and end.value.arg.value == 0
-            and end.following
-        ):
-            spec.args["end"] = "CURRENT ROW"
-            spec.args["end_side"] = None
-        return spec
-
-    def visit_TumbleWindowingTVF(self, op, *, table, time_col, window_size, offset):
-        args = [
-            self.v[f"TABLE {table.this.sql(self.dialect)}"],
-            # `time_col` has the table _alias_, instead of the table, but it is
-            # required to be bound to the table, this happens because of the
-            # way we construct the op in the tumble API using bind
-            #
-            # perhaps there's a better way to deal with this
-            self.f.descriptor(time_col.this),
-            window_size,
-            offset,
-        ]
-
-        return sg.select(
-            sge.Column(
-                this=STAR, table=sg.to_identifier(table.alias_or_name, quoted=True)
-            )
-        ).from_(
-            self.f.table(self.f.tumble(*filter(None, args))).as_(
-                table.alias_or_name, quoted=True
-            )
-        )
-
-    def visit_HopWindowingTVF(
-        self, op, *, table, time_col, window_size, window_slide, offset
-    ):
-        args = [
-            self.v[f"TABLE {table.this.sql(self.dialect)}"],
-            self.f.descriptor(time_col.this),
-            window_slide,
-            window_size,
-            offset,
-        ]
-        return sg.select(
-            sge.Column(
-                this=STAR, table=sg.to_identifier(table.alias_or_name, quoted=True)
-            )
-        ).from_(
-            self.f.table(self.f.hop(*filter(None, args))).as_(
-                table.alias_or_name, quoted=True
-            )
-        )
-
-    def visit_CumulateWindowingTVF(
-        self, op, *, table, time_col, window_size, window_step, offset
-    ):
-        args = [
-            self.v[f"TABLE {table.this.sql(self.dialect)}"],
-            self.f.descriptor(time_col.this),
-            window_step,
-            window_size,
-            offset,
-        ]
-        return sg.select(
-            sge.Column(
-                this=STAR, table=sg.to_identifier(table.alias_or_name, quoted=True)
-            )
-        ).from_(
-            self.f.table(self.f.cumulate(*filter(None, args))).as_(
-                table.alias_or_name, quoted=True
-            )
-        )
-
-    def visit_InMemoryTable(self, op, *, name, schema, data):
-        # the performance of this is rather terrible
-        tuples = data.to_frame().itertuples(index=False)
-        quoted = self.quoted
-        columns = [sg.column(col, quoted=quoted) for col in schema.names]
-        alias = sge.TableAlias(
-            this=sg.to_identifier(name, quoted=quoted), columns=columns
-        )
-        expressions = [
-            sge.Tuple(
-                expressions=[
-                    self.visit_Literal(
-                        ops.Literal(col, dtype=dtype), value=col, dtype=dtype
-                    )
-                    for col, dtype in zip(row, schema.types)
-                ]
-            )
-            for row in tuples
-        ]
-
-        expr = sge.Values(expressions=expressions, alias=alias)
-        return sg.select(*columns).from_(expr)
-
-    def visit_NonNullLiteral(self, op, *, value, dtype):
-        if dtype.is_binary():
-            # TODO: is this decode safe?
-            return self.cast(value.decode(), dtype)
-        elif dtype.is_uuid():
-            return sge.convert(str(value))
-        elif dtype.is_array():
-            value_type = dtype.value_type
-            result = self.f.array(
-                *(
-                    self.visit_Literal(
-                        ops.Literal(v, dtype=value_type), value=v, dtype=value_type
-                    )
-                    for v in value
-                )
-            )
-            if value:
-                return result
-            return sge.Cast(this=result, to=self.type_mapper.from_ibis(dtype))
-        elif dtype.is_map():
-            key_type = dtype.key_type
-            value_type = dtype.value_type
-            keys = self.f.array(
-                *(
-                    self.visit_Literal(
-                        ops.Literal(v, dtype=key_type), value=v, dtype=key_type
-                    )
-                    for v in value.keys()
-                )
-            )
-            values = self.f.array(
-                *(
-                    self.visit_Literal(
-                        ops.Literal(v, dtype=value_type), value=v, dtype=value_type
-                    )
-                    for v in value.values()
-                )
-            )
-            return self.cast(self.f.map_from_arrays(keys, values), dtype)
-        elif dtype.is_timestamp():
-            return self.cast(
-                value.replace(tzinfo=None).isoformat(sep=" ", timespec="microseconds"),
-                dtype,
-            )
-        elif dtype.is_date():
-            return self.cast(value.isoformat(), dtype)
-        elif dtype.is_time():
-            return self.cast(value.isoformat(timespec="microseconds"), dtype)
-        return None
-
-    def visit_ArrayIndex(self, op, *, arg, index):
-        return sge.Bracket(this=arg, expressions=[index + 1])
-
-    def visit_Xor(self, op, *, left, right):
-        return sg.or_(sg.and_(left, sg.not_(right)), sg.and_(sg.not_(left), right))
-
-    def visit_Literal(self, op, *, value, dtype):
-        if value is None:
-            assert dtype.nullable, "dtype is not nullable but value is None"
-            if not dtype.is_null():
-                return self.cast(NULL, dtype)
-            return NULL
-        return super().visit_Literal(op, value=value, dtype=dtype)
-
-    def visit_MapGet(self, op, *, arg, key, default):
-        if default is NULL:
-            default = self.cast(default, op.dtype)
-        return self.f.coalesce(arg[self.cast(key, op.arg.dtype.key_type)], default)
-
-    def visit_ArraySlice(self, op, *, arg, start, stop):
-        args = [arg, self.if_(start >= 0, start + 1, start)]
-
-        if stop is not None:
-            args.append(
-                self.if_(stop >= 0, stop, self.f.cardinality(arg) - self.f.abs(stop))
-            )
-
-        return self.f.array_slice(*args)
-
-    def visit_Not(self, op, *, arg):
-        return sg.not_(self.cast(arg, dt.boolean))
-
-    def visit_Date(self, op, *, arg):
-        return self.cast(arg, dt.date)
-
-    def visit_TryCast(self, op, *, arg, to):
-        type_mapper = self.type_mapper
-        if op.arg.dtype.is_temporal() and to.is_numeric():
-            return self.f.unix_timestamp(
-                sge.TryCast(this=arg, to=type_mapper.from_ibis(dt.string))
-            )
-        return sge.TryCast(this=arg, to=type_mapper.from_ibis(to))
-
-    def visit_FloorDivide(self, op, *, left, right):
-        return self.f.floor(left / right)
-
-    def visit_JSONGetItem(self, op, *, arg, index):
-        assert isinstance(op.index, ops.Literal)
-        idx = op.index
-        val = idx.value
-        if idx.dtype.is_integer():
-            query_path = f"$[{val}]"
+def to_sqlalchemy(expr, context=None, exists=False, dialect=None):
+    if context is not None:
+        dialect = dialect or context.dialect
+
+    ast = build_ast(expr, context=context, dialect=dialect)
+    query = ast.queries[0]
+
+    if exists:
+        query.exists = exists
+
+    return query.compile()
+
+
+def build_ast(expr, context=None, dialect=None):
+    builder = AlchemyQueryBuilder(expr, context=context, dialect=dialect)
+    return builder.get_result()
+
+
+class AlchemyTable(ops.DatabaseTable):
+
+    def __init__(self, table, source):
+        self.sqla_table = table
+
+        schema = schema_from_table(table)
+        name = table.name
+
+        ops.TableNode.__init__(self, [name, schema, source])
+        ops.HasSchema.__init__(self, schema, name=name)
+
+
+class AlchemyExprTranslator(comp.ExprTranslator):
+
+    _registry = _operation_registry
+    _rewrites = comp.ExprTranslator._rewrites.copy()
+    _type_map = _ibis_type_to_sqla
+
+    def name(self, translated, name, force=True):
+        return translated.label(name)
+
+    @property
+    def _context_class(self):
+        return AlchemyContext
+
+    def get_sqla_type(self, data_type):
+        return self._type_map[type(data_type)]
+
+
+rewrites = AlchemyExprTranslator.rewrites
+compiles = AlchemyExprTranslator.compiles
+
+
+class AlchemyQuery(Query):
+
+    def _fetch(self, cursor):
+        # No guarantees that the DBAPI cursor has data types
+        import pandas as pd
+        proxy = cursor.proxy
+        rows = proxy.fetchall()
+        colnames = proxy.keys()
+        return pd.DataFrame.from_records(rows, columns=colnames,
+                                         coerce_float=True)
+
+
+class AlchemyAsyncQuery(AsyncQuery):
+    pass
+
+
+class AlchemyDialect(object):
+
+    translator = AlchemyExprTranslator
+
+
+class AlchemyClient(SQLClient):
+
+    dialect = AlchemyDialect
+    sync_query = AlchemyQuery
+
+    @property
+    def async_query(self):
+        raise NotImplementedError
+
+    def create_table(self, name, expr=None, schema=None, database=None):
+        pass
+
+    def list_tables(self, like=None, database=None):
+        """
+        List tables in the current (or indicated) database.
+
+        Parameters
+        ----------
+        like : string, default None
+          Checks for this string contained in name
+        database : string, default None
+          If not passed, uses the current/default database
+
+        Returns
+        -------
+        tables : list of strings
+        """
+        if database is None:
+            database = self.current_database
+        names = self.con.table_names(schema=database)
+        if like is not None:
+            names = [x for x in names if like in x]
+        return names
+
+    def _execute(self, query, results=True):
+        return AlchemyProxy(self.con.execute(query))
+
+    def _build_ast(self, expr):
+        return build_ast(expr, dialect=self.dialect)
+
+    def _get_sqla_table(self, name):
+        return sa.Table(name, self.meta, autoload=True)
+
+    def _sqla_table_to_expr(self, table):
+        node = AlchemyTable(table, self)
+        return self._table_expr_klass(node)
+
+
+class AlchemySelect(Select):
+
+    def __init__(self, *args, **kwargs):
+        self.exists = kwargs.pop('exists', False)
+        Select.__init__(self, *args, **kwargs)
+
+    def compile(self):
+        # Can't tell if this is a hack or not. Revisit later
+        self.context.set_query(self)
+
+        self._compile_subqueries()
+
+        frag = self._compile_table_set()
+        steps = [self._add_select,
+                 self._add_groupby,
+                 self._add_where,
+                 self._add_order_by,
+                 self._add_limit]
+
+        for step in steps:
+            frag = step(frag)
+
+        return frag
+
+    def _compile_subqueries(self):
+        if len(self.subqueries) == 0:
+            return
+
+        for expr in self.subqueries:
+            result = self.context.get_compiled_expr(expr)
+            alias = self.context.get_ref(expr)
+            result = result.cte(alias)
+            self.context.set_table(expr, result)
+
+    def _compile_table_set(self):
+        if self.table_set is not None:
+            helper = _AlchemyTableSet(self, self.table_set)
+            return helper.get_result()
         else:
-            assert idx.dtype.is_string(), idx.dtype
-            query_path = f"$.{val}"
+            return None
 
-        key_hack = f"{sge.convert(query_path).sql(self.dialect)} WITH CONDITIONAL ARRAY WRAPPER"
-        return self.f.json_query(arg, self.v[key_hack])
+    def _add_select(self, table_set):
+        to_select = []
+        for expr in self.select_set:
+            if isinstance(expr, ir.ValueExpr):
+                arg = self._translate(expr, named=True)
+            elif isinstance(expr, ir.TableExpr):
+                if expr.equals(self.table_set):
+                    cached_table = self.context.get_table(expr)
+                    if cached_table is None:
+                        # the select * case from materialized join
+                        arg = '*'
+                    else:
+                        arg = table_set
+                else:
+                    arg = self.context.get_table(expr)
+                    if arg is None:
+                        raise ValueError(expr)
 
-    def visit_TimestampFromUNIX(self, op, *, arg, unit):
-        from ibis.common.temporal import TimestampUnit
+            to_select.append(arg)
 
-        if unit == TimestampUnit.MILLISECOND:
-            precision = 3
-        elif unit == TimestampUnit.SECOND:
-            precision = 0
+        if self.exists:
+            clause = sa.exists(to_select)
         else:
-            raise ValueError(f"{unit!r} unit is not supported!")
+            clause = sa.select(to_select)
+
+        if self.distinct:
+            clause = clause.distinct()
+
+        if table_set is not None:
+            return clause.select_from(table_set)
+        else:
+            return clause
+
+    def _add_groupby(self, fragment):
+        # GROUP BY and HAVING
+        if not len(self.group_by):
+            return fragment
+
+        group_keys = [self._translate(arg) for arg in self.group_by]
+        fragment = fragment.group_by(*group_keys)
+
+        if len(self.having) > 0:
+            having_args = [self._translate(arg) for arg in self.having]
+            having_clause = _and_all(having_args)
+            fragment = fragment.having(having_clause)
+
+        return fragment
+
+    def _add_where(self, fragment):
+        if not len(self.where):
+            return fragment
+
+        args = [self._translate(pred, permit_subquery=True)
+                for pred in self.where]
+        clause = _and_all(args)
+        return fragment.where(clause)
+
+    def _add_order_by(self, fragment):
+        if not len(self.order_by):
+            return fragment
+
+        clauses = []
+        for expr in self.order_by:
+            key = expr.op()
+            sort_expr = key.expr
+
+            # here we have to determine if key.expr is in the select set (as it
+            # will be in the case of order_by fused with an aggregation
+            if _can_lower_sort_column(self.table_set, sort_expr):
+                arg = sort_expr.get_name()
+            else:
+                arg = self._translate(sort_expr)
+
+            if not key.ascending:
+                arg = sa.desc(arg)
+
+            clauses.append(arg)
+
+        return fragment.order_by(*clauses)
+
+    def _among_select_set(self, expr):
+        for other in self.select_set:
+            if expr.equals(other):
+                return True
+        return False
+
+    def _add_limit(self, fragment):
+        if self.limit is None:
+            return fragment
+
+        n, offset = self.limit['n'], self.limit['offset']
+        fragment = fragment.limit(n)
+        if offset is not None and offset != 0:
+            fragment = fragment.offset(offset)
+
+        return fragment
+
+    @property
+    def translator(self):
+        return self.dialect.translator
+
+    @property
+    def dialect(self):
+        return self.context.dialect
+
+
+class _AlchemyTableSet(TableSetFormatter):
 
-        return self.cast(self.f.to_timestamp_ltz(arg, precision), dt.timestamp)
+    def get_result(self):
+        # Got to unravel the join stack; the nesting order could be
+        # arbitrary, so we do a depth first search and push the join tokens
+        # and predicates onto a flat list, then format them
+        op = self.expr.op()
 
-    def visit_Time(self, op, *, arg):
-        return self.cast(arg, op.dtype)
+        if isinstance(op, ops.Join):
+            self._walk_join_tree(op)
+        else:
+            self.join_tables.append(self._format_table(self.expr))
+
+        result = self.join_tables[0]
+        for jtype, table, preds in zip(self.join_types,
+                                       self.join_tables[1:],
+                                       self.join_predicates):
+            if len(preds):
+                sqla_preds = [self._translate(pred) for pred in preds]
+                onclause = _and_all(sqla_preds)
+            else:
+                onclause = None
 
-    def visit_TimeFromHMS(self, op, *, hours, minutes, seconds):
-        padded_hour = self.f.lpad(self.cast(hours, dt.string), 2, "0")
-        padded_minute = self.f.lpad(self.cast(minutes, dt.string), 2, "0")
-        padded_second = self.f.lpad(self.cast(seconds, dt.string), 2, "0")
-        return self.cast(
-            self.f.concat(padded_hour, ":", padded_minute, ":", padded_second), op.dtype
-        )
-
-    def visit_DateFromYMD(self, op, *, year, month, day):
-        padded_year = self.f.lpad(self.cast(year, dt.string), 4, "0")
-        padded_month = self.f.lpad(self.cast(month, dt.string), 2, "0")
-        padded_day = self.f.lpad(self.cast(day, dt.string), 2, "0")
-        return self.cast(
-            self.f.concat(padded_year, "-", padded_month, "-", padded_day), op.dtype
-        )
-
-    def visit_TimestampFromYMDHMS(
-        self, op, *, year, month, day, hours, minutes, seconds
-    ):
-        padded_year = self.f.lpad(self.cast(year, dt.string), 4, "0")
-        padded_month = self.f.lpad(self.cast(month, dt.string), 2, "0")
-        padded_day = self.f.lpad(self.cast(day, dt.string), 2, "0")
-        padded_hour = self.f.lpad(self.cast(hours, dt.string), 2, "0")
-        padded_minute = self.f.lpad(self.cast(minutes, dt.string), 2, "0")
-        padded_second = self.f.lpad(self.cast(seconds, dt.string), 2, "0")
-        return self.cast(
-            self.f.concat(
-                padded_year,
-                "-",
-                padded_month,
-                "-",
-                padded_day,
-                " ",
-                padded_hour,
-                ":",
-                padded_minute,
-                ":",
-                padded_second,
-            ),
-            op.dtype,
-        )
-
-    def visit_ExtractEpochSeconds(self, op, *, arg):
-        return self.f.unix_timestamp(self.cast(arg, dt.string))
-
-    def visit_Cast(self, op, *, arg, to):
-        from_ = op.arg.dtype
-        if to.is_timestamp():
-            if from_.is_numeric():
-                arg = self.f.from_unixtime(arg)
-            if (tz := to.timezone) is not None:
-                return self.f.to_timestamp(
-                    self.f.convert_tz(self.cast(arg, dt.string), "UTC+0", tz)
-                )
+            if jtype in (ops.InnerJoin, ops.CrossJoin):
+                result = result.join(table, onclause)
+            elif jtype is ops.LeftJoin:
+                result = result.join(table, onclause, isouter=True)
+            elif jtype is ops.RightJoin:
+                result = table.join(result, onclause, isouter=True)
+            elif jtype is ops.OuterJoin:
+                result = result.outerjoin(table, onclause)
             else:
-                return self.f.to_timestamp(arg, "yyyy-MM-dd HH:mm:ss.SSS")
-        elif to.is_json():
-            return arg
-        elif from_.is_temporal() and to.is_int64():
-            return 1_000_000 * self.f.unix_timestamp(arg)
+                raise NotImplementedError(jtype)
+
+        return result
+
+    def _get_join_type(self, op):
+        return type(op)
+
+    def _format_table(self, expr):
+        ctx = self.context
+        ref_expr = expr
+        op = ref_op = expr.op()
+
+        if isinstance(op, ops.SelfReference):
+            ref_expr = op.table
+            ref_op = ref_expr.op()
+
+        alias = ctx.get_ref(expr)
+
+        if isinstance(ref_op, AlchemyTable):
+            result = ref_op.sqla_table
         else:
-            return self.cast(arg, to)
+            # A subquery
+            if ctx.is_extracted(ref_expr):
+                # Was put elsewhere, e.g. WITH block, we just need to grab
+                # its alias
+                alias = ctx.get_ref(expr)
+
+                # hack
+                if isinstance(op, ops.SelfReference):
+                    table = ctx.get_table(ref_expr)
+                    self_ref = table.alias(alias)
+                    ctx.set_table(expr, self_ref)
+                    return self_ref
+                else:
+                    return ctx.get_table(expr)
+
+            result = ctx.get_compiled_expr(expr)
+            alias = ctx.get_ref(expr)
+
+        result = result.alias(alias)
+        ctx.set_table(expr, result)
+        return result
+
+
+def _can_lower_sort_column(table_set, expr):
+    # we can currently sort by just-appeared aggregate metrics, but the way
+    # these are references in the expression DSL is as a SortBy (blocking
+    # table operation) on an aggregation. There's a hack in _collect_SortBy
+    # in the generic SQL compiler that "fuses" the sort with the
+    # aggregation so they appear in same query. It's generally for
+    # cosmetics and doesn't really affect query semantics.
+    bases = ir.find_all_base_tables(expr)
+    if len(bases) > 1:
+        return False
+
+    base = list(bases.values())[0]
+    base_op = base.op()
+
+    if isinstance(base_op, ops.Aggregation):
+        return base_op.table.equals(table_set)
+    elif isinstance(base_op, ops.Projection):
+        return base.equals(table_set)
+    else:
+        return False
+
+
+def _and_all(clauses):
+    result = clauses[0]
+    for clause in clauses[1:]:
+        result = sql.and_(result, clause)
+    return result
 
-    def visit_IfElse(self, op, *, bool_expr, true_expr, false_null_expr):
-        return self.if_(
-            bool_expr,
-            true_expr if true_expr != NULL else self.cast(true_expr, op.dtype),
-            (
-                false_null_expr
-                if false_null_expr != NULL
-                else self.cast(false_null_expr, op.dtype)
-            ),
-        )
-
-    def visit_Log10(self, op, *, arg):
-        return self.f.anon.log(10, arg)
-
-    def visit_ExtractMillisecond(self, op, *, arg):
-        return self.f.extract(self.v.millisecond, arg)
-
-    def visit_ExtractMicrosecond(self, op, *, arg):
-        return self.f.extract(self.v.microsecond, arg)
-
-    def visit_DayOfWeekIndex(self, op, *, arg):
-        return (self.f.dayofweek(arg) + 5) % 7
-
-    def visit_DayOfWeekName(self, op, *, arg):
-        index = self.cast(self.f.dayofweek(self.cast(arg, dt.date)), op.dtype)
-        lookup_table = self.f.str_to_map(
-            "1=Sunday,2=Monday,3=Tuesday,4=Wednesday,5=Thursday,6=Friday,7=Saturday"
-        )
-        return lookup_table[index]
-
-    def visit_TimestampNow(self, op):
-        return self.v.current_timestamp
-
-    def visit_DateNow(self, op):
-        return self.v.current_date
-
-    def visit_TimestampBucket(self, op, *, arg, interval, offset):
-        unit = op.interval.dtype.unit.name
-        unit_var = self.v[unit]
 
-        if offset is None:
-            offset = 0
+class AlchemyUnion(Union):
+
+    def compile(self):
+        context = self.context
+
+        if self.distinct:
+            sa_func = sa.union
         else:
-            offset = op.offset.value
+            sa_func = sa.union_all
+
+        left_set = context.get_compiled_expr(self.left)
+        right_set = context.get_compiled_expr(self.right)
+
+        return sa_func(left_set, right_set)
+
+
+class AlchemyProxy(object):
+    """
+    Wraps a SQLAlchemy ResultProxy and ensures that .close() is called on
+    garbage collection
+    """
+    def __init__(self, proxy):
+        self.proxy = proxy
+
+    def __del__(self):
+        self._close_cursor()
+
+    def _close_cursor(self):
+        self.proxy.close()
+
+    def __enter__(self):
+        return self
+
+    def __exit__(self, type, value, tb):
+        self._close_cursor()
+
+    def fetchall(self):
+        return self.proxy.fetchall()
+
+
+@rewrites(ops.NullIfZero)
+def _nullifzero(expr):
+    arg = expr.op().args[0]
+    return (arg == 0).ifelse(ibis.NA, arg)
+
+
+@compiles(ops.Divide)
+def _true_divide(t, expr):
+    op = expr.op()
+    left, right = op.args
 
-        bucket_width = op.interval.value
-        unit_func = self.f["dayofmonth" if unit.upper() == "DAY" else unit]
+    if util.all_of(op.args, ir.IntegerValue):
+        new_expr = left.div(right.cast('double'))
+        return t.translate(new_expr)
 
-        arg = self.f.anon.timestampadd(unit_var, -sge.paren(offset, copy=False), arg)
-        mod = unit_func(arg) % bucket_width
+    return fixed_arity(lambda x, y: x / y, 2)(t, expr)
 
-        return self.f.anon.timestampadd(
-            unit_var,
-            -sge.paren(mod, copy=False) + offset,
-            self.v[f"FLOOR({arg.sql(self.dialect)} TO {unit_var.sql(self.dialect)})"],
-        )
-
-    def visit_TemporalDelta(self, op, *, part, left, right):
-        right = self.visit_TemporalTruncate(None, arg=right, unit=part)
-        left = self.visit_TemporalTruncate(None, arg=left, unit=part)
-        return self.f.anon.timestampdiff(
-            self.v[part.this],
-            self.cast(right, dt.timestamp),
-            self.cast(left, dt.timestamp),
-        )
-
-    visit_TimeDelta = visit_DateDelta = visit_TimestampDelta = visit_TemporalDelta
-
-    def visit_TemporalTruncate(self, op, *, arg, unit):
-        unit_var = self.v[unit.name]
-        arg_sql = arg.sql(self.dialect)
-        unit_sql = unit_var.sql(self.dialect)
-        return self.f.floor(self.v[f"{arg_sql} TO {unit_sql}"])
-
-    visit_TimestampTruncate = visit_DateTruncate = visit_TimeTruncate = (
-        visit_TemporalTruncate
-    )
-
-    def visit_StringContains(self, op, *, haystack, needle):
-        return self.f.instr(haystack, needle) > 0
-
-    def visit_StringFind(self, op, *, arg, substr, start, end):
-        if end is not None:
-            raise com.UnsupportedOperationError(
-                "String find doesn't support `end` argument"
-            )
-
-        if start is not None:
-            arg = self.f.substr(arg, start + 1)
-            pos = self.f.instr(arg, substr)
-            return self.if_(pos > 0, pos + start, 0)
-
-        return self.f.instr(arg, substr)
-
-    def visit_StartsWith(self, op, *, arg, start):
-        return self.f.left(arg, self.f.char_length(start)).eq(start)
-
-    def visit_EndsWith(self, op, *, arg, end):
-        return self.f.right(arg, self.f.char_length(end)).eq(end)
-
-    def visit_ExtractUrlField(self, op, *, arg):
-        return self.f.parse_url(arg, type(op).__name__[len("Extract") :].upper())
-
-    visit_ExtractAuthority = visit_ExtractHost = visit_ExtractUserInfo = (
-        visit_ExtractProtocol
-    ) = visit_ExtractFile = visit_ExtractPath = visit_ExtractUrlField
-
-    def visit_ExtractQuery(self, op, *, arg, key):
-        return self.f.parse_url(*filter(None, (arg, "QUERY", key)))
-
-    def visit_ExtractFragment(self, op, *, arg):
-        return self.f.parse_url(arg, "REF")
-
-    def visit_CountStar(self, op, *, arg, where):
-        if where is None:
-            return self.f.count(STAR)
-        return self.f.sum(self.cast(where, dt.int64))
-
-    def visit_CountDistinct(self, op, *, arg, where):
-        if where is not None:
-            arg = self.if_(where, arg, self.f.array(arg)[2])
-        return self.f.count(sge.Distinct(expressions=[arg]))
-
-    def visit_MapContains(self, op: ops.MapContains, *, arg, key):
-        key_type = op.arg.dtype.key_type
-        return self.f.array_contains(
-            self.cast(self.f.map_keys(arg), dt.Array(value_type=key_type)),
-            self.cast(key, key_type),
-        )
-
-    def visit_Map(self, op: ops.Map, *, keys, values):
-        return self.cast(self.f.map_from_arrays(keys, values), op.dtype)
-
-    def visit_MapMerge(self, op: ops.MapMerge, *, left, right):
-        left_keys = self.f.map_keys(left)
-        left_values = self.f.map_values(left)
-
-        right_keys = self.f.map_keys(right)
-        right_values = self.f.map_values(right)
 
-        keys = self.f.array_concat(left_keys, right_keys)
-        values = self.f.array_concat(left_values, right_values)
+@compiles(ops.FloorDivide)
+def _floor_divide(t, expr):
+    op = expr.op()
+    left, right = op.args
 
-        return self.cast(self.f.map_from_arrays(keys, values), op.dtype)
+    if util.any_of(op.args, ir.FloatingValue):
+        new_expr = expr.floor()
+        return t.translate(new_expr)
 
-    def visit_StructColumn(self, op, *, names, values):
-        return self.cast(sge.Struct(expressions=list(values)), op.dtype)
+    return fixed_arity(lambda x, y: x / y, 2)(t, expr)
```

### Comparing `ibis_framework-9.0.0.dev686/ibis/backends/impala/__init__.py` & `ibis-framework-v0.6.0/ibis/expr/tests/test_table.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,1331 +1,1122 @@
-"""Impala backend."""
+# Copyright 2014 Cloudera Inc.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+# http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from ibis.expr.types import ArrayExpr, TableExpr, RelationError
+from ibis.common import ExpressionError
+from ibis.expr.datatypes import array_type
+import ibis.expr.api as api
+import ibis.expr.types as ir
+import ibis.expr.operations as ops
+import ibis
 
-from __future__ import annotations
+from ibis.compat import unittest
+from ibis.expr.tests.mocks import MockConnection, BasicTestCase
 
-import contextlib
-import operator
-import os
-from functools import cached_property
-from typing import TYPE_CHECKING, Any, Literal
-from urllib.parse import parse_qs, urlparse
-
-import impala.dbapi as impyla
-import sqlglot as sg
-import sqlglot.expressions as sge
-from impala.error import Error as ImpylaError
-
-import ibis.common.exceptions as com
-import ibis.config
-import ibis.expr.schema as sch
-import ibis.expr.types as ir
-from ibis import util
-from ibis.backends.impala import ddl, udf
-from ibis.backends.impala.client import ImpalaTable
-from ibis.backends.impala.compiler import ImpalaCompiler
-from ibis.backends.impala.ddl import (
-    CTAS,
-    CreateDatabase,
-    CreateTableWithSchema,
-    CreateView,
-    DropDatabase,
-    DropTable,
-    DropView,
-    RenameTable,
-    TruncateTable,
-)
-from ibis.backends.impala.udf import (
-    aggregate_function,
-    scalar_function,
-    wrap_uda,
-    wrap_udf,
-)
-from ibis.backends.sql import SQLBackend
-
-if TYPE_CHECKING:
-    from collections.abc import Mapping
-    from pathlib import Path
-
-    import pandas as pd
-    import pyarrow as pa
-
-    import ibis.expr.operations as ops
-
-
-__all__ = (
-    "Backend",
-    "aggregate_function",
-    "scalar_function",
-    "wrap_uda",
-    "wrap_udf",
-)
-
-
-class Backend(SQLBackend):
-    name = "impala"
-    compiler = ImpalaCompiler()
-
-    supports_in_memory_tables = True
-
-    def _from_url(self, url: str, **kwargs: Any) -> Backend:
-        """Connect to a backend using a URL `url`.
-
-        Parameters
-        ----------
-        url
-            URL with which to connect to a backend.
-        kwargs
-            Additional keyword arguments passed to the `connect` method.
-
-        Returns
-        -------
-        BaseBackend
-            A backend instance
-
-        """
-        url = urlparse(url)
-
-        for name in ("username", "hostname", "port", "password"):
-            if value := (
-                getattr(url, name, None)
-                or os.environ.get(f"{self.name.upper()}_{name.upper()}")
-            ):
-                kwargs[name] = value
-
-        with contextlib.suppress(KeyError):
-            kwargs["host"] = kwargs.pop("hostname")
-
-        (database,) = url.path[1:].split("/", 1)
-        if database:
-            kwargs["database"] = database
-
-        query_params = parse_qs(url.query)
-
-        for name, value in query_params.items():
-            if len(value) > 1:
-                kwargs[name] = value
-            elif len(value) == 1:
-                kwargs[name] = value[0]
-            else:
-                raise com.IbisError(f"Invalid URL parameter: {name}")
-
-        self._convert_kwargs(kwargs)
-        return self.connect(**kwargs)
-
-    def do_connect(
-        self,
-        host: str = "localhost",
-        port: int = 21050,
-        database: str = "default",
-        timeout: int = 45,
-        use_ssl: bool = False,
-        ca_cert: str | Path | None = None,
-        user: str | None = None,
-        password: str | None = None,
-        auth_mechanism: Literal["NOSASL", "PLAIN", "GSSAPI", "LDAP"] = "NOSASL",
-        kerberos_service_name: str = "impala",
-        **params: Any,
-    ):
-        """Create an Impala `Backend` for use with Ibis.
-
-        Parameters
-        ----------
-        host
-            Host name of the impalad or HiveServer2 in Hive
-        port
-            Impala's HiveServer2 port
-        database
-            Default database when obtaining new cursors
-        timeout
-            Connection timeout in seconds when communicating with HiveServer2
-        use_ssl
-            Use SSL when connecting to HiveServer2
-        ca_cert
-            Local path to 3rd party CA certificate or copy of server
-            certificate for self-signed certificates. If SSL is enabled, but
-            this argument is ``None``, then certificate validation is skipped.
-        user
-            LDAP user to authenticate
-        password
-            LDAP password to authenticate
-        auth_mechanism
-            |   Value    | Meaning                        |
-            | :--------: | :----------------------------- |
-            | `'NOSASL'` | insecure Impala connections    |
-            | `'PLAIN'`  | insecure Hive clusters         |
-            |  `'LDAP'`  | LDAP authenticated connections |
-            | `'GSSAPI'` | Kerberos-secured clusters      |
-        kerberos_service_name
-            Specify a particular `impalad` service principal.
-        params
-            Any additional parameters necessary to open a connection to Impala.
-            Please refer to impyla documentation for the full list of
-            possible arguments.
-
-        Examples
-        --------
-        >>> import os
-        >>> import ibis
-        >>> impala_host = os.environ.get("IBIS_TEST_IMPALA_HOST", "localhost")
-        >>> impala_port = int(os.environ.get("IBIS_TEST_IMPALA_PORT", 21050))
-        >>> client = ibis.impala.connect(host=impala_host, port=impala_port)
-        >>> client  # doctest: +ELLIPSIS
-        <ibis.backends.impala.Backend object at 0x...>
-
-        """
-        if ca_cert is not None:
-            params["ca_cert"] = str(ca_cert)
-
-        # make sure the connection works
-        con = impyla.connect(
-            host=host,
-            port=port,
-            database=database,
-            timeout=timeout,
-            use_ssl=use_ssl,
-            user=user,
-            password=password,
-            auth_mechanism=auth_mechanism,
-            kerberos_service_name=kerberos_service_name,
-            **params,
-        )
-        with contextlib.closing(
-            con.cursor(user=params.get("user"), convert_types=True)
-        ) as cur:
-            cur.ping()
-
-        self.con = con
-        self.options = {}
-
-    @cached_property
-    def version(self):
-        with self._safe_raw_sql("SELECT VERSION()") as cursor:
-            (result,) = cursor.fetchone()
-        return result
-
-    def list_databases(self, like=None):
-        with self._safe_raw_sql("SHOW DATABASES") as cur:
-            databases = fetchall(cur)
-        return self._filter_with_like(databases.name.tolist(), like)
-
-    def list_tables(self, like=None, database=None):
-        """Return the list of table names in the current database.
-
-        Parameters
-        ----------
-        like
-            A pattern in Python's regex format.
-        database
-            The database from which to list tables.
-            If not provided, the current database is used.
-
-        Returns
-        -------
-        list[str]
-            The list of the table names that match the pattern `like`.
-        """
-
-        statement = "SHOW TABLES"
-        if database is not None:
-            statement += f" IN {database}"
-
-        with self._safe_raw_sql(statement) as cursor:
-            tables = fetchall(cursor)
-        return self._filter_with_like(tables.name.tolist(), like=like)
-
-    def raw_sql(self, query: str):
-        cursor = self.con.cursor()
-
-        try:
-            for k, v in self.options.items():
-                q = f"SET {k} = {v!r}"
-                util.log(q)
-                cursor.execute_async(q)
-
-            cursor._wait_to_finish()
-
-            util.log(query)
-            cursor.execute_async(query)
-
-            cursor._wait_to_finish()
-        except (Exception, KeyboardInterrupt):
-            cursor.cancel_operation()
-            cursor.close()
-            raise
-
-        return cursor
-
-    def _fetch_from_cursor(self, cursor, schema):
-        from ibis.formats.pandas import PandasData
-
-        results = fetchall(cursor)
-        return PandasData.convert_table(results, schema)
-
-    @contextlib.contextmanager
-    def _safe_raw_sql(self, query: str):
-        if not isinstance(query, str):
-            try:
-                query = query.sql(dialect=self.dialect)
-            except AttributeError:
-                query = query.compile()
-
-        assert isinstance(query, str), type(query)
-        with contextlib.closing(self.raw_sql(query)) as cur:
-            yield cur
-
-    def _safe_exec_sql(self, *args, **kwargs):
-        with self._safe_raw_sql(*args, **kwargs):
-            pass
-
-    def _fully_qualified_name(self, name, database):
-        database = database or self.current_database
-        return sg.table(name, db=database, quoted=self.compiler.quoted).sql(
-            self.dialect
-        )
-
-    @property
-    def current_database(self) -> str:
-        with self._safe_raw_sql("SELECT CURRENT_DATABASE()") as cur:
-            [(db,)] = cur.fetchall()
-        return db
-
-    def create_database(self, name, path=None, force=False):
-        """Create a new Impala database.
-
-        Parameters
-        ----------
-        name
-            Database name
-        path
-            Path where to store the database data; otherwise uses the Impala default
-        force
-            Forcibly create the database
-
-        """
-        statement = CreateDatabase(name, path=path, can_exist=force)
-        self._safe_exec_sql(statement)
-
-    def drop_database(self, name, force=False):
-        """Drop an Impala database.
-
-        Parameters
-        ----------
-        name
-            Database name
-        force
-            If False and there are any tables in this database, raises an
-            IntegrityError
-
-        """
-        if not force or name in self.list_databases():
-            tables = self.list_tables(database=name)
-            udfs = self.list_udfs(database=name)
-            udas = self.list_udas(database=name)
-        else:
-            tables = []
-            udfs = []
-            udas = []
-        if force:
-            for table in tables:
-                util.log(f"Dropping {name}.{table}")
-                self.drop_table_or_view(table, database=name)
-            for func in udfs:
-                util.log(f"Dropping function {func.name}({func.inputs})")
-                self.drop_udf(
-                    func.name,
-                    input_types=func.inputs,
-                    database=name,
-                    force=True,
-                )
-            for func in udas:
-                util.log(f"Dropping aggregate function {func.name}({func.inputs})")
-                self.drop_uda(
-                    func.name,
-                    input_types=func.inputs,
-                    database=name,
-                    force=True,
-                )
-        elif tables or udfs or udas:
-            raise com.IntegrityError(
-                f"Database {name} must be empty before "
-                "being dropped, or set force=True"
-            )
-        statement = DropDatabase(name, must_exist=not force)
-        self._safe_exec_sql(statement)
-
-    def get_schema(
-        self,
-        table_name: str,
-        *,
-        catalog: str | None = None,
-        database: str | None = None,
-    ) -> sch.Schema:
-        """Return a Schema object for the indicated table and database.
-
-        Parameters
-        ----------
-        table_name
-            Table name
-        catalog
-            Catalog name. Unused in the impala backend.
-        database
-            Database name
-
-        Returns
-        -------
-        Schema
-            Ibis schema
-
-        """
-        query = sge.Describe(
-            this=sg.table(
-                table_name, db=database, catalog=catalog, quoted=self.compiler.quoted
-            )
-        )
-
-        with self._safe_raw_sql(query) as cur:
-            meta = fetchall(cur)
-        return sch.Schema.from_tuples(
-            zip(meta["name"], meta["type"].map(self.compiler.type_mapper.from_string))
-        )
-
-    def _get_schema_using_query(self, query: str) -> sch.Schema:
-        """Return a Schema object for the indicated table and database."""
-        name = util.gen_name(f"{self.name}_metadata")
-        ident = sg.to_identifier(name, quoted=self.compiler.quoted)
-        create_sql = sge.Create(
-            kind="VIEW", this=ident, exists=True, expression=query, dialect=self.dialect
-        )
-        drop_sql = sge.Drop(kind="VIEW", this=ident, exists=True)
-
-        with self._safe_raw_sql(create_sql):
-            pass
-
-        try:
-            return self.get_schema(name)
-        finally:
-            with self._safe_raw_sql(drop_sql):
-                pass
-
-    @property
-    def client_options(self):
-        return self.con.options
-
-    def get_options(self) -> dict[str, str]:
-        """Return current query options for the Impala session."""
-        with self._safe_raw_sql("SET") as cur:
-            opts = fetchall(cur)
-
-        return dict(zip(opts.option, opts.value))
-
-    def set_options(self, options):
-        self.options.update(options)
-
-    def set_compression_codec(self, codec):
-        self.set_options({"COMPRESSION_CODEC": str(codec).lower()})
-
-    def create_view(
-        self,
-        name: str,
-        obj: ir.Table,
-        *,
-        database: str | None = None,
-        overwrite: bool = False,
-    ) -> ir.Table:
-        select = self.compile(obj)
-        statement = CreateView(name, select, database=database, can_exist=overwrite)
-        self._safe_exec_sql(statement)
-        return self.table(name, database=database)
-
-    def drop_view(self, name, database=None, force=False):
-        stmt = DropView(name, database=database, must_exist=not force)
-        self._safe_exec_sql(stmt)
-
-    def table(self, name: str, database: str | None = None, **kwargs: Any) -> ir.Table:
-        expr = super().table(name, database=database, **kwargs)
-        return ImpalaTable(expr.op())
-
-    def create_table(
-        self,
-        name: str,
-        obj: ir.Table | None = None,
-        *,
-        schema=None,
-        database=None,
-        temp: bool | None = None,
-        overwrite: bool = False,
-        external: bool = False,
-        format="parquet",
-        location=None,
-        partition=None,
-        like_parquet=None,
-    ) -> ir.Table:
-        """Create a new table in Impala using an Ibis table expression.
-
-        Parameters
-        ----------
-        name
-            Table name
-        obj
-            If passed, creates table from select statement results
-        schema
-            Mutually exclusive with obj, creates an empty table with a
-            particular schema
-        database
-            Database name
-        temp
-            Whether a table is temporary
-        overwrite
-            Do not create table if table with indicated name already exists
-        external
-            Create an external table; Impala will not delete the underlying
-            data when the table is dropped
-        format
-            File format
-        location
-            Specify the directory location where Impala reads and writes files
-            for the table
-        partition
-            Must pass a schema to use this. Cannot partition from an
-            expression.
-        like_parquet
-            Can specify instead of a schema
-
-        """
-        if obj is None and schema is None:
-            raise com.IbisError("The schema or obj parameter is required")
-
-        if temp is not None:
-            raise NotImplementedError(
-                "Impala backend does not yet support temporary tables"
-            )
-        if like_parquet is not None:
-            raise NotImplementedError
-
-        if obj is not None:
-            if not isinstance(obj, ir.Table):
-                obj = ibis.memtable(obj)
-
-            self._run_pre_execute_hooks(obj)
-
-            select = self.compile(obj)
-
-            if overwrite:
-                self.drop_table(name, force=True)
-
-            self._safe_exec_sql(
-                CTAS(
-                    name,
-                    select,
-                    database=database or self.current_database,
-                    format=format,
-                    external=True if location is not None else external,
-                    partition=partition,
-                    path=location,
-                )
-            )
-        else:  # schema is not None
-            if overwrite:
-                self.drop_table(name, force=True)
-            self._safe_exec_sql(
-                CreateTableWithSchema(
-                    name,
-                    schema if schema is not None else obj.schema(),
-                    database=database or self.current_database,
-                    format=format,
-                    external=external,
-                    path=location,
-                    partition=partition,
-                )
-            )
-        return self.table(name, database=database or self.current_database)
-
-    def avro_file(
-        self, directory, avro_schema, name=None, database=None, external=True
-    ):
-        """Create a table to read a collection of Avro data.
-
-        Parameters
-        ----------
-        directory
-            Server path to directory containing avro files
-        avro_schema
-            The Avro schema for the data as a Python dict
-        name
-            Table name
-        database
-            Database name
-        external
-            Whether the table is external
-
-        Returns
-        -------
-        ImpalaTable
-            Impala table expression
-
-        """
-        name, database = self._get_concrete_table_path(name, database)
-
-        stmt = ddl.CreateTableAvro(
-            name, directory, avro_schema, database=database, external=external
-        )
-        self._safe_exec_sql(stmt)
-        return self._wrap_new_table(name, database)
-
-    def delimited_file(
-        self,
-        directory,
-        schema,
-        name=None,
-        database=None,
-        delimiter=",",
-        na_rep=None,
-        escapechar=None,
-        lineterminator=None,
-        external=True,
-    ):
-        """Interpret delimited text files as an Ibis table expression.
-
-        See the `parquet_file` method for more details on what happens under
-        the hood.
-
-        Parameters
-        ----------
-        directory
-            Server directory containing delimited text files
-        schema
-            Ibis schema
-        name
-            Name for the table; otherwise random names are generated
-        database
-            Database to create the table in
-        delimiter
-            Character used to delimit columns
-        na_rep
-            Character used to represent NULL values
-        escapechar
-            Character used to escape special characters
-        lineterminator
-            Character used to delimit lines
-        external
-            Create table as EXTERNAL (data will not be deleted on drop).
-
-        Returns
-        -------
-        ImpalaTable
-            Impala table expression
-
-        """
-        name, database = self._get_concrete_table_path(name, database)
-
-        stmt = ddl.CreateTableDelimited(
-            name,
-            directory,
-            schema,
-            database=database,
-            delimiter=delimiter,
-            external=external,
-            na_rep=na_rep,
-            lineterminator=lineterminator,
-            escapechar=escapechar,
-        )
-        self._safe_exec_sql(stmt)
-        return self._wrap_new_table(name, database)
-
-    def parquet_file(
-        self,
-        directory: str | Path,
-        schema: sch.Schema | None = None,
-        name: str | None = None,
-        database: str | None = None,
-        external: bool = True,
-        like_file: str | Path | None = None,
-        like_table: str | None = None,
-    ):
-        """Create an Ibis table from the passed directory of Parquet files.
-
-        The table can be optionally named, otherwise a unique name will be
-        generated.
-
-        Parameters
-        ----------
-        directory
-            Path
-        schema
-            If no schema provided, and neither of the like_* argument is
-            passed, one will be inferred from one of the parquet files in the
-            directory.
-        like_file
-            Absolute path to Parquet file on the server to use for schema
-            definitions. An alternative to having to supply an explicit schema
-        like_table
-            Fully scoped and escaped string to an Impala table whose schema we
-            will use for the newly created table.
-        name
-            Random unique name generated otherwise
-        database
-            Database to create the (possibly temporary) table in
-        external
-            If a table is external, the referenced data will not be deleted
-            when the table is dropped in Impala. Otherwise Impala takes
-            ownership of the Parquet file.
-
-        Returns
-        -------
-        ImpalaTable
-            Impala table expression
-
-        """
-        name, database = self._get_concrete_table_path(name, database)
-
-        stmt = ddl.CreateTableParquet(
-            name,
-            directory,
-            schema=schema,
-            database=database,
-            example_file=like_file,
-            example_table=like_table,
-            external=external,
-            can_exist=False,
-        )
-        self._safe_exec_sql(stmt)
-        return self._wrap_new_table(name, database)
-
-    def _get_concrete_table_path(
-        self, name: str | None, database: str | None
-    ) -> tuple[str, str | None]:
-        return name if name is not None else util.gen_name("impala_table"), database
-
-    def _drop_table(self, name: str) -> None:
-        # database might have been dropped, so we suppress the
-        # corresponding Exception
-        with contextlib.suppress(ImpylaError):
-            self.drop_table(name)
-
-    def _wrap_new_table(self, name, database):
-        qualified_name = self._fully_qualified_name(name, database)
-        t = self.table(name, database=database)
-
-        # Compute number of rows in table for better default query planning
-        cardinality = t.count().execute()
-        self._safe_exec_sql(
-            f"ALTER TABLE {qualified_name} SET tblproperties('numRows'='{cardinality:d}', "
-            "'STATS_GENERATED_VIA_STATS_TASK' = 'true')"
-        )
-
-        return t
-
-    def insert(
-        self,
-        table_name,
-        obj=None,
-        database=None,
-        overwrite=False,
-        partition=None,
-        values=None,
-        validate=True,
-    ):
-        """Insert data into an existing table.
-
-        See
-        [`ImpalaTable.insert`](../backends/impala.qmd#ibis.backends.impala.client.ImpalaTable.insert)
-        for parameters.
-
-        Examples
-        --------
-        >>> table = "my_table"
-        >>> con.insert(table, table_expr)  # quartodoc: +SKIP # doctest: +SKIP
-
-        Completely overwrite contents
-        >>> con.insert(table, table_expr, overwrite=True)  # quartodoc: +SKIP # doctest: +SKIP
-
-        """
-        if isinstance(obj, ir.Table):
-            self._run_pre_execute_hooks(obj)
-        table = self.table(table_name, database=database)
-        return table.insert(
-            obj=obj,
-            overwrite=overwrite,
-            partition=partition,
-            values=values,
-            validate=validate,
-        )
-
-    def drop_table(
-        self, name: str, *, database: str | None = None, force: bool = False
-    ) -> None:
-        """Drop an Impala table.
-
-        Parameters
-        ----------
-        name
-            Table name
-        database
-            Database name
-        force
-            Database may throw exception if table does not exist
-
-        Examples
-        --------
-        >>> table = "my_table"
-        >>> db = "operations"
-        >>> con.drop_table(table, database=db, force=True)  # quartodoc: +SKIP # doctest: +SKIP
-
-        """
-        statement = DropTable(name, database=database, must_exist=not force)
-        self._safe_exec_sql(statement)
-
-    def truncate_table(self, name: str, database: str | None = None) -> None:
-        """Delete all rows from an existing table.
-
-        Parameters
-        ----------
-        name
-            Table name
-        database
-            Database name
-
-        """
-        statement = TruncateTable(name, database=database)
-        self._safe_exec_sql(statement)
-
-    def rename_table(self, old_name: str, new_name: str) -> None:
-        """Rename an existing table.
-
-        Parameters
-        ----------
-        old_name
-            The old name of the table.
-        new_name
-            The new name of the table.
-
-        """
-        statement = RenameTable(old_name, new_name)
-        self._safe_exec_sql(statement)
-
-    def drop_table_or_view(self, name, *, database=None, force=False):
-        """Drop view or table."""
-        try:
-            self.drop_table(name, database=database)
-        except Exception as e:  # noqa: BLE001
-            try:
-                self.drop_view(name, database=database)
-            except Exception:  # noqa: BLE001
-                raise e
-
-    def cache_table(self, table_name, *, database=None, pool="default"):
-        """Caches a table in cluster memory in the given pool.
-
-        Parameters
-        ----------
-        table_name
-            Table name
-        database
-            Database name
-        pool
-            The name of the pool in which to cache the table
-
-        Examples
-        --------
-        >>> table = "my_table"
-        >>> db = "operations"
-        >>> pool = "op_4GB_pool"
-        >>> con.cache_table("my_table", database=db, pool=pool)  # quartodoc: +SKIP # doctest: +SKIP
-
-        """
-        statement = ddl.CacheTable(table_name, database=database, pool=pool)
-        self._safe_exec_sql(statement)
-
-    def create_function(self, func, name=None, database=None):
-        """Create a function within Impala.
-
-        Parameters
-        ----------
-        func
-            UDF or UDAF
-        name
-            Function name
-        database
-            Database name
-
-        """
-        if name is None:
-            name = func.name
-        database = database or self.current_database
-
-        if isinstance(func, udf.ImpalaUDF):
-            stmt = ddl.CreateUDF(func, name=name, database=database)
-        elif isinstance(func, udf.ImpalaUDA):
-            stmt = ddl.CreateUDA(func, name=name, database=database)
-        else:
-            raise TypeError(func)
-        self._safe_exec_sql(stmt)
-
-    def drop_udf(
-        self,
-        name,
-        input_types=None,
-        database=None,
-        force=False,
-        aggregate=False,
-    ):
-        """Drop a UDF.
-
-        If only name is given, this will search for the relevant UDF and drop
-        it. To delete an overloaded UDF, give only a name and force=True
-
-        Parameters
-        ----------
-        name
-            Function name
-        input_types
-            Input types
-        force
-            Must be set to `True` to drop overloaded UDFs
-        database
-            Database name
-        aggregate
-            Whether the function is an aggregate
-
-        """
-        if not input_types:
-            if not database:
-                database = self.current_database
-            result = self.list_udfs(database=database, like=name)
-            if len(result) > 1:
-                if force:
-                    for func in result:
-                        self._drop_single_function(
-                            func.name,
-                            func.inputs,
-                            database=database,
-                            aggregate=aggregate,
-                        )
-                    return
-                else:
-                    raise com.DuplicateUDFError(name)
-            elif len(result) == 1:
-                func = result.pop()
-                self._drop_single_function(
-                    func.name,
-                    func.inputs,
-                    database=database,
-                    aggregate=aggregate,
-                )
-                return
-            else:
-                raise com.MissingUDFError(name)
-        self._drop_single_function(
-            name, input_types, database=database, aggregate=aggregate
-        )
-
-    def drop_uda(self, name, input_types=None, database=None, force=False):
-        """Drop an aggregate function."""
-        return self.drop_udf(
-            name, input_types=input_types, database=database, force=force
-        )
-
-    def _drop_single_function(self, name, input_types, database=None, aggregate=False):
-        stmt = ddl.DropFunction(
-            name,
-            input_types,
-            must_exist=False,
-            aggregate=aggregate,
-            database=database,
-        )
-        self._safe_exec_sql(stmt)
-
-    def _drop_all_functions(self, database):
-        udfs = self.list_udfs(database=database)
-        for fnct in udfs:
-            stmt = ddl.DropFunction(
-                fnct.name,
-                fnct.inputs,
-                must_exist=False,
-                aggregate=False,
-                database=database,
-            )
-            self._safe_exec_sql(stmt)
-        udafs = self.list_udas(database=database)
-        for udaf in udafs:
-            stmt = ddl.DropFunction(
-                udaf.name,
-                udaf.inputs,
-                must_exist=False,
-                aggregate=True,
-                database=database,
-            )
-            self._safe_exec_sql(stmt)
-
-    def list_udfs(self, database=None, like=None):
-        """Lists all UDFs associated with given database."""
-        if not database:
-            database = self.current_database
-        statement = ddl.ListFunction(database, like=like, aggregate=False)
-        with self._safe_raw_sql(statement) as cur:
-            return self._get_udfs(cur)
-
-    def list_udas(self, database=None, like=None):
-        """Lists all UDAFs associated with a given database."""
-        if not database:
-            database = self.current_database
-        statement = ddl.ListFunction(database, like=like, aggregate=True)
-        with self._safe_raw_sql(statement) as cur:
-            return self._get_udfs(cur)
-
-    def _get_udfs(self, cur):
-        rows = fetchall(cur)
-
-        if rows.empty:
-            return []
-
-        current_database = self.current_database
-        type_mapper = self.compiler.type_mapper
-        result = []
-        for return_type, signature, *_ in rows.itertuples(index=False):
-            anon = sg.parse_one(signature)
-            name = anon.this
-            inputs = [
-                type_mapper.from_string(expr.this.this) for expr in anon.expressions
-            ]
-
-            output = type_mapper.from_string(return_type)
-
-            result.append((current_database, name, tuple(inputs), output))
-        return result
-
-    def exists_udf(self, name: str, database: str | None = None) -> bool:
-        """Checks if a given UDF exists within a specified database."""
-        return bool(self.list_udfs(database=database, like=name))
-
-    def exists_uda(self, name: str, database: str | None = None) -> bool:
-        """Checks if a given UDAF exists within a specified database."""
-        return bool(self.list_udas(database=database, like=name))
-
-    def compute_stats(
-        self,
-        name: str,
-        database: str | None = None,
-        incremental: bool = False,
-    ) -> None:
-        """Issue a `COMPUTE STATS` command for a given table.
-
-        Parameters
-        ----------
-        name
-            Can be fully qualified (with database name)
-        database
-            Database name
-        incremental
-            If True, issue COMPUTE INCREMENTAL STATS
-
-        """
-        maybe_inc = "INCREMENTAL " if incremental else ""
-        cmd = f"COMPUTE {maybe_inc}STATS"
-
-        stmt = self._table_command(cmd, name, database=database)
-        self._safe_exec_sql(stmt)
-
-    def invalidate_metadata(
-        self,
-        name: str | None = None,
-        database: str | None = None,
-    ) -> None:
-        """Issue an `INVALIDATE METADATA` command.
-
-        Optionally this applies to a specific table. See Impala documentation.
-
-        Parameters
-        ----------
-        name
-            Table name. Can be fully qualified (with database)
-        database
-            Database name
-
-        """
-        stmt = "INVALIDATE METADATA"
-        if name is not None:
-            stmt = self._table_command(stmt, name, database=database)
-        self._safe_exec_sql(stmt)
-
-    def refresh(self, name: str, database: str | None = None) -> None:
-        """Reload metadata for a table.
-
-        This can be useful after ingesting data as part of an ETL pipeline, for
-        example.
-
-        Related to `INVALIDATE METADATA`. See Impala documentation for more.
-
-        Parameters
-        ----------
-        name
-            Table name. Can be fully qualified (with database)
-        database
-            Database name
-
-        """
-        # TODO(wesm): can this statement be cancelled?
-        stmt = self._table_command("REFRESH", name, database=database)
-        self._safe_exec_sql(stmt)
-
-    def describe_formatted(
-        self,
-        name: str,
-        database: str | None = None,
-    ) -> pd.DataFrame:
-        """Retrieve the results of a `DESCRIBE FORMATTED` command.
-
-        See Impala documentation for more.
-
-        Parameters
-        ----------
-        name
-            Table name. Can be fully qualified (with database)
-        database
-            Database name
-
-        """
-        from ibis.backends.impala.metadata import parse_metadata
-
-        stmt = self._table_command("DESCRIBE FORMATTED", name, database=database)
-        result = self._exec_statement(stmt)
-
-        # Leave formatting to pandas
-        for c in result.columns:
-            result[c] = result[c].str.strip()
-
-        return parse_metadata(result)
-
-    def show_files(
-        self,
-        name: str,
-        database: str | None = None,
-    ) -> pd.DataFrame:
-        """Retrieve results of a `SHOW FILES` command for a table.
-
-        See Impala documentation for more.
-
-        Parameters
-        ----------
-        name
-            Table name. Can be fully qualified (with database)
-        database
-            Database name
-
-        """
-        stmt = self._table_command("SHOW FILES IN", name, database=database)
-        return self._exec_statement(stmt)
-
-    def list_partitions(self, name, database=None):
-        stmt = self._table_command("SHOW PARTITIONS", name, database=database)
-        return self._exec_statement(stmt)
-
-    def table_stats(self, name, database=None):
-        """Return results of `SHOW TABLE STATS` for the table `name`."""
-        stmt = self._table_command("SHOW TABLE STATS", name, database=database)
-        return self._exec_statement(stmt)
-
-    def column_stats(self, name, database=None):
-        """Return results of `SHOW COLUMN STATS` for the table `name`."""
-        stmt = self._table_command("SHOW COLUMN STATS", name, database=database)
-        return self._exec_statement(stmt)
-
-    def _exec_statement(self, stmt):
-        with self._safe_raw_sql(stmt) as cur:
-            return fetchall(cur)
-
-    def _table_command(self, cmd, name, database=None):
-        qualified_name = self._fully_qualified_name(name, database)
-        return f"{cmd} {qualified_name}"
-
-    def to_pyarrow(
-        self,
-        expr: ir.Expr,
-        params: Mapping[ir.Scalar, Any] | None = None,
-        limit: int | str | None = None,
-        **kwargs: Any,
-    ) -> pa.Table:
-        import pyarrow as pa
-        import pyarrow_hotfix  # noqa: F401
-
-        from ibis.formats.pyarrow import PyArrowData
-
-        self._run_pre_execute_hooks(expr)
-
-        table_expr = expr.as_table()
-        output = pa.Table.from_pandas(
-            self.execute(table_expr, params=params, limit=limit, **kwargs),
-            preserve_index=False,
-        )
-        table = PyArrowData.convert_table(output, table_expr.schema())
-        return expr.__pyarrow_result__(table)
-
-    def to_pyarrow_batches(
-        self,
-        expr: ir.Expr,
-        *,
-        params: Mapping[ir.Scalar, Any] | None = None,
-        limit: int | str | None = None,
-        chunk_size: int = 1000000,
-        **kwargs: Any,
-    ) -> pa.ipc.RecordBatchReader:
-        pa = self._import_pyarrow()
-        self._run_pre_execute_hooks(expr)
-
-        pa_table = self.to_pyarrow(
-            expr.as_table(), params=params, limit=limit, **kwargs
-        )
-        return pa.ipc.RecordBatchReader.from_batches(
-            pa_table.schema, pa_table.to_batches(max_chunksize=chunk_size)
-        )
-
-    def explain(
-        self, expr: ir.Expr | str, params: Mapping[ir.Expr, Any] | None = None
-    ) -> str:
-        """Explain an expression.
-
-        Return the query plan associated with the indicated expression or SQL
-        query.
-
-        Returns
-        -------
-        str
-            Query plan
-
-        """
-        query = self.compile(expr, params=params)
-        statement = f"EXPLAIN {query}"
-
-        with self._safe_raw_sql(statement) as cur:
-            results = fetchall(cur)
-
-        return "\n".join(["Query:", util.indent(query, 2), "", *results.iloc[:, 0]])
-
-    def _register_in_memory_table(self, op: ops.InMemoryTable) -> None:
-        schema = op.schema
-        if null_columns := [col for col, dtype in schema.items() if dtype.is_null()]:
-            raise com.IbisTypeError(
-                "Impala cannot yet reliably handle `null` typed columns; "
-                f"got null typed columns: {null_columns}"
-            )
-
-        # only register if we haven't already done so
-        if (name := op.name) not in self.list_tables():
-            type_mapper = self.compiler.type_mapper
-            quoted = self.compiler.quoted
-            column_defs = [
-                sg.exp.ColumnDef(
-                    this=sg.to_identifier(colname, quoted=quoted),
-                    kind=type_mapper.from_ibis(typ),
-                    # we don't support `NOT NULL` constraints in trino because
-                    # because each trino connector differs in whether it
-                    # supports nullability constraints, and whether the
-                    # connector supports it isn't visible to ibis via a
-                    # metadata query
-                )
-                for colname, typ in schema.items()
-            ]
-
-            create_stmt = sg.exp.Create(
-                kind="TABLE",
-                this=sg.exp.Schema(
-                    this=sg.to_identifier(name, quoted=quoted), expressions=column_defs
-                ),
-            ).sql(self.name, pretty=True)
-
-            data = op.data.to_frame().itertuples(index=False)
-            specs = ", ".join("?" * len(schema))
-            table = sg.table(name, quoted=quoted).sql(self.name)
-            insert_stmt = f"INSERT INTO {table} VALUES ({specs})"
-            with self._safe_raw_sql(create_stmt) as cur:
-                for row in data:
-                    cur.execute(insert_stmt, row)
-
-
-def fetchall(cur):
-    batches = cur.fetchcolumnar()
-    names = list(map(operator.itemgetter(0), cur.description))
-    df = _column_batches_to_dataframe(names, batches)
-    return df
-
-
-def _column_batches_to_dataframe(names, batches):
-    import pandas as pd
-
-    cols = {
-        name: _chunks_to_pandas_array(chunks)
-        for name, chunks in zip(names, zip(*(b.columns for b in batches)))
-    }
-
-    return pd.DataFrame(cols, columns=names)
-
-
-_HS2_TTypeId_to_dtype = {
-    "BOOLEAN": "bool",
-    "TINYINT": "int8",
-    "SMALLINT": "int16",
-    "INT": "int32",
-    "BIGINT": "int64",
-    "TIMESTAMP": "datetime64[ns]",
-    "FLOAT": "float32",
-    "DOUBLE": "float64",
-    "STRING": "object",
-    "DECIMAL": "object",
-    "BINARY": "object",
-    "VARCHAR": "object",
-    "CHAR": "object",
-    "DATE": "datetime64[ns]",
-    "VOID": None,
-}
-
-
-def _chunks_to_pandas_array(chunks):
-    import numpy as np
-
-    total_length = 0
-    have_nulls = False
-    for c in chunks:
-        total_length += len(c)
-        have_nulls = have_nulls or c.nulls.any()
-
-    type_ = chunks[0].data_type
-    numpy_type = _HS2_TTypeId_to_dtype[type_]
-
-    def fill_nonnull(target, chunks):
-        pos = 0
-        for c in chunks:
-            target[pos : pos + len(c)] = c.values
-            pos += len(c.values)
-
-    def fill(target, chunks, na_rep):
-        pos = 0
-        for c in chunks:
-            nulls = c.nulls.copy()
-            nulls.bytereverse()
-            bits = np.frombuffer(nulls.tobytes(), dtype="u1")
-            mask = np.unpackbits(bits).view(np.bool_)
-
-            k = len(c)
-
-            dest = target[pos : pos + k]
-            dest[:] = c.values
-            dest[mask[:k]] = na_rep
-
-            pos += k
-
-    if have_nulls:
-        if numpy_type in ("bool", "datetime64[ns]"):
-            target = np.empty(total_length, dtype="O")
-            na_rep = np.nan
-        elif numpy_type.startswith("int"):
-            target = np.empty(total_length, dtype="f8")
-            na_rep = np.nan
-        else:
-            target = np.empty(total_length, dtype=numpy_type)
-            na_rep = np.nan
-
-        fill(target, chunks, na_rep)
-    else:
-        target = np.empty(total_length, dtype=numpy_type)
-        fill_nonnull(target, chunks)
+import ibis.common as com
+import ibis.config as config
+
+
+from ibis.tests.util import assert_equal
+
+
+class TestTableExprBasics(BasicTestCase, unittest.TestCase):
+
+    def test_empty_schema(self):
+        table = api.table([], 'foo')
+        assert len(table.schema()) == 0
+
+    def test_columns(self):
+        t = self.con.table('alltypes')
+        result = t.columns
+        expected = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i']
+        assert result == expected
+
+    def test_view_new_relation(self):
+        # For assisting with self-joins and other self-referential operations
+        # where we need to be able to treat instances of the same TableExpr as
+        # semantically distinct
+        #
+        # This thing is not exactly a projection, since it has no semantic
+        # meaning when it comes to execution
+        tview = self.table.view()
+
+        roots = tview._root_tables()
+        assert len(roots) == 1
+        assert roots[0] is tview.op()
+
+    def test_get_type(self):
+        for k, v in self.schema_dict.items():
+            assert self.table._get_type(k) == v
+
+    def test_getitem_column_select(self):
+        for k, v in self.schema_dict.items():
+            col = self.table[k]
+
+            # Make sure it's the right type
+            assert isinstance(col, ArrayExpr)
+            assert isinstance(col, array_type(v))
+
+            # Ensure we have a field selection with back-reference to the table
+            parent = col.parent()
+            assert isinstance(parent, ops.TableColumn)
+            assert parent.parent() is self.table
+
+    def test_getitem_attribute(self):
+        result = self.table.a
+        assert_equal(result, self.table['a'])
 
-    return target
+        assert 'a' in dir(self.table)
+
+        # Project and add a name that conflicts with a TableExpr built-in
+        # attribute
+        view = self.table[[self.table, self.table['a'].name('schema')]]
+        assert not isinstance(view.schema, ArrayExpr)
+
+    def test_projection(self):
+        cols = ['f', 'a', 'h']
+
+        proj = self.table[cols]
+        assert isinstance(proj, TableExpr)
+        assert isinstance(proj.op(), ops.Projection)
+
+        assert proj.schema().names == cols
+        for c in cols:
+            expr = proj[c]
+            assert isinstance(expr, type(self.table[c]))
+
+    def test_projection_no_list(self):
+        expr = (self.table.f * 2).name('bar')
+        result = self.table.select(expr)
+        expected = self.table.projection([expr])
+        assert_equal(result, expected)
+
+    def test_projection_with_exprs(self):
+        # unnamed expr to test
+        mean_diff = (self.table['a'] - self.table['c']).mean()
+
+        col_exprs = [self.table['b'].log().name('log_b'),
+                     mean_diff.name('mean_diff')]
+
+        proj = self.table[col_exprs + ['g']]
+        schema = proj.schema()
+        assert schema.names == ['log_b', 'mean_diff', 'g']
+        assert schema.types == ['double', 'double', 'string']
+
+        # Test with unnamed expr
+        self.assertRaises(ExpressionError, self.table.projection,
+                          ['g', self.table['a'] - self.table['c']])
+
+    def test_projection_duplicate_names(self):
+        self.assertRaises(com.IntegrityError, self.table.projection,
+                          [self.table.c, self.table.c])
+
+    def test_projection_invalid_root(self):
+        schema1 = {
+            'foo': 'double',
+            'bar': 'int32'
+        }
+
+        left = api.table(schema1, name='foo')
+        right = api.table(schema1, name='bar')
+
+        exprs = [right['foo'], right['bar']]
+        self.assertRaises(RelationError, left.projection, exprs)
+
+    def test_projection_unnamed_literal_interactive_blowup(self):
+        # #147 and #153 alike
+        table = self.con.table('functional_alltypes')
+
+        with config.option_context('interactive', True):
+            try:
+                table.select([table.bigint_col, ibis.literal(5)])
+            except Exception as e:
+                assert 'named' in e.args[0]
+
+    def test_projection_of_aggregated(self):
+        # Fully-formed aggregations "block"; in a projection, column
+        # expressions referencing table expressions below the aggregation are
+        # invalid.
+        pass
+
+    def test_projection_with_star_expr(self):
+        new_expr = (self.table['a'] * 5).name('bigger_a')
+
+        t = self.table
+
+        # it lives!
+        proj = t[t, new_expr]
+        repr(proj)
+
+        ex_names = self.table.schema().names + ['bigger_a']
+        assert proj.schema().names == ex_names
+
+        # cannot pass an invalid table expression
+        t2 = t.aggregate([t['a'].sum().name('sum(a)')], by=['g'])
+        self.assertRaises(RelationError, t.__getitem__, [t2])
+
+        # TODO: there may be some ways this can be invalid
+
+    def test_projection_convenient_syntax(self):
+        proj = self.table[self.table, self.table['a'].name('foo')]
+        proj2 = self.table[[self.table, self.table['a'].name('foo')]]
+        assert_equal(proj, proj2)
+
+    def test_projection_mutate_analysis_bug(self):
+        # GH #549
+
+        t = self.con.table('airlines')
+
+        # it works!
+        (t[t.depdelay.notnull()]
+         .mutate(leg=ibis.literal('-').join([t.origin, t.dest]))
+         ['year', 'month', 'day', 'depdelay', 'leg'])
+
+    def test_projection_self(self):
+        result = self.table[self.table]
+        expected = self.table.projection(self.table)
+
+        assert_equal(result, expected)
+
+    def test_projection_array_expr(self):
+        result = self.table[self.table.a]
+        expected = self.table[[self.table.a]]
+        assert_equal(result, expected)
+
+    def test_add_column(self):
+        # Creates a projection with a select-all on top of a non-projection
+        # TableExpr
+        new_expr = (self.table['a'] * 5).name('bigger_a')
+
+        t = self.table
+
+        result = t.add_column(new_expr)
+        expected = t[[t, new_expr]]
+        assert_equal(result, expected)
+
+        result = t.add_column(new_expr, 'wat')
+        expected = t[[t, new_expr.name('wat')]]
+        assert_equal(result, expected)
+
+    def test_add_column_scalar_expr(self):
+        # Check literals, at least
+        pass
+
+    def test_add_column_aggregate_crossjoin(self):
+        # A new column that depends on a scalar value produced by this or some
+        # other table.
+        #
+        # For example:
+        # SELECT *, b - VAL
+        # FROM table1
+        #
+        # Here, VAL could be something produced by aggregating table1 or any
+        # other table for that matter.
+        pass
+
+    def test_add_column_existing_projection(self):
+        # The "blocking" predecessor table is a projection; we can simply add
+        # the column to the existing projection
+        foo = (self.table.f * 2).name('foo')
+        bar = (self.table.f * 4).name('bar')
+        t2 = self.table.add_column(foo)
+        t3 = t2.add_column(bar)
+
+        expected = self.table[self.table, foo, bar]
+        assert_equal(t3, expected)
+
+    def test_mutate(self):
+        one = self.table.f * 2
+        foo = (self.table.a + self.table.b).name('foo')
+
+        expr = self.table.mutate(foo, one=one, two=2)
+        expected = self.table[self.table, foo, one.name('one'),
+                              ibis.literal(2).name('two')]
+        assert_equal(expr, expected)
+
+    def test_mutate_alter_existing_columns(self):
+        new_f = self.table.f * 2
+        foo = self.table.d * 2
+        expr = self.table.mutate(f=new_f, foo=foo)
+
+        expected = self.table['a', 'b', 'c', 'd', 'e',
+                              new_f.name('f'), 'g', 'h',
+                              foo.name('foo')]
+
+        assert_equal(expr, expected)
+
+    def test_replace_column(self):
+        tb = api.table([
+            ('a', 'int32'),
+            ('b', 'double'),
+            ('c', 'string')
+        ])
+
+        expr = tb.b.cast('int32')
+        tb2 = tb.set_column('b', expr)
+        expected = tb[tb.a, expr.name('b'), tb.c]
+
+        assert_equal(tb2, expected)
+
+    def test_filter_no_list(self):
+        pred = self.table.a > 5
+
+        result = self.table.filter(pred)
+        expected = self.table[pred]
+        assert_equal(result, expected)
+
+    def test_add_predicate(self):
+        pred = self.table['a'] > 5
+        result = self.table[pred]
+        assert isinstance(result.op(), ops.Filter)
+
+    def test_filter_root_table_preserved(self):
+        result = self.table[self.table['a'] > 5]
+        roots = result.op().root_tables()
+        assert roots[0] is self.table.op()
+
+    def test_invalid_predicate(self):
+        # a lookalike
+        table2 = api.table(self.schema, name='bar')
+        self.assertRaises(RelationError, self.table.__getitem__,
+                          table2['a'] > 5)
+
+    def test_add_predicate_coalesce(self):
+        # Successive predicates get combined into one rather than nesting. This
+        # is mainly to enhance readability since we could handle this during
+        # expression evaluation anyway.
+        pred1 = self.table['a'] > 5
+        pred2 = self.table['b'] > 0
+
+        result = self.table[pred1][pred2]
+        expected = self.table.filter([pred1, pred2])
+        assert_equal(result, expected)
+
+        # 59, if we are not careful, we can obtain broken refs
+        interm = self.table[pred1]
+        result = interm.filter([interm['b'] > 0])
+        assert_equal(result, expected)
+
+    def test_repr_same_but_distinct_objects(self):
+        t = self.con.table('test1')
+        t_copy = self.con.table('test1')
+        table2 = t[t_copy['f'] > 0]
+
+        result = repr(table2)
+        assert result.count('DatabaseTable') == 1
+
+    def test_filter_fusion_distinct_table_objects(self):
+        t = self.con.table('test1')
+        tt = self.con.table('test1')
+
+        expr = t[t.f > 0][t.c > 0]
+        expr2 = t[t.f > 0][tt.c > 0]
+        expr3 = t[tt.f > 0][tt.c > 0]
+        expr4 = t[tt.f > 0][t.c > 0]
+
+        assert_equal(expr, expr2)
+        assert repr(expr) == repr(expr2)
+        assert_equal(expr, expr3)
+        assert_equal(expr, expr4)
+
+    def test_column_relabel(self):
+        # GH #551. Keeping the test case very high level to not presume that
+        # the relabel is necessarily implemented using a projection
+        types = ['int32', 'string', 'double']
+        table = api.table(zip(['foo', 'bar', 'baz'], types))
+        result = table.relabel({'foo': 'one', 'baz': 'three'})
+
+        schema = result.schema()
+        ex_schema = api.schema(zip(['one', 'bar', 'three'], types))
+        assert_equal(schema, ex_schema)
+
+    def test_limit(self):
+        limited = self.table.limit(10, offset=5)
+        assert limited.op().n == 10
+        assert limited.op().offset == 5
+
+    def test_sort_by(self):
+        # Commit to some API for ascending and descending
+        #
+        # table.sort_by(['g', expr1, desc(expr2), desc(expr3)])
+        #
+        # Default is ascending for anything coercable to an expression,
+        # and we'll have ascending/descending wrappers to help.
+        result = self.table.sort_by(['f'])
+        sort_key = result.op().keys[0].op()
+        assert_equal(sort_key.expr, self.table.f)
+        assert sort_key.ascending
+
+        # non-list input. per #150
+        result2 = self.table.sort_by('f')
+        assert_equal(result, result2)
+
+        result2 = self.table.sort_by([('f', False)])
+        result3 = self.table.sort_by([('f', 'descending')])
+        result4 = self.table.sort_by([('f', 0)])
+
+        key2 = result2.op().keys[0].op()
+        key3 = result3.op().keys[0].op()
+        key4 = result4.op().keys[0].op()
+
+        assert not key2.ascending
+        assert not key3.ascending
+        assert not key4.ascending
+        assert_equal(result2, result3)
+
+    def test_sort_by_desc_deferred_sort_key(self):
+        result = (self.table.group_by('g')
+                  .size()
+                  .sort_by(ibis.desc('count')))
+
+        tmp = self.table.group_by('g').size()
+        expected = tmp.sort_by((tmp['count'], False))
+        expected2 = tmp.sort_by(ibis.desc(tmp['count']))
+
+        assert_equal(result, expected)
+        assert_equal(result, expected2)
+
+    def test_slice_convenience(self):
+        expr = self.table[:5]
+        expr2 = self.table[:5:1]
+        assert_equal(expr, self.table.limit(5))
+        assert_equal(expr, expr2)
+
+        expr = self.table[2:7]
+        expr2 = self.table[2:7:1]
+        assert_equal(expr, self.table.limit(5, offset=2))
+        assert_equal(expr, expr2)
+
+        self.assertRaises(ValueError, self.table.__getitem__, slice(2, 15, 2))
+        self.assertRaises(ValueError, self.table.__getitem__, slice(5, None))
+        self.assertRaises(ValueError, self.table.__getitem__, slice(None, -5))
+        self.assertRaises(ValueError, self.table.__getitem__, slice(-10, -5))
+
+
+class TestAggregation(BasicTestCase, unittest.TestCase):
+
+    def test_count(self):
+        result = self.table['a'].count()
+        assert isinstance(result, api.Int64Scalar)
+        assert isinstance(result.op(), ops.Count)
+
+    def test_table_count(self):
+        result = self.table.count()
+        assert isinstance(result, api.Int64Scalar)
+        assert isinstance(result.op(), ops.Count)
+        assert result.get_name() == 'count'
+
+    def test_len_raises_expression_error(self):
+        with self.assertRaises(com.ExpressionError):
+            len(self.table)
+
+    def test_sum_expr_basics(self):
+        # Impala gives bigint for all integer types
+        ex_class = api.Int64Scalar
+        for c in self.int_cols + self.bool_cols:
+            result = self.table[c].sum()
+            assert isinstance(result, ex_class)
+            assert isinstance(result.op(), ops.Sum)
+
+        # Impala gives double for all floating point types
+        ex_class = api.DoubleScalar
+        for c in self.float_cols:
+            result = self.table[c].sum()
+            assert isinstance(result, ex_class)
+            assert isinstance(result.op(), ops.Sum)
+
+    def test_mean_expr_basics(self):
+        cols = self.int_cols + self.float_cols + self.bool_cols
+        for c in cols:
+            result = self.table[c].mean()
+            assert isinstance(result, api.DoubleScalar)
+            assert isinstance(result.op(), ops.Mean)
+
+    def test_aggregate_no_keys(self):
+        agg_exprs = [self.table['a'].sum().name('sum(a)'),
+                     self.table['c'].mean().name('mean(c)')]
+
+        # A TableExpr, which in SQL at least will yield a table with a single
+        # row
+        result = self.table.aggregate(agg_exprs)
+        assert isinstance(result, TableExpr)
+
+    def test_aggregate_keys_basic(self):
+        agg_exprs = [self.table['a'].sum().name('sum(a)'),
+                     self.table['c'].mean().name('mean(c)')]
+
+        # A TableExpr, which in SQL at least will yield a table with a single
+        # row
+        result = self.table.aggregate(agg_exprs, by=['g'])
+        assert isinstance(result, TableExpr)
+
+        # it works!
+        repr(result)
+
+    def test_aggregate_non_list_inputs(self):
+        # per #150
+        metric = self.table.f.sum().name('total')
+        by = 'g'
+        having = self.table.c.sum() > 10
+
+        result = self.table.aggregate(metric, by=by, having=having)
+        expected = self.table.aggregate([metric], by=[by], having=[having])
+        assert_equal(result, expected)
+
+    def test_aggregate_keywords(self):
+        t = self.table
+
+        expr = t.aggregate(foo=t.f.sum(), bar=lambda x: x.f.mean(),
+                           by='g')
+        expr2 = t.group_by('g').aggregate(foo=t.f.sum(),
+                                          bar=lambda x: x.f.mean())
+        expected = t.aggregate([t.f.mean().name('bar'),
+                                t.f.sum().name('foo')], by='g')
+
+        assert_equal(expr, expected)
+        assert_equal(expr2, expected)
+
+    def test_summary_expand_list(self):
+        summ = self.table.f.summary()
+
+        metric = self.table.g.group_concat().name('bar')
+        result = self.table.aggregate([metric, summ])
+        expected = self.table.aggregate([metric] + summ.exprs())
+        assert_equal(result, expected)
+
+    def test_aggregate_invalid(self):
+        # Pass a non-aggregation or non-scalar expr
+        pass
+
+    def test_filter_aggregate_pushdown_predicate(self):
+        # In the case where we want to add a predicate to an aggregate
+        # expression after the fact, rather than having to backpedal and add it
+        # before calling aggregate.
+        #
+        # TODO (design decision): This could happen automatically when adding a
+        # predicate originating from the same root table; if an expression is
+        # created from field references from the aggregated table then it
+        # becomes a filter predicate applied on top of a view
+
+        pred = self.table.f > 0
+        metrics = [self.table.a.sum().name('total')]
+        agged = self.table.aggregate(metrics, by=['g'])
+        filtered = agged.filter([pred])
+        expected = self.table[pred].aggregate(metrics, by=['g'])
+        assert_equal(filtered, expected)
+
+    def test_filter_aggregate_partial_pushdown(self):
+        pass
+
+    def test_aggregate_post_predicate(self):
+        # Test invalid having clause
+        metrics = [self.table.f.sum().name('total')]
+        by = ['g']
+
+        invalid_having_cases = [
+            self.table.f.sum(),
+            self.table.f > 2
+        ]
+        for case in invalid_having_cases:
+            self.assertRaises(com.ExpressionError, self.table.aggregate,
+                              metrics, by=by, having=[case])
+
+    def test_group_by_having_api(self):
+        # #154, add a HAVING post-predicate in a composable way
+        metric = self.table.f.sum().name('foo')
+        postp = self.table.d.mean() > 1
+
+        expr = (self.table
+                .group_by('g')
+                .having(postp)
+                .aggregate(metric))
+
+        expected = self.table.aggregate(metric, by='g', having=postp)
+        assert_equal(expr, expected)
+
+    def test_aggregate_root_table_internal(self):
+        pass
+
+    def test_compound_aggregate_expr(self):
+        # See ibis #24
+        compound_expr = (self.table['a'].sum() /
+                         self.table['a'].mean()).name('foo')
+        assert ops.is_reduction(compound_expr)
+
+        # Validates internally
+        self.table.aggregate([compound_expr])
+
+    def test_groupby_convenience(self):
+        metrics = [self.table.f.sum().name('total')]
+
+        expr = self.table.group_by('g').aggregate(metrics)
+        expected = self.table.aggregate(metrics, by=['g'])
+        assert_equal(expr, expected)
+
+        group_expr = self.table.g.cast('double').name('g')
+        expr = self.table.group_by(group_expr).aggregate(metrics)
+        expected = self.table.aggregate(metrics, by=[group_expr])
+        assert_equal(expr, expected)
+
+    def test_group_by_count_size(self):
+        # #148, convenience for interactive use, and so forth
+        result1 = self.table.group_by('g').size()
+        result2 = self.table.group_by('g').count()
+
+        expected = (self.table.group_by('g')
+                    .aggregate([self.table.count().name('count')]))
+
+        assert_equal(result1, expected)
+        assert_equal(result2, expected)
+
+        result = self.table.group_by('g').count('foo')
+        expected = (self.table.group_by('g')
+                    .aggregate([self.table.count().name('foo')]))
+        assert_equal(result, expected)
+
+    def test_group_by_column_select_api(self):
+        grouped = self.table.group_by('g')
+
+        result = grouped.f.sum()
+        expected = grouped.aggregate(self.table.f.sum().name('sum(f)'))
+        assert_equal(result, expected)
+
+        supported_functions = ['sum', 'mean', 'count', 'size', 'max', 'min']
+
+        # make sure they all work
+        for fn in supported_functions:
+            getattr(grouped.f, fn)()
+
+    def test_value_counts_convenience(self):
+        # #152
+        result = self.table.g.value_counts()
+        expected = (self.table.group_by('g')
+                    .aggregate(self.table.count().name('count')))
+
+        assert_equal(result, expected)
+
+    def test_isin_value_counts(self):
+        # #157, this code path was untested before
+        bool_clause = self.table.g.notin(['1', '4', '7'])
+        # it works!
+        bool_clause.name('notin').value_counts()
+
+    def test_value_counts_unnamed_expr(self):
+        nation = self.con.table('tpch_nation')
+
+        expr = nation.n_name.lower().value_counts()
+        expected = nation.n_name.lower().name('unnamed').value_counts()
+        assert_equal(expr, expected)
+
+    def test_aggregate_unnamed_expr(self):
+        nation = self.con.table('tpch_nation')
+        expr = nation.n_name.lower().left(1)
+        self.assertRaises(com.ExpressionError, nation.group_by(expr).aggregate,
+                          nation.count().name('metric'))
+
+    def test_default_reduction_names(self):
+        d = self.table.f
+        cases = [
+            (d.count(), 'count'),
+            (d.sum(), 'sum'),
+            (d.mean(), 'mean'),
+            (d.approx_nunique(), 'approx_nunique'),
+            (d.approx_median(), 'approx_median'),
+            (d.min(), 'min'),
+            (d.max(), 'max')
+        ]
+
+        for expr, ex_name in cases:
+            assert expr.get_name() == ex_name
+
+
+class TestJoinsUnions(BasicTestCase, unittest.TestCase):
+
+    def test_join_no_predicate_list(self):
+        region = self.con.table('tpch_region')
+        nation = self.con.table('tpch_nation')
+
+        pred = region.r_regionkey == nation.n_regionkey
+        joined = region.inner_join(nation, pred)
+        expected = region.inner_join(nation, [pred])
+        assert_equal(joined, expected)
+
+    def test_equijoin_schema_merge(self):
+        table1 = ibis.table([('key1',  'string'), ('value1', 'double')])
+        table2 = ibis.table([('key2',  'string'), ('stuff', 'int32')])
+
+        pred = table1['key1'] == table2['key2']
+        join_types = ['inner_join', 'left_join', 'outer_join']
+
+        ex_schema = api.Schema(['key1', 'value1', 'key2', 'stuff'],
+                               ['string', 'double', 'string', 'int32'])
+
+        for fname in join_types:
+            f = getattr(table1, fname)
+            joined = f(table2, [pred]).materialize()
+            assert_equal(joined.schema(), ex_schema)
+
+    def test_join_combo_with_projection(self):
+        # Test a case where there is column name overlap, but the projection
+        # passed makes it a non-issue. Highly relevant with self-joins
+        #
+        # For example, where left/right have some field names in common:
+        # SELECT left.*, right.a, right.b
+        # FROM left join right on left.key = right.key
+        t = self.table
+        t2 = t.add_column(t['f'] * 2, 'foo')
+        t2 = t2.add_column(t['f'] * 4, 'bar')
+
+        # this works
+        joined = t.left_join(t2, [t['g'] == t2['g']])
+        proj = joined.projection([t, t2['foo'], t2['bar']])
+        repr(proj)
+
+    def test_join_getitem_projection(self):
+        region = self.con.table('tpch_region')
+        nation = self.con.table('tpch_nation')
+
+        pred = region.r_regionkey == nation.n_regionkey
+        joined = region.inner_join(nation, pred)
+
+        result = joined[nation]
+        expected = joined.projection(nation)
+        assert_equal(result, expected)
+
+    def test_self_join(self):
+        # Self-joins are problematic with this design because column
+        # expressions may reference either the left or right self. For example:
+        #
+        # SELECT left.key, sum(left.value - right.value) as total_deltas
+        # FROM table left
+        #  INNER JOIN table right
+        #    ON left.current_period = right.previous_period + 1
+        # GROUP BY 1
+        #
+        # One way around the self-join issue is to force the user to add
+        # prefixes to the joined fields, then project using those. Not that
+        # satisfying, though.
+        left = self.table
+        right = self.table.view()
+        metric = (left['a'] - right['b']).mean().name('metric')
+
+        joined = left.inner_join(right, [right['g'] == left['g']])
+        # basic check there's no referential problems
+        result_repr = repr(joined)
+        assert 'ref_0' in result_repr
+        assert 'ref_1' in result_repr
+
+        # Cannot be immediately materialized because of the schema overlap
+        self.assertRaises(RelationError, joined.materialize)
+
+        # Project out left table schema
+        proj = joined[[left]]
+        assert_equal(proj.schema(), left.schema())
+
+        # Try aggregating on top of joined
+        aggregated = joined.aggregate([metric], by=[left['g']])
+        ex_schema = api.Schema(['g', 'metric'], ['string', 'double'])
+        assert_equal(aggregated.schema(), ex_schema)
+
+    def test_self_join_no_view_convenience(self):
+        # #165, self joins ought to be possible when the user specifies the
+        # column names to join on rather than referentially-valid expressions
+
+        result = self.table.join(self.table, [('g', 'g')])
+
+        t2 = self.table.view()
+        expected = self.table.join(t2, self.table.g == t2.g)
+        assert_equal(result, expected)
+
+    def test_materialized_join_reference_bug(self):
+        # GH#403
+        orders = self.con.table('tpch_orders')
+        customer = self.con.table('tpch_customer')
+        lineitem = self.con.table('tpch_lineitem')
+
+        items = (orders
+                 .join(lineitem, orders.o_orderkey == lineitem.l_orderkey)
+                 [lineitem, orders.o_custkey, orders.o_orderpriority]
+                 .join(customer, [('o_custkey', 'c_custkey')])
+                 .materialize())
+        items['o_orderpriority'].value_counts()
+
+    def test_join_project_after(self):
+        # e.g.
+        #
+        # SELECT L.foo, L.bar, R.baz, R.qux
+        # FROM table1 L
+        #   INNER JOIN table2 R
+        #     ON L.key = R.key
+        #
+        # or
+        #
+        # SELECT L.*, R.baz
+        # ...
+        #
+        # The default for a join is selecting all fields if possible
+        table1 = ibis.table([('key1',  'string'), ('value1', 'double')])
+        table2 = ibis.table([('key2',  'string'), ('stuff', 'int32')])
+
+        pred = table1['key1'] == table2['key2']
+
+        joined = table1.left_join(table2, [pred])
+        projected = joined.projection([table1, table2['stuff']])
+        assert projected.schema().names == ['key1', 'value1', 'stuff']
+
+        projected = joined.projection([table2, table1['key1']])
+        assert projected.schema().names == ['key2', 'stuff', 'key1']
+
+    def test_semi_join_schema(self):
+        # A left semi join discards the schema of the right table
+        table1 = ibis.table([('key1',  'string'), ('value1', 'double')])
+        table2 = ibis.table([('key2',  'string'), ('stuff', 'double')])
+
+        pred = table1['key1'] == table2['key2']
+        semi_joined = table1.semi_join(table2, [pred]).materialize()
+
+        result_schema = semi_joined.schema()
+        assert_equal(result_schema, table1.schema())
+
+    def test_cross_join(self):
+        agg_exprs = [self.table['a'].sum().name('sum_a'),
+                     self.table['b'].mean().name('mean_b')]
+        scalar_aggs = self.table.aggregate(agg_exprs)
+
+        joined = self.table.cross_join(scalar_aggs).materialize()
+        agg_schema = api.Schema(['sum_a', 'mean_b'], ['int64', 'double'])
+        ex_schema = self.table.schema().append(agg_schema)
+        assert_equal(joined.schema(), ex_schema)
+
+    def test_cross_join_multiple(self):
+        a = self.table['a', 'b', 'c']
+        b = self.table['d', 'e']
+        c = self.table['f', 'h']
+
+        joined = ibis.cross_join(a, b, c)
+        expected = a.cross_join(b.cross_join(c))
+        assert joined.equals(expected)
+
+    def test_join_compound_boolean_predicate(self):
+        # The user might have composed predicates through logical operations
+        pass
+
+    def test_filter_join_unmaterialized(self):
+        table1 = ibis.table({'key1': 'string', 'key2': 'string',
+                            'value1': 'double'})
+        table2 = ibis.table({'key3': 'string', 'value2': 'double'})
+
+        # It works!
+        joined = table1.inner_join(table2, [table1['key1'] == table2['key3']])
+        filtered = joined.filter([table1.value1 > 0])
+        repr(filtered)
+
+    def test_join_overlapping_column_names(self):
+        t1 = ibis.table([('foo', 'string'),
+                         ('bar', 'string'),
+                         ('value1', 'double')])
+        t2 = ibis.table([('foo', 'string'),
+                         ('bar', 'string'),
+                         ('value2', 'double')])
+
+        joined = t1.join(t2, 'foo')
+        expected = t1.join(t2, t1.foo == t2.foo)
+        assert_equal(joined, expected)
+
+        joined = t1.join(t2, ['foo', 'bar'])
+        expected = t1.join(t2, [t1.foo == t2.foo,
+                                t1.bar == t2.bar])
+        assert_equal(joined, expected)
+
+    def test_join_key_alternatives(self):
+        t1 = self.con.table('star1')
+        t2 = self.con.table('star2')
+
+        # Join with tuples
+        joined = t1.inner_join(t2, [('foo_id', 'foo_id')])
+        joined2 = t1.inner_join(t2, [(t1.foo_id, t2.foo_id)])
+
+        # Join with single expr
+        joined3 = t1.inner_join(t2, t1.foo_id == t2.foo_id)
+
+        expected = t1.inner_join(t2, [t1.foo_id == t2.foo_id])
+
+        assert_equal(joined, expected)
+        assert_equal(joined2, expected)
+        assert_equal(joined3, expected)
+
+        self.assertRaises(com.ExpressionError, t1.inner_join, t2,
+                          [('foo_id', 'foo_id', 'foo_id')])
+
+    def test_join_invalid_refs(self):
+        t1 = self.con.table('star1')
+        t2 = self.con.table('star2')
+        t3 = self.con.table('star3')
+
+        predicate = t1.bar_id == t3.bar_id
+        self.assertRaises(com.RelationError, t1.inner_join, t2, [predicate])
+
+    def test_join_non_boolean_expr(self):
+        t1 = self.con.table('star1')
+        t2 = self.con.table('star2')
+
+        # oops
+        predicate = t1.f * t2.value1
+        self.assertRaises(com.ExpressionError, t1.inner_join, t2, [predicate])
+
+    def test_unravel_compound_equijoin(self):
+        t1 = ibis.table([
+            ('key1', 'string'),
+            ('key2', 'string'),
+            ('key3', 'string'),
+            ('value1', 'double')
+        ], 'foo_table')
+
+        t2 = ibis.table([
+            ('key1', 'string'),
+            ('key2', 'string'),
+            ('key3', 'string'),
+            ('value2', 'double')
+        ], 'bar_table')
+
+        p1 = t1.key1 == t2.key1
+        p2 = t1.key2 == t2.key2
+        p3 = t1.key3 == t2.key3
+
+        joined = t1.inner_join(t2, [p1 & p2 & p3])
+        expected = t1.inner_join(t2, [p1, p2, p3])
+        assert_equal(joined, expected)
+
+    def test_join_add_prefixes(self):
+        pass
+
+    def test_join_nontrivial_exprs(self):
+        pass
+
+    def test_union(self):
+        schema1 = [
+            ('key', 'string'),
+            ('value', 'double')
+        ]
+        schema2 = [
+            ('key', 'string'),
+            ('key2', 'string'),
+            ('value', 'double')
+        ]
+        t1 = ibis.table(schema1, 'foo')
+        t2 = ibis.table(schema1, 'bar')
+        t3 = ibis.table(schema2, 'baz')
+
+        result = t1.union(t2)
+        assert isinstance(result.op(), ops.Union)
+        assert not result.op().distinct
+
+        result = t1.union(t2, distinct=True)
+        assert isinstance(result.op(), ops.Union)
+        assert result.op().distinct
+
+        self.assertRaises(ir.RelationError, t1.union, t3)
+
+    def test_column_ref_on_projection_rename(self):
+        region = self.con.table('tpch_region')
+        nation = self.con.table('tpch_nation')
+        customer = self.con.table('tpch_customer')
+
+        joined = (region.inner_join(
+            nation, [region.r_regionkey == nation.n_regionkey])
+            .inner_join(
+                customer, [customer.c_nationkey == nation.n_nationkey]))
+
+        proj_exprs = [customer, nation.n_name.name('nation'),
+                      region.r_name.name('region')]
+        joined = joined.projection(proj_exprs)
+
+        metrics = [joined.c_acctbal.sum().name('metric')]
+
+        # it works!
+        joined.aggregate(metrics, by=['region'])
+
+
+class TestSemiAntiJoinPredicates(unittest.TestCase):
+
+    def setUp(self):
+        self.con = MockConnection()
+
+        self.t1 = ibis.table([
+            ('key1', 'string'),
+            ('key2', 'string'),
+            ('value1', 'double')
+        ], 'foo')
+
+        self.t2 = ibis.table([
+            ('key1', 'string'),
+            ('key2', 'string')
+        ], 'bar')
+
+    def test_simple_existence_predicate(self):
+        cond = (self.t1.key1 == self.t2.key1).any()
+
+        assert isinstance(cond, ir.BooleanArray)
+        op = cond.op()
+        assert isinstance(op, ops.Any)
+
+        # it works!
+        expr = self.t1[cond]
+        assert isinstance(expr.op(), ops.Filter)
+
+    def test_cannot_use_existence_expression_in_join(self):
+        # Join predicates must consist only of comparisons
+        pass
+
+    def test_not_exists_predicate(self):
+        cond = -(self.t1.key1 == self.t2.key1).any()
+        assert isinstance(cond.op(), ops.NotAny)
+
+
+class TestLateBindingFunctions(BasicTestCase, unittest.TestCase):
+
+    def test_aggregate_metrics(self):
+        functions = [lambda x: x.e.sum().name('esum'),
+                     lambda x: x.f.sum().name('fsum')]
+        exprs = [self.table.e.sum().name('esum'),
+                 self.table.f.sum().name('fsum')]
+
+        result = self.table.aggregate(functions[0])
+        expected = self.table.aggregate(exprs[0])
+        assert_equal(result, expected)
+
+        result = self.table.aggregate(functions)
+        expected = self.table.aggregate(exprs)
+        assert_equal(result, expected)
+
+    def test_group_by_keys(self):
+        m = self.table.mutate(foo=self.table.f * 2,
+                              bar=self.table.e / 2)
+
+        expr = m.group_by(lambda x: x.foo).size()
+        expected = m.group_by('foo').size()
+        assert_equal(expr, expected)
+
+        expr = m.group_by([lambda x: x.foo, lambda x: x.bar]).size()
+        expected = m.group_by(['foo', 'bar']).size()
+        assert_equal(expr, expected)
+
+    def test_having(self):
+        m = self.table.mutate(foo=self.table.f * 2,
+                              bar=self.table.e / 2)
+
+        expr = (m.group_by('foo')
+                .having(lambda x: x.foo.sum() > 10)
+                .size())
+        expected = (m.group_by('foo')
+                    .having(m.foo.sum() > 10)
+                    .size())
+
+        assert_equal(expr, expected)
+
+    def test_filter(self):
+        m = self.table.mutate(foo=self.table.f * 2,
+                              bar=self.table.e / 2)
+
+        result = m.filter(lambda x: x.foo > 10)
+        result2 = m[lambda x: x.foo > 10]
+        expected = m[m.foo > 10]
+
+        assert_equal(result, expected)
+        assert_equal(result2, expected)
+
+        result = m.filter([lambda x: x.foo > 10,
+                           lambda x: x.bar < 0])
+        expected = m.filter([m.foo > 10, m.bar < 0])
+        assert_equal(result, expected)
+
+    def test_sort_by(self):
+        m = self.table.mutate(foo=self.table.e + self.table.f)
+
+        result = m.sort_by(lambda x: -x.foo)
+        expected = m.sort_by(-m.foo)
+        assert_equal(result, expected)
+
+        result = m.sort_by(lambda x: ibis.desc(x.foo))
+        expected = m.sort_by(ibis.desc('foo'))
+        assert_equal(result, expected)
+
+        result = m.sort_by(ibis.desc(lambda x: x.foo))
+        expected = m.sort_by(ibis.desc('foo'))
+        assert_equal(result, expected)
+
+    def test_projection(self):
+        m = self.table.mutate(foo=self.table.f * 2)
+
+        def f(x):
+            return (x.foo * 2).name('bar')
+
+        result = m.projection([f, 'f'])
+        result2 = m[f, 'f']
+        expected = m.projection([f(m), 'f'])
+        assert_equal(result, expected)
+        assert_equal(result2, expected)
+
+    def test_mutate(self):
+        m = self.table.mutate(foo=self.table.f * 2)
+
+        def g(x):
+            return x.foo * 2
+
+        def h(x):
+            return x.bar * 2
+
+        result = m.mutate(bar=g).mutate(baz=h)
+
+        m2 = m.mutate(bar=g(m))
+        expected = m2.mutate(baz=h(m2))
+
+        assert_equal(result, expected)
+
+    def test_add_column(self):
+        def g(x):
+            return x.f * 2
+
+        result = self.table.add_column(g, name='foo')
+        expected = self.table.mutate(foo=g)
+        assert_equal(result, expected)
+
+    def test_groupby_mutate(self):
+        t = self.table
+
+        g = t.group_by('g').order_by('f')
+        expr = g.mutate(foo=lambda x: x.f.lag(),
+                        bar=lambda x: x.f.rank())
+        expected = g.mutate(foo=t.f.lag(),
+                            bar=t.f.rank())
+
+        assert_equal(expr, expected)
+
+    def test_groupby_projection(self):
+        t = self.table
+
+        g = t.group_by('g').order_by('f')
+        expr = g.projection([lambda x: x.f.lag().name('foo'),
+                             lambda x: x.f.rank().name('bar')])
+        expected = g.projection([t.f.lag().name('foo'),
+                                 t.f.rank().name('bar')])
+
+        assert_equal(expr, expected)
+
+    def test_set_column(self):
+        def g(x):
+            return x.f * 2
+
+        result = self.table.set_column('f', g)
+        expected = self.table.set_column('f', self.table.f * 2)
+        assert_equal(result, expected)
```

### Comparing `ibis_framework-9.0.0.dev686/ibis/backends/impala/client.py` & `ibis-framework-v0.6.0/ibis/expr/datatypes.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,357 +1,495 @@
-from __future__ import annotations
+# Copyright 2014 Cloudera Inc.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+# http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-from typing import TYPE_CHECKING
+import re
+import six
 
-import sqlglot as sg
-
-import ibis
-import ibis.common.exceptions as com
-import ibis.expr.schema as sch
 import ibis.expr.types as ir
-from ibis.backends.impala import ddl
-from ibis.backends.impala.ddl import AlterTable, InsertSelect
+import ibis.common as com
+import ibis.util as util
 
-if TYPE_CHECKING:
-    import pandas as pd
+if six.PY3:
+    from io import StringIO
+else:
+    from io import BytesIO as StringIO
 
 
-class ImpalaTable(ir.Table):
-    """A physical table in the Impala-Hive metastore."""
+class Schema(object):
 
-    @property
-    def _qualified_name(self) -> str:
-        op = self.op()
-        return sg.table(op.name, catalog=op.namespace.database).sql(dialect="hive")
+    """
+    Holds table schema information
+    """
 
-    @property
-    def _unqualified_name(self) -> str:
-        return self.op().name
+    def __init__(self, names, types):
+        if not isinstance(names, list):
+            names = list(names)
+        self.names = names
+        self.types = [validate_type(x) for x in types]
+
+        self._name_locs = dict((v, i) for i, v in enumerate(self.names))
+
+        if len(self._name_locs) < len(self.names):
+            raise com.IntegrityError('Duplicate column names')
+
+    def __repr__(self):
+        return self._repr()
+
+    def __len__(self):
+        return len(self.names)
+
+    def __iter__(self):
+        return iter(self.names)
+
+    def _repr(self):
+        buf = StringIO()
+        space = 2 + max(len(x) for x in self.names)
+        for name, tipo in zip(self.names, self.types):
+            buf.write('\n{0}{1}'.format(name.ljust(space), str(tipo)))
+
+        return "ibis.Schema {{{0}\n}}".format(util.indent(buf.getvalue(), 2))
+
+    def __contains__(self, name):
+        return name in self._name_locs
+
+    def __getitem__(self, name):
+        return self.types[self._name_locs[name]]
+
+    def delete(self, names_to_delete):
+        for name in names_to_delete:
+            if name not in self:
+                raise KeyError(name)
+
+        new_names, new_types = [], []
+        for name, type_ in zip(self.names, self.types):
+            if name in names_to_delete:
+                continue
+            new_names.append(name)
+            new_types.append(type_)
+
+        return Schema(new_names, new_types)
+
+    @classmethod
+    def from_tuples(cls, values):
+        if not isinstance(values, (list, tuple)):
+            values = list(values)
+
+        if len(values):
+            names, types = zip(*values)
+        else:
+            names, types = [], []
+        return Schema(names, types)
+
+    @classmethod
+    def from_dict(cls, values):
+        names = list(values.keys())
+        types = values.values()
+        return Schema(names, types)
+
+    def equals(self, other):
+        return ((self.names == other.names) and
+                (self.types == other.types))
+
+    def __eq__(self, other):
+        return self.equals(other)
+
+    def get_type(self, name):
+        return self.types[self._name_locs[name]]
+
+    def append(self, schema):
+        names = self.names + schema.names
+        types = self.types + schema.types
+        return Schema(names, types)
+
+    def items(self):
+        return zip(self.names, self.types)
+
+
+class HasSchema(object):
+
+    """
+    Base class representing a structured dataset with a well-defined
+    schema.
+
+    Base implementation is for tables that do not reference a particular
+    concrete dataset or database table.
+    """
+
+    def __init__(self, schema, name=None):
+        assert isinstance(schema, Schema)
+        self._schema = schema
+        self._name = name
+
+    def __repr__(self):
+        return self._repr()
+
+    def _repr(self):
+        return "%s(%s)" % (type(self).__name__, repr(self.schema))
 
     @property
-    def _client(self):
-        return self.op().source
+    def schema(self):
+        return self._schema
+
+    def get_schema(self):
+        return self._schema
+
+    def has_schema(self):
+        return True
 
     @property
-    def _database(self) -> str:
-        return self.op().namespace.database
+    def name(self):
+        return self._name
+
+    def equals(self, other):
+        if type(self) != type(other):
+            return False
+        return self.schema.equals(other.schema)
+
+    def root_tables(self):
+        return [self]
+
+
+class DataType(object):
+
+    def __init__(self, nullable=True):
+        self.nullable = nullable
+
+    def __call__(self, nullable=True):
+        return self._factory(nullable=nullable)
+
+    def _factory(self, nullable=True):
+        return type(self)(nullable=nullable)
+
+    def __eq__(self, other):
+        return self.equals(other)
+
+    def __ne__(self, other):
+        return not (self == other)
+
+    def __hash__(self):
+        return hash(type(self))
+
+    def __repr__(self):
+        name = self.name()
+        if not self.nullable:
+            name = '{0}[non-nullable]'.format(name)
+        return name
+
+    def name(self):
+        return type(self).__name__.lower()
+
+    def equals(self, other):
+        if isinstance(other, six.string_types):
+            other = validate_type(other)
+
+        return (isinstance(other, type(self)) and
+                self.nullable == other.nullable)
+
+    def can_implicit_cast(self, other):
+        return self.equals(other)
+
+    def scalar_type(self):
+        name = type(self).__name__
+        return getattr(ir, '{0}Scalar'.format(name))
+
+    def array_type(self):
+        name = type(self).__name__
+        return getattr(ir, '{0}Array'.format(name))
+
+
+class Any(DataType):
+    pass
+
+
+class Primitive(DataType):
+    pass
+
+
+class Null(DataType):
+    pass
+
+
+class Variadic(DataType):
+    pass
+
+
+class Boolean(Primitive):
+    pass
+
+
+class Integer(Primitive):
+
+    def can_implicit_cast(self, other):
+        if isinstance(other, Integer):
+            return ((type(self) == Integer) or
+                    (other._nbytes <= self._nbytes))
+        else:
+            return False
+
+
+class String(Variadic):
+    pass
+
+
+class Timestamp(Primitive):
+    pass
+
+
+class SignedInteger(Integer):
+    pass
+
 
-    def compute_stats(self, incremental=False):
-        """Invoke Impala COMPUTE STATS command on the table."""
-        return self._client.compute_stats(
-            self.op().name, database=self._database, incremental=incremental
-        )
-
-    def invalidate_metadata(self):
-        self._client.invalidate_metadata(self.op().name, database=self._database)
-
-    def refresh(self):
-        self._client.refresh(self.op().name, database=self._database)
-
-    def metadata(self):
-        """Return results of `DESCRIBE FORMATTED` statement."""
-        return self._client.describe_formatted(self.op().name, database=self._database)
-
-    describe_formatted = metadata
-
-    def files(self):
-        """Return results of SHOW FILES statement."""
-        return self._client.show_files(self.op().name, database=self._database)
-
-    def drop(self):
-        """Drop the table from the database."""
-        self._client.drop_table_or_view(self.op().name, database=self._database)
-
-    def truncate(self):
-        self._client.truncate_table(self.op().name, database=self._database)
-
-    def insert(
-        self,
-        obj=None,
-        overwrite=False,
-        partition=None,
-        values=None,
-        validate=True,
-    ):
-        """Insert into an Impala table.
-
-        Parameters
-        ----------
-        obj
-            Table expression or DataFrame
-        overwrite
-            If True, will replace existing contents of table
-        partition
-            For partitioned tables, indicate the partition that's being
-            inserted into, either with an ordered list of partition keys or a
-            dict of partition field name to value. For example for the
-            partition (year=2007, month=7), this can be either (2007, 7) or
-            {'year': 2007, 'month': 7}.
-        values
-            Unsupported and unused
-        validate
-            If True, do more rigorous validation that schema of table being
-            inserted is compatible with the existing table
-
-        Examples
-        --------
-        Append to an existing table
-
-        >>> t.insert(table_expr)  # quartodoc: +SKIP # doctest: +SKIP
-
-        Completely overwrite contents
-
-        >>> t.insert(table_expr, overwrite=True)  # quartodoc: +SKIP # doctest: +SKIP
-
-        """
-        if values is not None:
-            raise NotImplementedError
-
-        if not isinstance(obj, ir.Table):
-            obj = ibis.memtable(obj)
-
-        self._client._run_pre_execute_hooks(obj)
-
-        expr = obj
-        if validate:
-            existing_schema = self.schema()
-            insert_schema = expr.schema()
-            if not insert_schema.equals(existing_schema):
-                _validate_compatible(insert_schema, existing_schema)
-
-        if partition is not None:
-            partition_schema = self.partition_schema()
-            partition_schema_names = frozenset(partition_schema.names)
-            expr = expr.select(
-                [
-                    column
-                    for column in expr.columns
-                    if column not in partition_schema_names
-                ]
-            )
+class Floating(Primitive):
+
+    def can_implicit_cast(self, other):
+        if isinstance(other, Integer):
+            return True
+        elif isinstance(other, Floating):
+            # return other._nbytes <= self._nbytes
+            return True
         else:
-            partition_schema = None
+            return False
+
+
+class Int8(Integer):
+
+    _nbytes = 1
+    bounds = (-128, 127)
+
+
+class Int16(Integer):
+
+    _nbytes = 2
+    bounds = (-32768, 32767)
+
+
+class Int32(Integer):
+
+    _nbytes = 4
+    bounds = (-2147483648, 2147483647)
+
+
+class Int64(Integer):
+
+    _nbytes = 8
+    bounds = (-9223372036854775808, 9223372036854775807)
+
+
+class Float(Floating):
+
+    _nbytes = 4
+
+
+class Double(Floating):
+
+    _nbytes = 8
+
+
+class Decimal(DataType):
+    # Decimal types are parametric, we store the parameters in this object
+
+    def __init__(self, precision, scale, nullable=True):
+        self.precision = precision
+        self.scale = scale
+        DataType.__init__(self, nullable=nullable)
+
+    def _base_type(self):
+        return 'decimal'
+
+    def __repr__(self):
+        return ('decimal(precision=%s, scale=%s)'
+                % (self.precision, self.scale))
 
-        statement = InsertSelect(
-            self._qualified_name,
-            self._client.compile(expr),
-            partition=partition,
-            partition_schema=partition_schema,
-            overwrite=overwrite,
-        )
-        self._client._safe_exec_sql(statement.compile())
-        return self
-
-    def load_data(self, path, overwrite=False, partition=None):
-        """Load data into an Impala table.
-
-        Parameters
-        ----------
-        path
-            Data to load
-        overwrite
-            Overwrite the existing data in the entire table or indicated
-            partition
-        partition
-            If specified, the partition must already exist
-
-        """
-        if partition is not None:
-            partition_schema = self.partition_schema()
+    def __hash__(self):
+        return hash((self.precision, self.scale))
+
+    def __ne__(self, other):
+        return not self.__eq__(other)
+
+    def __eq__(self, other):
+        if not isinstance(other, Decimal):
+            return False
+
+        return (self.precision == other.precision and
+                self.scale == other.scale)
+
+    @classmethod
+    def can_implicit_cast(cls, other):
+        return isinstance(other, (Floating, Decimal))
+
+    def array_type(self):
+        def constructor(op, name=None):
+            from ibis.expr.types import DecimalArray
+            return DecimalArray(op, self, name=name)
+        return constructor
+
+    def scalar_type(self):
+        def constructor(op, name=None):
+            from ibis.expr.types import DecimalScalar
+            return DecimalScalar(op, self, name=name)
+        return constructor
+
+
+class Category(DataType):
+
+    def __init__(self, cardinality=None, nullable=True):
+        self.cardinality = cardinality
+        DataType.__init__(self, nullable=nullable)
+
+    def _base_type(self):
+        return 'category'
+
+    def __repr__(self):
+        card = (self.cardinality if self.cardinality is not None
+                else 'unknown')
+        return ('category(K=%s)' % card)
+
+    def __hash__(self):
+        return hash((self.cardinality))
+
+    def __eq__(self, other):
+        if not isinstance(other, Category):
+            return False
+
+        return self.cardinality == other.cardinality
+
+    def to_integer_type(self):
+        if self.cardinality is None:
+            return 'int64'
+        elif self.cardinality < (2 ** 7 - 1):
+            return 'int8'
+        elif self.cardinality < (2 ** 15 - 1):
+            return 'int16'
+        elif self.cardinality < (2 ** 31 - 1):
+            return 'int32'
         else:
-            partition_schema = None
+            return 'int64'
 
-        stmt = ddl.LoadData(
-            self._qualified_name,
-            path,
-            partition=partition,
-            partition_schema=partition_schema,
-            overwrite=overwrite,
-        )
+    def array_type(self):
+        def constructor(op, name=None):
+            from ibis.expr.types import CategoryArray
+            return CategoryArray(op, self, name=name)
+        return constructor
 
-        self._client._safe_exec_sql(stmt.compile())
-        return self
+    def scalar_type(self):
+        def constructor(op, name=None):
+            from ibis.expr.types import CategoryScalar
+            return CategoryScalar(op, self, name=name)
+        return constructor
 
-    @property
-    def name(self) -> str:
-        return self.op().name
 
-    @property
-    def is_partitioned(self):
-        """True if the table is partitioned."""
-        return self.metadata().is_partitioned
-
-    def partition_schema(self):
-        """Return the schema for the partition columns."""
-        schema = self.schema()
-        result = self.partitions()
-
-        partition_fields = []
-        for col in result.columns:
-            if col not in schema:
-                break
-            partition_fields.append((col, schema[col]))
-
-        return sch.Schema(dict(partition_fields))
-
-    def add_partition(self, spec, location=None):
-        """Add a new table partition.
-
-        Partition parameters can be set in a single DDL statement or you can
-        use `alter_partition` to set them after the fact.
-        """
-        part_schema = self.partition_schema()
-        stmt = ddl.AddPartition(
-            self._qualified_name, spec, part_schema, location=location
-        )
-        self._client._safe_exec_sql(stmt)
-        return self
-
-    def alter(
-        self,
-        location=None,
-        format=None,
-        tbl_properties=None,
-        serde_properties=None,
-    ):
-        """Change settings and parameters of the table.
-
-        Parameters
-        ----------
-        location
-            For partitioned tables, you may want the alter_partition function
-        format
-            Table format
-        tbl_properties
-            Table properties
-        serde_properties
-            Serialization/deserialization properties
-
-        """
-
-        def _run_ddl(**kwds):
-            stmt = AlterTable(self._qualified_name, **kwds)
-            self._client._safe_exec_sql(stmt)
-            return self
-
-        return self._alter_table_helper(
-            _run_ddl,
-            location=location,
-            format=format,
-            tbl_properties=tbl_properties,
-            serde_properties=serde_properties,
-        )
-
-    def set_external(self, is_external=True):
-        """Toggle the `EXTERNAL` table property."""
-        self.alter(tbl_properties={"EXTERNAL": is_external})
-
-    def alter_partition(
-        self,
-        spec,
-        location=None,
-        format=None,
-        tbl_properties=None,
-        serde_properties=None,
-    ):
-        """Change settings and parameters of an existing partition.
-
-        Parameters
-        ----------
-        spec
-            The partition keys for the partition being modified
-        location
-            Location of the partition
-        format
-            Table format
-        tbl_properties
-            Table properties
-        serde_properties
-            Serialization/deserialization properties
-
-        """
-        part_schema = self.partition_schema()
-
-        def _run_ddl(**kwds):
-            stmt = ddl.AlterPartition(self._qualified_name, spec, part_schema, **kwds)
-            self._client._safe_exec_sql(stmt)
-            return self
-
-        return self._alter_table_helper(
-            _run_ddl,
-            location=location,
-            format=format,
-            tbl_properties=tbl_properties,
-            serde_properties=serde_properties,
-        )
-
-    def _alter_table_helper(self, f, **alterations):
-        results = []
-        for k, v in alterations.items():
-            if v is None:
-                continue
-            result = f(**{k: v})
-            results.append(result)
-        return results
+class Struct(DataType):
+
+    def __init__(self, names, types, nullable=True):
+        DataType.__init__(self, nullable=nullable)
+
+
+class Array(Variadic):
+
+    def __init__(self, value_type, nullable=True):
+        Variadic.__init__(self, nullable=nullable)
+
+
+class Enum(DataType):
+
+    def __init__(self, rep_type, value_type, nullable=True):
+        DataType.__init__(self, nullable=nullable)
+
+
+class Map(DataType):
+
+    def __init__(self, key_type, value_type, nullable=True):
+        DataType.__init__(self, nullable=nullable)
+
+
+# ---------------------------------------------------------------------
+
+
+any = Any()
+null = Null()
+boolean = Boolean()
+int_ = Integer()
+int8 = Int8()
+int16 = Int16()
+int32 = Int32()
+int64 = Int64()
+float = Float()
+double = Double()
+string = String()
+timestamp = Timestamp()
 
-    def drop_partition(self, spec):
-        """Drop an existing table partition."""
-        part_schema = self.partition_schema()
-        stmt = ddl.DropPartition(self._qualified_name, spec, part_schema)
-        self._client._safe_exec_sql(stmt)
-        return self
 
-    def partitions(self):
-        """Return information about the table's partitions.
+_primitive_types = {
+    'any': any,
+    'null': null,
+    'boolean': boolean,
+    'int8': int8,
+    'int16': int16,
+    'int32': int32,
+    'int64': int64,
+    'float': float,
+    'double': double,
+    'string': string,
+    'timestamp': timestamp
+}
 
-        Raises an exception if the table is not partitioned.
-        """
-        return self._client.list_partitions(self._qualified_name)
 
-    def stats(self) -> pd.DataFrame:
-        """Return results of `SHOW TABLE STATS`.
+def validate_type(t):
+    if isinstance(t, DataType):
+        return t
 
-        If not partitioned, contains only one row.
+    parsed_type = _parse_type(t)
+    if parsed_type is not None:
+        return parsed_type
 
-        Returns
-        -------
-        DataFrame
-            Table statistics
+    if t in _primitive_types:
+        return _primitive_types[t]
+    else:
+        raise ValueError('Invalid type: %s' % repr(t))
 
-        """
-        return self._client.table_stats(self._qualified_name)
 
-    def column_stats(self) -> pd.DataFrame:
-        """Return results of `SHOW COLUMN STATS`.
+_DECIMAL_RE = re.compile('decimal\((\d+),[\s]*(\d+)\)')
 
-        Returns
-        -------
-        DataFrame
-            Column statistics
 
-        """
-        return self._client.column_stats(self._qualified_name)
+def _parse_decimal(t):
+    m = _DECIMAL_RE.match(t)
+    if m:
+        precision, scale = m.groups()
+        return Decimal(int(precision), int(scale))
 
+    if t == 'decimal':
+        # From the Impala documentation
+        return Decimal(9, 0)
 
-# ----------------------------------------------------------------------
-# ORM-ish usability layer
 
+_type_parsers = [
+    _parse_decimal
+]
 
-class ScalarFunction:
-    def drop(self):
-        pass
 
+def _parse_type(t):
+    for parse_fn in _type_parsers:
+        parsed = parse_fn(t)
+        if parsed is not None:
+            return parsed
+    return None
 
-class AggregateFunction:
-    def drop(self):
-        pass
 
+def array_type(t):
+    # compatibility
+    return validate_type(t).array_type()
 
-def _validate_compatible(from_schema, to_schema):
-    if set(from_schema.names) != set(to_schema.names):
-        raise com.IbisInputError("Schemas have different names")
 
-    for name in from_schema:
-        lt = from_schema[name]
-        rt = to_schema[name]
-        if not lt.castable(rt):
-            raise com.IbisInputError(f"Cannot safely cast {lt!r} to {rt!r}")
+def scalar_type(t):
+    # compatibility
+    return validate_type(t).scalar_type()
```

### Comparing `ibis_framework-9.0.0.dev686/ibis/backends/impala/metadata.py` & `ibis-framework-v0.6.0/ibis/impala/metadata.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,23 +1,23 @@
-from __future__ import annotations
-
 # Copyright 2014 Cloudera Inc.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 # http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-from io import StringIO
+
+from six import StringIO
+import pandas as pd
 
 
 def parse_metadata(descr_table):
     parser = MetadataParser(descr_table)
     return parser.parse()
 
 
@@ -33,65 +33,56 @@
                 result = converter(result)
             return result
 
         return _converter
 
     return _get_item
 
-
 _get_type = _item_converter(1)
 _get_comment = _item_converter(2)
 
 
 def _try_timestamp(x):
-    import pandas as pd
-
     try:
-        ts = pd.Timestamp(x, tz="UTC")
-        return ts.to_pydatetime().replace(tzinfo=None)
+        return pd.Timestamp(x)
     except (ValueError, TypeError):
         return x
 
 
 def _try_unix_timestamp(x):
     try:
-        value = int(x)
+        return pd.Timestamp.fromtimestamp(int(x))
     except (ValueError, TypeError):
         return x
-    else:
-        import pandas as pd
-
-        return (
-            pd.Timestamp.fromtimestamp(value, tz="UTC")
-            .tz_localize(None)
-            .to_pydatetime()
-        )
 
 
 def _try_boolean(x):
     try:
         x = x.lower()
-        if x in ("true", "yes"):
+        if x in ('true', 'yes'):
             return True
-        elif x in ("false", "no"):
+        elif x in ('false', 'no'):
             return False
         return x
     except (ValueError, TypeError):
         return x
 
 
 def _try_int(x):
     try:
         return int(x)
     except (ValueError, TypeError):
         return x
 
 
-class MetadataParser:
-    """A simple state machine to parse the results of `DESCRIBE FORMATTED`."""
+class MetadataParser(object):
+
+    """
+    A simple state-ish machine to parse the results of DESCRIBE FORMATTED
+    """
 
     def __init__(self, table):
         self.table = table
         self.tuples = list(self.table.itertuples(index=False))
 
     def _reset(self):
         self.pos = 0
@@ -108,141 +99,138 @@
         self.pos += 1
         return result
 
     def parse(self):
         self._reset()
         self._parse()
 
-        return TableMetadata(
-            self.schema, self.info, self.storage, partitions=self.partitions
-        )
+        return TableMetadata(self.schema, self.info, self.storage,
+                             partitions=self.partitions)
 
     def _parse(self):
         self.schema = self._parse_schema()
 
         next_section = self._next_tuple()
-        if "partition" in next_section[0].lower():
+        if 'partition' in next_section[0].lower():
             self._parse_partitions()
         else:
             self._parse_info()
 
     def _parse_partitions(self):
         self.partitions = self._parse_schema()
 
         next_section = self._next_tuple()
-        if "table information" not in next_section[0].lower():
-            raise ValueError("Table information not present")
+        if 'table information' not in next_section[0].lower():
+            raise ValueError('Table information not present')
 
         self._parse_info()
 
     def _parse_schema(self):
         tup = self._next_tuple()
-        if "col_name" not in tup[0]:
-            raise ValueError(
-                f"DESCRIBE FORMATTED did not return the expected results: {tup}"
-            )
+        if 'col_name' not in tup[0]:
+            raise ValueError('DESCRIBE FORMATTED did not return '
+                             'the expected results: {0}'
+                             .format(tup))
         self._next_tuple()
 
         # Use for both main schema and partition schema (if any)
         schema = []
         while True:
             tup = self._next_tuple()
-            if not tup[0].strip():
+            if tup[0].strip() == '':
                 break
             schema.append((tup[0], tup[1]))
 
         return schema
 
     def _parse_info(self):
         self.info = {}
         while True:
             tup = self._next_tuple()
-            orig_key = tup[0].strip(":")
+            orig_key = tup[0].strip(':')
             key = _clean_param_name(tup[0])
 
-            if not key or key.startswith("#"):
+            if key == '' or key.startswith('#'):
                 # section is done
                 break
 
-            if key == "table parameters":
+            if key == 'table parameters':
                 self._parse_table_parameters()
             elif key in self._info_cleaners:
                 result = self._info_cleaners[key](tup)
                 self.info[orig_key] = result
             else:
                 self.info[orig_key] = tup[1]
 
-        if "storage information" not in key:
-            raise ValueError("Storage information not present")
+        if 'storage information' not in key:
+            raise ValueError('Storage information not present')
 
         self._parse_storage_info()
 
     _info_cleaners = {
-        "database": _get_type(),
-        "owner": _get_type(),
-        "createtime": _get_type(_try_timestamp),
-        "lastaccesstime": _get_type(_try_timestamp),
-        "protect mode": _get_type(),
-        "retention": _get_type(_try_int),
-        "location": _get_type(),
-        "table type": _get_type(),
+        'database': _get_type(),
+        'owner': _get_type(),
+        'createtime': _get_type(_try_timestamp),
+        'lastaccesstime': _get_type(_try_timestamp),
+        'protect mode': _get_type(),
+        'retention': _get_type(_try_int),
+        'location': _get_type(),
+        'table type': _get_type()
     }
 
     def _parse_table_parameters(self):
         params = self._parse_nested_params(self._table_param_cleaners)
-        self.info["Table Parameters"] = params
+        self.info['Table Parameters'] = params
 
     _table_param_cleaners = {
-        "external": _try_boolean,
-        "column_stats_accurate": _try_boolean,
-        "numfiles": _try_int,
-        "totalsize": _try_int,
-        "stats_generated_via_stats_task": _try_boolean,
-        "numrows": _try_int,
-        "transient_lastddltime": _try_unix_timestamp,
+        'external': _try_boolean,
+        'column_stats_accurate': _try_boolean,
+        'numfiles': _try_int,
+        'totalsize': _try_int,
+        'stats_generated_via_stats_task': _try_boolean,
+        'numrows': _try_int,
+        'transient_lastddltime': _try_unix_timestamp,
     }
 
     def _parse_storage_info(self):
         self.storage = {}
         while True:
             # end of the road
             try:
                 tup = self._next_tuple()
             except StopIteration:
                 break
 
-            orig_key = tup[0].strip(":")
+            orig_key = tup[0].strip(':')
             key = _clean_param_name(tup[0])
 
-            if not key or key.startswith("#"):
+            if key == '' or key.startswith('#'):
                 # section is done
                 break
 
-            if key == "storage desc params":
+            if key == 'storage desc params':
                 self._parse_storage_desc_params()
             elif key in self._storage_cleaners:
                 result = self._storage_cleaners[key](tup)
                 self.storage[orig_key] = result
             else:
                 self.storage[orig_key] = tup[1]
 
     _storage_cleaners = {
-        "compressed": _get_type(_try_boolean),
-        "num buckets": _get_type(_try_int),
+        'compressed': _get_type(_try_boolean),
+        'num buckets': _get_type(_try_int),
     }
 
     def _parse_storage_desc_params(self):
         params = self._parse_nested_params(self._storage_param_cleaners)
-        self.storage["Desc Params"] = params
+        self.storage['Desc Params'] = params
 
     _storage_param_cleaners = {}
 
     def _parse_nested_params(self, cleaners):
-        import pandas as pd
-
         params = {}
         while True:
             try:
                 tup = self._next_tuple()
             except StopIteration:
                 break
             if pd.isnull(tup[1]):
@@ -255,15 +243,15 @@
                 value = cleaner(value)
             params[key] = value
 
         return params
 
 
 def _clean_param_name(x):
-    return x.strip().strip(":").lower()
+    return x.strip().strip(':').lower()
 
 
 def _get_meta(attr, key):
     @property
     def f(self):
         data = getattr(self, attr)
         if isinstance(key, list):
@@ -271,60 +259,62 @@
             for k in key:
                 if k not in result:
                     raise KeyError(k)
                 result = result[k]
             return result
         else:
             return data[key]
-
     return f
 
 
-class TableMetadata:
-    """Container for the parsed and wrangled results of `DESCRIBE FORMATTED`."""
+class TableMetadata(object):
 
+    """
+    Container for the parsed and wrangled results of DESCRIBE FORMATTED for
+    easier Ibis use (and testing).
+    """
     def __init__(self, schema, info, storage, partitions=None):
         self.schema = schema
         self.info = info
         self.storage = storage
         self.partitions = partitions
 
     def __repr__(self):
         import pprint
 
         # Quick and dirty for now
         buf = StringIO()
         buf.write(str(type(self)))
-        buf.write("\n")
+        buf.write('\n')
 
         data = {
-            "schema": self.schema,
-            "info": self.info,
-            "storage info": self.storage,
+            'schema': self.schema,
+            'info': self.info,
+            'storage info': self.storage
         }
         if self.partitions is not None:
-            data["partition schema"] = self.partitions
+            data['partition schema'] = self.partitions
 
-        pprint.pprint(data, stream=buf)  # noqa: T203
+        pprint.pprint(data, stream=buf)
 
         return buf.getvalue()
 
     @property
     def is_partitioned(self):
         return self.partitions is not None
 
-    create_time = _get_meta("info", "CreateTime")
-    location = _get_meta("info", "Location")
-    owner = _get_meta("info", "Owner")
-    num_rows = _get_meta("info", ["Table Parameters", "numRows"])
-    hive_format = _get_meta("storage", "InputFormat")
+    create_time = _get_meta('info', 'CreateTime')
+    location = _get_meta('info', 'Location')
+    owner = _get_meta('info', 'Owner')
+    num_rows = _get_meta('info', ['Table Parameters', 'numRows'])
+    hive_format = _get_meta('storage', 'InputFormat')
 
-    tbl_properties = _get_meta("info", "Table Parameters")
-    serde_properties = _get_meta("storage", "Desc Params")
+    tbl_properties = _get_meta('info', 'Table Parameters')
+    serde_properties = _get_meta('storage', 'Desc Params')
 
 
-class TableInfo:
+class TableInfo(object):
     pass
 
 
-class TableStorageInfo:
+class TableStorageInfo(object):
     pass
```

### Comparing `ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/test_metadata.py` & `ibis-framework-v0.6.0/ibis/impala/tests/test_metadata.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,140 +1,129 @@
-from __future__ import annotations
+# Copyright 2014 Cloudera Inc.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+# http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 import pandas as pd
-import pytest
-import toolz
-from numpy import nan
 
-from ibis.backends.impala.metadata import parse_metadata
+from numpy import nan
 
+from ibis.compat import unittest
+from ibis.impala.metadata import parse_metadata
 
-@pytest.fixture(scope="module")
-def spacer():
-    return ("", nan, nan)
-
-
-@pytest.fixture(scope="module")
-def schema(spacer):
-    return [
-        ("# col_name", "data_type", "comment"),
-        spacer,
-        ("foo", "int", nan),
-        ("bar", "tinyint", nan),
-        ("baz", "bigint", nan),
-    ]
-
-
-@pytest.fixture(scope="module")
-def partitions(spacer):
-    return [
-        ("# Partition Information", nan, nan),
-        ("# col_name", "data_type", "comment"),
-        spacer,
-        ("qux", "bigint", nan),
-    ]
-
-
-@pytest.fixture(scope="module")
-def info():
-    return [
-        ("# Detailed Table Information", nan, nan),
-        ("Database:", "tpcds", nan),
-        ("Owner:", "wesm", nan),
-        ("CreateTime:", "2015-11-08 01:09:42-08:00", nan),
-        ("LastAccessTime:", "UNKNOWN", nan),
-        ("Protect Mode:", "None", nan),
-        ("Retention:", "0", nan),
-        ("Location:", "hdfs://host-name:20500/my.db/dbname.table_name", nan),
-        ("Table Type:", "EXTERNAL_TABLE", nan),
-        ("Table Parameters:", nan, nan),
-        ("", "EXTERNAL", "TRUE"),
-        ("", "STATS_GENERATED_VIA_STATS_TASK", "true"),
-        ("", "numRows", "183592"),
-        ("", "transient_lastDdlTime", "1447340941"),
-    ]
-
-
-@pytest.fixture(scope="module")
-def storage_info():
-    return [
-        ("# Storage Information", nan, nan),
-        ("SerDe Library:", "org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe", nan),
-        ("InputFormat:", "org.apache.hadoop.mapred.TextInputFormat", nan),
-        (
-            "OutputFormat:",
-            "org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat",
-            nan,
-        ),
-        ("Compressed:", "No", nan),
-        ("Num Buckets:", "0", nan),
-        ("Bucket Columns:", "[]", nan),
-        ("Sort Columns:", "[]", nan),
-        ("Storage Desc Params:", nan, nan),
-        ("", "field.delim", "|"),
-        ("", "serialization.format", "|"),
-    ]
-
-
-@pytest.fixture(scope="module")
-def part_metadata(spacer, schema, partitions, info, storage_info):
-    return pd.DataFrame.from_records(
-        list(
-            toolz.concat(
-                toolz.interpose([spacer], [schema, partitions, info, storage_info])
-            )
-        ),
-        columns=["name", "type", "comment"],
-    )
-
-
-@pytest.fixture(scope="module")
-def unpart_metadata(spacer, schema, info, storage_info):
-    return pd.DataFrame.from_records(
-        list(toolz.concat(toolz.interpose([spacer], [schema, info, storage_info]))),
-        columns=["name", "type", "comment"],
-    )
-
-
-@pytest.fixture(scope="module")
-def parsed_part(part_metadata):
-    return parse_metadata(part_metadata)
-
-
-@pytest.fixture(scope="module")
-def parsed_unpart(unpart_metadata):
-    return parse_metadata(unpart_metadata)
-
-
-def test_table_params(parsed_part):
-    params = parsed_part.info["Table Parameters"]
-
-    assert params["EXTERNAL"] is True
-    assert params["STATS_GENERATED_VIA_STATS_TASK"] is True
-    assert params["numRows"] == 183592
-    assert params["transient_lastDdlTime"] == pd.Timestamp("2015-11-12 15:09:01")
-
-
-def test_partitions(parsed_unpart, parsed_part):
-    assert parsed_unpart.partitions is None
-    assert parsed_part.partitions == [("qux", "bigint")]
-
-
-def test_schema(parsed_part):
-    assert parsed_part.schema == [
-        ("foo", "int"),
-        ("bar", "tinyint"),
-        ("baz", "bigint"),
-    ]
-
-
-def test_storage_info(parsed_part):
-    storage = parsed_part.storage
-    assert storage["Compressed"] is False
-    assert storage["Num Buckets"] == 0
 
+def _glue_lists_spacer(spacer, lists):
+    result = list(lists[0])
+    for lst in lists[1:]:
+        result.append(spacer)
+        result.extend(lst)
+    return result
+
+
+class TestMetadataParser(unittest.TestCase):
+
+    @classmethod
+    def setUpClass(cls):
+        cls.spacer = ('', nan, nan)
+
+        cls.schema = [
+            ('# col_name', 'data_type', 'comment'),
+            cls.spacer,
+            ('foo', 'int', nan),
+            ('bar', 'tinyint', nan),
+            ('baz', 'bigint', nan)
+        ]
+
+        cls.partitions = [
+            ('# Partition Information', nan, nan),
+            ('# col_name', 'data_type', 'comment'),
+            cls.spacer,
+            ('qux', 'bigint', nan)
+        ]
+
+        cls.info = [
+            ('# Detailed Table Information', nan, nan),
+            ('Database:', 'tpcds', nan),
+            ('Owner:', 'wesm', nan),
+            ('CreateTime:', 'Sun Nov 08 01:09:42 PST 2015', nan),
+            ('LastAccessTime:', 'UNKNOWN', nan),
+            ('Protect Mode:', 'None', nan),
+            ('Retention:', '0', nan),
+            ('Location:', ('hdfs://host-name:20500/my.db'
+                           '/dbname.table_name'), nan),
+            ('Table Type:', 'EXTERNAL_TABLE', nan),
+            ('Table Parameters:', nan, nan),
+            ('', 'EXTERNAL', 'TRUE'),
+            ('', 'STATS_GENERATED_VIA_STATS_TASK', 'true'),
+            ('', 'numRows', '183592'),
+            ('', 'transient_lastDdlTime', '1447369741'),
+        ]
+
+        cls.storage_info = [
+            ('# Storage Information', nan, nan),
+            ('SerDe Library:', ('org.apache.hadoop'
+                                '.hive.serde2.lazy.LazySimpleSerDe'), nan),
+            ('InputFormat:', 'org.apache.hadoop.mapred.TextInputFormat', nan),
+            ('OutputFormat:',
+             'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat',
+             nan),
+            ('Compressed:', 'No', nan),
+            ('Num Buckets:', '0', nan),
+            ('Bucket Columns:', '[]', nan),
+            ('Sort Columns:', '[]', nan),
+            ('Storage Desc Params:', nan, nan),
+            ('', 'field.delim', '|'),
+            ('', 'serialization.format', '|')
+        ]
+
+        cls.part_metadata = pd.DataFrame.from_records(
+            _glue_lists_spacer(cls.spacer, [cls.schema, cls.partitions,
+                                            cls.info, cls.storage_info]),
+            columns=['name', 'type', 'comment'])
+
+        cls.unpart_metadata = pd.DataFrame.from_records(
+            _glue_lists_spacer(cls.spacer, [cls.schema, cls.info,
+                                            cls.storage_info]),
+            columns=['name', 'type', 'comment'])
+
+        cls.parsed_part = parse_metadata(cls.part_metadata)
+        cls.parsed_unpart = parse_metadata(cls.unpart_metadata)
+
+    def test_table_params(self):
+        params = self.parsed_part.info['Table Parameters']
+
+        assert params['EXTERNAL'] is True
+        assert params['STATS_GENERATED_VIA_STATS_TASK'] is True
+        assert params['numRows'] == 183592
+        assert (params['transient_lastDdlTime'] ==
+                pd.Timestamp('2015-11-12 15:09:01'))
+
+    def test_partitions(self):
+        assert self.parsed_unpart.partitions is None
+        assert self.parsed_part.partitions == [('qux', 'bigint')]
+
+    def test_schema(self):
+        assert self.parsed_part.schema == [
+            ('foo', 'int'),
+            ('bar', 'tinyint'),
+            ('baz', 'bigint')
+        ]
+
+    def test_storage_info(self):
+        storage = self.parsed_part.storage
+        assert storage['Compressed'] is False
+        assert storage['Num Buckets'] == 0
 
-def test_storage_params(parsed_part):
-    params = parsed_part.storage["Desc Params"]
+    def test_storage_params(self):
+        params = self.parsed_part.storage['Desc Params']
 
-    assert params["field.delim"] == "|"
-    assert params["serialization.format"] == "|"
+        assert params['field.delim'] == '|'
+        assert params['serialization.format'] == '|'
```

### Comparing `ibis_framework-9.0.0.dev686/ibis/backends/impala/tests/test_udf.py` & `ibis-framework-v0.6.0/ibis/impala/tests/test_udf.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,630 +1,502 @@
-from __future__ import annotations
+# Copyright 2015 Cloudera Inc
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+# http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-from decimal import Decimal
 from posixpath import join as pjoin
-
-import numpy as np
-import pandas as pd
 import pytest
 
 import ibis
-import ibis.backends.impala as api
-import ibis.common.exceptions as com
-import ibis.expr.datatypes as dt
+
 import ibis.expr.types as ir
-from ibis import util
-from ibis.backends.impala import ddl
-from ibis.common.annotations import ValidationError
-from ibis.expr import rules
-
-pytest.importorskip("impala")
-
-
-@pytest.fixture(scope="module")
-def table(mockcon):
-    return mockcon.table("functional_alltypes")
-
-
-@pytest.fixture
-def i8(table):
-    return table.tinyint_col
-
-
-@pytest.fixture
-def i16(table):
-    return table.smallint_col
-
-
-@pytest.fixture
-def i32(table):
-    return table.int_col
-
-
-@pytest.fixture
-def i64(table):
-    return table.bigint_col
-
-
-@pytest.fixture
-def d(table):
-    return table.double_col
-
-
-@pytest.fixture
-def f(table):
-    return table.float_col
-
-
-@pytest.fixture
-def s(table):
-    return table.string_col
-
-
-@pytest.fixture
-def b(table):
-    return table.bool_col
-
-
-@pytest.fixture
-def t(table):
-    return table.timestamp_col
-
-
-@pytest.fixture
-def tpch_customer(con):
-    return con.table("customer")
-
-
-@pytest.fixture
-def dec(tpch_customer):
-    return tpch_customer.c_acctbal
-
-
-@pytest.fixture
-def all_cols(i8, i16, i32, i64, d, f, dec, s, b, t):
-    return [
-        i8,
-        i16,
-        i32,
-        i64,
-        d,
-        f,
-        dec,
-        s,
-        b,
-        t,
-    ]
-
-
-def test_sql_generation(snapshot):
-    func = api.scalar_function(
-        ["string"], "string", name="identity", database="udf_testing"
-    )
-    result = func("hello world")
-    snapshot.assert_match(ibis.impala.compile(result), "out.sql")
-
-
-def test_sql_generation_from_infoclass(snapshot):
-    func = api.wrap_udf(
-        "test.so",
-        ["string"],
-        "string",
-        "info_test",
-        name="info_test",
-        database="udf_testing",
-    )
-    repr(func)
-
-    result = func("hello world").name("tmp")
-    snapshot.assert_match(ibis.impala.compile(result), "out.sql")
-
-
-@pytest.mark.parametrize(
-    ("ty", "value", "column"),
-    [
-        pytest.param("boolean", True, "bool_col", id="boolean"),
-        pytest.param("int8", 1, "tinyint_col", id="int8"),
-        pytest.param("int16", 1, "smallint_col", id="int16"),
-        pytest.param("int32", 1, "int_col", id="int32"),
-        pytest.param("int64", 1, "bigint_col", id="int64"),
-        pytest.param("float", 1.0, "float_col", id="float"),
-        pytest.param("double", 1.0, "double_col", id="double"),
-        pytest.param("string", "1", "string_col", id="string"),
-        pytest.param(
-            "timestamp",
-            ibis.timestamp("1961-04-10"),
-            "timestamp_col",
-            id="timestamp",
-        ),
-    ],
-)
-def test_udf_primitive_output_types(ty, value, column, table):
-    func = _register_udf([ty], ty, "test")
-
-    ibis_type = dt.validate_type(ty)
-
-    expr = func(value)
-    assert type(expr) == getattr(ir, ibis_type.scalar)
-
-    expr = func(table[column])
-    assert type(expr) == getattr(ir, ibis_type.column)
-
-
-@pytest.mark.parametrize(
-    ("ty", "value"),
-    [
-        pytest.param("boolean", True, id="boolean"),
-        pytest.param("int8", 1, id="int8"),
-        pytest.param("int16", 1, id="int16"),
-        pytest.param("int32", 1, id="int32"),
-        pytest.param("int64", 1, id="int64"),
-        pytest.param("float", 1.0, id="float"),
-        pytest.param("double", 1.0, id="double"),
-        pytest.param("string", "1", id="string"),
-        pytest.param(
-            "timestamp",
-            ibis.timestamp("1961-04-10"),
-            id="timestamp",
-        ),
-    ],
-)
-def test_uda_primitive_output_types(ty, value):
-    func = _register_uda([ty], ty, "test")
-
-    ibis_type = dt.validate_type(ty)
-    scalar_type = getattr(ir, ibis_type.scalar)
-
-    expr1 = func(value)
-    assert isinstance(expr1, scalar_type)
-
-    expr2 = func(value)
-    assert isinstance(expr2, scalar_type)
-
-
-def test_decimal(dec):
-    func = _register_udf(["decimal(12, 2)"], "decimal(12, 2)", "test")
-    expr = func(1.0)
-    assert type(expr) == ir.DecimalScalar
-    expr = func(dec)
-    assert type(expr) == ir.DecimalColumn
-
-
-@pytest.mark.parametrize(
-    ("ty", "valid_cast_indexer"),
-    [
-        pytest.param("decimal(12, 2)", slice(7), id="decimal"),
-        pytest.param("double", slice(6), id="double"),
-        pytest.param("float", slice(6), id="float"),
-        pytest.param("int16", slice(2), id="int16"),
-        pytest.param("int32", slice(3), id="int32"),
-        pytest.param("int64", slice(4), id="int64"),
-        pytest.param("int8", slice(1), id="int8"),
-    ],
-)
-def test_udf_valid_typecasting(ty, valid_cast_indexer, all_cols):
-    func = _register_udf([ty], "int32", "typecast")
-
-    for expr in all_cols[valid_cast_indexer]:
-        func(expr)
-
-
-@pytest.mark.parametrize(
-    ("ty", "valid_cast_indexer"),
-    [
-        pytest.param("boolean", slice(8), id="boolean_first_8"),
-        pytest.param("boolean", slice(9, None), id="boolean_9_onwards"),
-        pytest.param("decimal", slice(7, None), id="decimal"),
-        pytest.param("double", slice(-3, None), id="double"),
-        pytest.param("float", slice(-3, None), id="float"),
-        pytest.param("int16", slice(2, None), id="int16"),
-        pytest.param("int32", slice(3, None), id="int32"),
-        pytest.param("int64", slice(4, None), id="int64"),
-        pytest.param("int8", slice(1, None), id="int8"),
-        pytest.param("string", slice(7), id="string_first_7"),
-        pytest.param("string", slice(8, None), id="string_8_onwards"),
-        pytest.param("timestamp", slice(-1), id="timestamp"),
-    ],
-)
-def test_udf_invalid_typecasting(ty, valid_cast_indexer, all_cols):
-    func = _register_udf([ty], "int32", "typecast")
-
-    for expr in all_cols[valid_cast_indexer]:
-        with pytest.raises(ValidationError):
-            func(expr)
-
-
-def test_mult_args(i32, d, s, b, t):
-    func = _register_udf(
-        ["int32", "double", "string", "boolean", "timestamp"],
-        "int64",
-        "mult_types",
-    )
-
-    expr = func(i32, d, s, b, t)
-    assert issubclass(type(expr), ir.Column)
-
-    expr = func(1, 1.0, "a", True, ibis.timestamp("1961-04-10"))
-    assert issubclass(type(expr), ir.Scalar)
-
-
-def _register_udf(inputs, output, name):
-    func = api.scalar_function(inputs, output, name=name, database="ibis_testing")
-    return func
-
-
-def _register_uda(inputs, output, name):
-    func = api.aggregate_function(inputs, output, name=name, database="ibis_testing")
-    return func
-
-
-@pytest.fixture
-def udf_ll(test_data_dir):
-    return pjoin(test_data_dir, "udf/udf-sample.ll")
-
-
-@pytest.fixture
-def uda_ll(test_data_dir):
-    return pjoin(test_data_dir, "udf/uda-sample.ll")
-
-
-@pytest.fixture
-def uda_so(test_data_dir):
-    return pjoin(test_data_dir, "udf/libudasample.so")
-
-
-@pytest.mark.parametrize(
-    ("typ", "lit_val", "col_name"),
-    [
-        pytest.param("boolean", True, "bool_col", id="boolean"),
-        pytest.param("int8", ibis.literal(5), "tinyint_col", id="int8"),
-        pytest.param(
-            "int16",
-            ibis.literal(2**10),
-            "smallint_col",
-            id="int16",
-        ),
-        pytest.param("int32", ibis.literal(2**17), "int_col", id="int16"),
-        pytest.param("int64", ibis.literal(2**33), "bigint_col", id="int64"),
-        pytest.param("float", ibis.literal(3.14), "float_col", id="float"),
-        pytest.param("double", ibis.literal(3.14), "double_col", id="double"),
-        pytest.param(
-            "string",
-            ibis.literal("ibis"),
-            "string_col",
-            id="string",
-        ),
-        pytest.param(
-            "timestamp",
-            ibis.timestamp("1961-04-10"),
-            "timestamp_col",
-            id="timestamp",
-        ),
-    ],
-)
-@pytest.mark.xfail(
-    reason="Unknown reason. xfailing to restore the CI for udf tests. #2358"
-)
-def test_identity_primitive_types(
-    con, alltypes, test_data_db, udf_ll, typ, lit_val, col_name
-):
-    col_val = alltypes[col_name]
-    identity_func_testing(udf_ll, con, test_data_db, typ, lit_val, col_val)
-
-
-@pytest.mark.xfail(
-    reason="Unknown reason. xfailing to restore the CI for udf tests. #2358"
-)
-def test_decimal_fail(con, test_data_db, udf_ll):
-    col = con.table("customer").c_acctbal
-    literal = ibis.literal(1).cast("decimal(12,2)")
-    name = "__tmp_udf_" + util.guid()
-
-    func = udf_creation_to_op(
-        udf_ll,
-        con,
-        test_data_db,
-        name,
-        "Identity",
-        ["decimal(12,2)"],
-        "decimal(12,2)",
-    )
-
-    expr = func(literal)
-    assert issubclass(type(expr), ir.Scalar)
-    result = con.execute(expr)
-    assert result == Decimal(1)
-
-    expr = func(col)
-    assert issubclass(type(expr), ir.Column)
-    con.execute(expr)
-
-
-@pytest.mark.xfail(
-    reason="Unknown reason. xfailing to restore the CI for udf tests. #2358"
-)
-def test_mixed_inputs(con, alltypes, test_data_db, udf_ll):
-    name = "two_args"
-    symbol = "TwoArgs"
-    inputs = ["int32", "int32"]
-    output = "int32"
-    func = udf_creation_to_op(udf_ll, con, test_data_db, name, symbol, inputs, output)
-
-    expr = func(alltypes.int_col, 1)
-    assert issubclass(type(expr), ir.Column)
-    con.execute(expr)
-
-    expr = func(1, alltypes.int_col)
-    assert issubclass(type(expr), ir.Column)
-    con.execute(expr)
-
-    expr = func(alltypes.int_col, alltypes.tinyint_col)
-    con.execute(expr)
-
-
-@pytest.mark.xfail(
-    reason="Unknown reason. xfailing to restore the CI for udf tests. #2358"
-)
-def test_implicit_typecasting(con, alltypes, test_data_db, udf_ll):
-    col = alltypes.tinyint_col
-    literal = ibis.literal(1000)
-    identity_func_testing(udf_ll, con, test_data_db, "int32", literal, col)
-
-
-def identity_func_testing(udf_ll, con, test_data_db, datatype, literal, column):
-    inputs = [datatype]
-    name = "__tmp_udf_" + util.guid()
-    func = udf_creation_to_op(
-        udf_ll, con, test_data_db, name, "Identity", inputs, datatype
-    )
-
-    expr = func(literal)
-    assert issubclass(type(expr), ir.Scalar)
-    result = con.execute(expr)
-    # Hacky
-    if datatype == "timestamp":
-        assert type(result) == pd.Timestamp
-    else:
-        lop = literal.op()
-        if isinstance(lop, ir.Literal):
-            np.testing.assert_allclose(lop.value, 5)
-        else:
-            np.testing.assert_allclose(result, con.execute(literal), 5)
 
-    expr = func(column)
-    assert issubclass(type(expr), ir.Column)
-    con.execute(expr)
-
-
-@pytest.mark.xfail(
-    reason="Unknown reason. xfailing to restore the CI for udf tests. #2358"
-)
-def test_mult_type_args(con, alltypes, test_data_db, udf_ll):
-    symbol = "AlmostAllTypes"
-    name = "most_types"
-    inputs = [
-        "string",
-        "boolean",
-        "int8",
-        "int16",
-        "int32",
-        "int64",
-        "float",
-        "double",
-    ]
-    output = "int32"
-
-    func = udf_creation_to_op(udf_ll, con, test_data_db, name, symbol, inputs, output)
-
-    expr = func("a", True, 1, 1, 1, 1, 1.0, 1.0)
-    result = con.execute(expr)
-    assert result == 8
-
-    table = alltypes
-    expr = func(
-        table.string_col,
-        table.bool_col,
-        table.tinyint_col,
-        table.tinyint_col,
-        table.smallint_col,
-        table.smallint_col,
-        1.0,
-        1.0,
-    )
-    con.execute(expr)
-
-
-@pytest.mark.xfail(
-    reason="Unknown reason. xfailing to restore the CI for udf tests. #2358"
-)
-def test_udf_varargs(con, alltypes, udf_ll, test_data_db):
-    t = alltypes
-
-    name = f"add_numbers_{util.guid()[:4]}"
-
-    input_sig = rules.varargs(rules.double)
-    func = api.wrap_udf(
-        udf_ll, input_sig, "double", "AddNumbers", name=name, database=test_data_db
-    )
-    con.create_function(func, database=test_data_db)
-
-    expr = func(t.double_col, t.double_col)
-    expr.execute()
-
-
-def test_drop_udf_not_exists(con):
-    random_name = util.guid()
-    with pytest.raises(com.MissingUDFError, match=random_name):
-        con.drop_udf(random_name)
-
-
-def test_drop_uda_not_exists(con):
-    random_name = util.guid()
-    with pytest.raises(com.MissingUDFError, match=random_name):
-        con.drop_uda(random_name)
-
-
-def udf_creation_to_op(udf_ll, con, test_data_db, name, symbol, inputs, output):
-    func = api.wrap_udf(udf_ll, inputs, output, symbol, name, database=test_data_db)
-
-    con.create_function(func, database=test_data_db)
-
-    assert con.exists_udf(name, test_data_db)
-    return func
-
-
-def test_ll_uda_not_supported(uda_ll):
-    # LLVM IR UDAs are not supported as of Impala 2.2
-    with pytest.raises(com.IbisError):
-        conforming_wrapper(uda_ll, ["double"], "double", "Variance")
-
-
-def conforming_wrapper(where, inputs, output, prefix, serialize=True, name=None):
-    kwds = {"name": name}
-    if serialize:
-        kwds["serialize_fn"] = f"{prefix}Serialize"
-    return api.wrap_uda(
-        where,
-        inputs,
-        output,
-        f"{prefix}Update",
-        init_fn=f"{prefix}Init",
-        merge_fn=f"{prefix}Merge",
-        finalize_fn=f"{prefix}Finalize",
-        **kwds,
-    )
-
-
-@pytest.fixture
-def wrapped_count_uda(uda_so, test_data_db):
-    name = f"user_count_{util.guid()}"
-    return api.wrap_uda(
-        uda_so, ["int32"], "int64", "CountUpdate", name=name, database=test_data_db
-    )
-
-
-def test_count_uda(con, alltypes, test_data_db, wrapped_count_uda):
-    con.create_function(wrapped_count_uda, database=test_data_db)
-
-    # it works!
-    wrapped_count_uda(alltypes.int_col).execute()
-
-
-def test_list_udas(con, wrapped_count_uda):
-    func = wrapped_count_uda
-    con.create_function(func)
-
-    funcs = con.list_udas()
-
-    ((name, inputs, output),) = (
-        (name, inputs, output) for _, name, inputs, output in funcs if func.name == name
-    )
-    assert func.name == name
-    assert func.inputs == inputs
-    assert func.output == output
-
-
-@pytest.fixture
-def inputs():
-    return ["string", "string"]
-
-
-@pytest.fixture
-def output():
-    return "int64"
-
-
-@pytest.fixture
-def name():
-    return "test_name"
-
-
-def test_create_udf(inputs, output, name, snapshot):
-    func = api.wrap_udf(
-        "/foo/bar.so",
-        inputs,
-        output,
-        so_symbol="testFunc",
-        name=name,
-    )
-    stmt = ddl.CreateUDF(func)
-    result = stmt.compile()
-    snapshot.assert_match(result, "out.sql")
-
-
-def test_create_udf_type_conversions(output, name, snapshot):
-    inputs = ["string", "int8", "int16", "int32"]
-    func = api.wrap_udf(
-        "/foo/bar.so",
-        inputs,
-        output,
-        so_symbol="testFunc",
-        name=name,
-    )
-    stmt = ddl.CreateUDF(func)
-    result = stmt.compile()
-    snapshot.assert_match(result, "out.sql")
-
-
-def test_delete_udf_simple(name, inputs, snapshot):
-    stmt = ddl.DropFunction(name, inputs)
-    result = stmt.compile()
-    snapshot.assert_match(result, "out.sql")
-
-
-def test_delete_udf_if_exists(name, inputs, snapshot):
-    stmt = ddl.DropFunction(name, inputs, must_exist=False)
-    result = stmt.compile()
-    snapshot.assert_match(result, "out.sql")
-
-
-def test_delete_udf_aggregate(name, inputs, snapshot):
-    stmt = ddl.DropFunction(name, inputs, aggregate=True)
-    result = stmt.compile()
-    snapshot.assert_match(result, "out.sql")
-
-
-def test_delete_udf_db(name, inputs, snapshot):
-    stmt = ddl.DropFunction(name, inputs, database="test")
-    result = stmt.compile()
-    snapshot.assert_match(result, "out.sql")
-
-
-@pytest.mark.parametrize("series", [True, False])
-def test_create_uda(name, inputs, output, series, snapshot):
-    func = api.wrap_uda(
-        "/foo/bar.so",
-        inputs,
-        output,
-        update_fn="Update",
-        init_fn="Init",
-        merge_fn="Merge",
-        finalize_fn="Finalize",
-        serialize_fn="Serialize" if series else None,
-    )
-    stmt = ddl.CreateUDA(func, name=name, database="bar")
-    result = stmt.compile()
-    snapshot.assert_match(result, "out.sql")
-
-
-def test_list_udf(snapshot):
-    stmt = ddl.ListFunction("test")
-    result = stmt.compile()
-    snapshot.assert_match(result, "out.sql")
-
-
-def test_list_udfs_like(snapshot):
-    stmt = ddl.ListFunction("test", like="identity")
-    result = stmt.compile()
-    snapshot.assert_match(result, "out.sql")
-
-
-def test_list_udafs(snapshot):
-    stmt = ddl.ListFunction("test", aggregate=True)
-    result = stmt.compile()
-    snapshot.assert_match(result, "out.sql")
-
-
-def test_list_udafs_like(snapshot):
-    stmt = ddl.ListFunction("test", like="identity", aggregate=True)
-    result = stmt.compile()
-    snapshot.assert_match(result, "out.sql")
+from ibis.impala import ddl
+import ibis.impala as api
+
+from ibis.common import IbisTypeError
+from ibis.compat import unittest, Decimal
+from ibis.expr.datatypes import validate_type
+from ibis.expr.tests.mocks import MockConnection
+from ibis.impala.tests.common import ImpalaE2E
+import ibis.expr.rules as rules
+import ibis.common as com
+import ibis.util as util
+
+
+class TestWrapping(unittest.TestCase):
+
+    def setUp(self):
+        self.con = MockConnection()
+        self.table = self.con.table('functional_alltypes')
+
+        self.i8 = self.table.tinyint_col
+        self.i16 = self.table.smallint_col
+        self.i32 = self.table.int_col
+        self.i64 = self.table.bigint_col
+        self.d = self.table.double_col
+        self.f = self.table.float_col
+        self.s = self.table.string_col
+        self.b = self.table.bool_col
+        self.t = self.table.timestamp_col
+        self.dec = self.con.table('tpch_customer').c_acctbal
+        self.all_cols = [self.i8, self.i16, self.i32, self.i64, self.d,
+                         self.f, self.dec, self.s, self.b, self.t]
+
+    def test_sql_generation(self):
+        func = api.scalar_function(['string'], 'string', name='Tester')
+        func.register('identity', 'udf_testing')
+
+        result = func('hello world')
+        assert result == "SELECT udf_testing.identity('hello world')"
+
+    def test_sql_generation_from_infoclass(self):
+        func = api.wrap_udf('test.so', ['string'], 'string', 'info_test')
+        repr(func)
+
+        func.register('info_test', 'udf_testing')
+        result = func('hello world')
+        assert result == "SELECT udf_testing.info_test('hello world')"
+
+    def test_udf_primitive_output_types(self):
+        types = [
+            ('boolean', True, self.b),
+            ('int8', 1, self.i8),
+            ('int16', 1, self.i16),
+            ('int32', 1, self.i32),
+            ('int64', 1, self.i64),
+            ('float', 1.0, self.f),
+            ('double', 1.0, self.d),
+            ('string', '1', self.s),
+            ('timestamp', ibis.timestamp('1961-04-10'), self.t)
+        ]
+        for t, sv, av in types:
+            func = self._register_udf([t], t, 'test')
+
+            ibis_type = validate_type(t)
+
+            expr = func(sv)
+            assert type(expr) == ibis_type.scalar_type()
+            expr = func(av)
+            assert type(expr) == ibis_type.array_type()
+
+    def test_uda_primitive_output_types(self):
+        types = [
+            ('boolean', True, self.b),
+            ('int8', 1, self.i8),
+            ('int16', 1, self.i16),
+            ('int32', 1, self.i32),
+            ('int64', 1, self.i64),
+            ('float', 1.0, self.f),
+            ('double', 1.0, self.d),
+            ('string', '1', self.s),
+            ('timestamp', ibis.timestamp('1961-04-10'), self.t)
+        ]
+        for t, sv, av in types:
+            func = self._register_uda([t], t, 'test')
+
+            ibis_type = validate_type(t)
+
+            expr1 = func(sv)
+            expr2 = func(sv)
+            assert isinstance(expr1, ibis_type.scalar_type())
+            assert isinstance(expr2, ibis_type.scalar_type())
+
+    def test_decimal(self):
+        func = self._register_udf(['decimal(9,0)'], 'decimal(9,0)', 'test')
+        expr = func(1.0)
+        assert type(expr) == ir.DecimalScalar
+        expr = func(self.dec)
+        assert type(expr) == ir.DecimalArray
+
+    def test_udf_invalid_typecasting(self):
+        cases = [
+            ('int8', self.all_cols[:1], self.all_cols[1:]),
+            ('int16', self.all_cols[:2], self.all_cols[2:]),
+            ('int32', self.all_cols[:3], self.all_cols[3:]),
+            ('int64', self.all_cols[:4], self.all_cols[4:]),
+            ('boolean', [], self.all_cols[:8] + self.all_cols[9:]),
+
+            # allowing double here for now
+            ('float', self.all_cols[:4], [self.s, self.b, self.t, self.dec]),
+
+            ('double', self.all_cols[:4], [self.s, self.b, self.t, self.dec]),
+            ('string', [], self.all_cols[:7] + self.all_cols[8:]),
+            ('timestamp', [], self.all_cols[:-1]),
+            ('decimal', [], self.all_cols[:4] + self.all_cols[7:])
+        ]
+
+        for t, valid_casts, invalid_casts in cases:
+            func = self._register_udf([t], 'int32', 'typecast')
+
+            for expr in valid_casts:
+                func(expr)
+
+            for expr in invalid_casts:
+                self.assertRaises(IbisTypeError, func, expr)
+
+    def test_mult_args(self):
+        func = self._register_udf(['int32', 'double', 'string',
+                                   'boolean', 'timestamp'],
+                                  'int64', 'mult_types')
+
+        expr = func(self.i32, self.d, self.s, self.b, self.t)
+        assert issubclass(type(expr), ir.ArrayExpr)
+
+        expr = func(1, 1.0, 'a', True, ibis.timestamp('1961-04-10'))
+        assert issubclass(type(expr), ir.ScalarExpr)
+
+    def _register_udf(self, inputs, output, name):
+        func = api.scalar_function(inputs, output, name=name)
+        func.register(name, 'ibis_testing')
+        return func
+
+    def _register_uda(self, inputs, output, name):
+        func = api.aggregate_function(inputs, output, name=name)
+        func.register(name, 'ibis_testing')
+        return func
+
+
+class TestUDFE2E(ImpalaE2E, unittest.TestCase):
+
+    def setUp(self):
+        super(TestUDFE2E, self).setUp()
+        self.udf_ll = pjoin(self.test_data_dir, 'udf/udf-sample.ll')
+        self.uda_ll = pjoin(self.test_data_dir, 'udf/uda-sample.ll')
+        self.uda_so = pjoin(self.test_data_dir, 'udf/libudasample.so')
+
+    @pytest.mark.udf
+    def test_identity_primitive_types(self):
+        cases = [
+            ('boolean', True, self.alltypes.bool_col),
+            ('int8', 5, self.alltypes.tinyint_col),
+            ('int16', 2**10, self.alltypes.smallint_col),
+            ('int32', 2**17, self.alltypes.int_col),
+            ('int64', 2**33, self.alltypes.bigint_col),
+            ('float', 3.14, self.alltypes.float_col),
+            ('double', 3.14, self.alltypes.double_col),
+            ('string', 'ibis', self.alltypes.string_col),
+            ('timestamp', ibis.timestamp('1961-04-10'),
+             self.alltypes.timestamp_col),
+        ]
+
+        for t, lit_val, array_val in cases:
+            if not isinstance(lit_val, ir.Expr):
+                lit_val = ibis.literal(lit_val)
+            self._identity_func_testing(t, lit_val, array_val)
+
+    @pytest.mark.udf
+    def test_decimal(self):
+        col = self.con.table('tpch_customer').c_acctbal
+        literal = ibis.literal(1).cast('decimal(12,2)')
+        name = '__tmp_udf_' + util.guid()
+        func = self._udf_creation_to_op(name, 'Identity',
+                                        ['decimal(12,2)'],
+                                        'decimal(12,2)')
+
+        expr = func(literal)
+        assert issubclass(type(expr), ir.ScalarExpr)
+        result = self.con.execute(expr)
+        assert result == Decimal(1)
+
+        expr = func(col)
+        assert issubclass(type(expr), ir.ArrayExpr)
+        self.con.execute(expr)
+
+    @pytest.mark.udf
+    def test_mixed_inputs(self):
+        name = 'two_args'
+        symbol = 'TwoArgs'
+        inputs = ['int32', 'int32']
+        output = 'int32'
+        func = self._udf_creation_to_op(name, symbol, inputs, output)
+
+        expr = func(self.alltypes.int_col, 1)
+        assert issubclass(type(expr), ir.ArrayExpr)
+        self.con.execute(expr)
+
+        expr = func(1, self.alltypes.int_col)
+        assert issubclass(type(expr), ir.ArrayExpr)
+        self.con.execute(expr)
+
+        expr = func(self.alltypes.int_col, self.alltypes.tinyint_col)
+        self.con.execute(expr)
+
+    @pytest.mark.udf
+    def test_implicit_typecasting(self):
+        col = self.alltypes.tinyint_col
+        literal = ibis.literal(1000)
+        self._identity_func_testing('int32', literal, col)
+
+    def _identity_func_testing(self, datatype, literal, column):
+        inputs = [datatype]
+        name = '__tmp_udf_' + util.guid()
+        func = self._udf_creation_to_op(name, 'Identity', inputs, datatype)
+
+        expr = func(literal)
+        assert issubclass(type(expr), ir.ScalarExpr)
+        result = self.con.execute(expr)
+        # Hacky
+        if datatype is 'timestamp':
+            import pandas as pd
+            assert type(result) == pd.tslib.Timestamp
+        else:
+            lop = literal.op()
+            if isinstance(lop, ir.Literal):
+                self.assertAlmostEqual(result, lop.value, 5)
+            else:
+                self.assertAlmostEqual(result, self.con.execute(literal), 5)
+
+        expr = func(column)
+        assert issubclass(type(expr), ir.ArrayExpr)
+        self.con.execute(expr)
+
+    @pytest.mark.udf
+    def test_mult_type_args(self):
+        symbol = 'AlmostAllTypes'
+        name = 'most_types'
+        inputs = ['string', 'boolean', 'int8', 'int16', 'int32',
+                  'int64', 'float', 'double']
+        output = 'int32'
+
+        func = self._udf_creation_to_op(name, symbol, inputs, output)
+
+        expr = func('a', True, 1, 1, 1, 1, 1.0, 1.0)
+        result = self.con.execute(expr)
+        assert result == 8
+
+        table = self.alltypes
+        expr = func(table.string_col, table.bool_col, table.tinyint_col,
+                    table.tinyint_col, table.smallint_col,
+                    table.smallint_col, 1.0, 1.0)
+        self.con.execute(expr)
+
+    def test_all_type_args(self):
+        pytest.skip('failing test, to be fixed later')
+
+        symbol = 'AllTypes'
+        name = 'all_types'
+        inputs = ['string', 'boolean', 'int8', 'int16', 'int32',
+                  'int64', 'float', 'double', 'decimal']
+        output = 'int32'
+
+        func = self._udf_creation_to_op(name, symbol, inputs, output)
+        expr = func('a', True, 1, 1, 1, 1, 1.0, 1.0, 1.0)
+        result = self.con.execute(expr)
+        assert result == 9
+
+    @pytest.mark.udf
+    def test_udf_varargs(self):
+        t = self.alltypes
+
+        name = 'add_numbers_{0}'.format(util.guid()[:4])
+
+        input_sig = rules.varargs(rules.double)
+        func = api.wrap_udf(self.udf_ll, input_sig, 'double', 'AddNumbers',
+                            name=name)
+        func.register(name, self.test_data_db)
+        self.con.create_function(func, database=self.test_data_db)
+
+        expr = func(t.double_col, t.double_col)
+        expr.execute()
+
+    def test_drop_udf_not_exists(self):
+        random_name = util.guid()
+        self.assertRaises(Exception, self.con.drop_udf, random_name)
+
+    def test_drop_uda_not_exists(self):
+        random_name = util.guid()
+        self.assertRaises(Exception, self.con.drop_uda, random_name)
+
+    def _udf_creation_to_op(self, name, symbol, inputs, output):
+        func = api.wrap_udf(self.udf_ll, inputs, output, symbol, name)
+
+        self.temp_udfs.append((name, inputs))
+
+        self.con.create_function(func, database=self.test_data_db)
+
+        func.register(name, self.test_data_db)
+
+        assert self.con.exists_udf(name, self.test_data_db)
+        return func
+
+    def test_ll_uda_not_supported(self):
+        # LLVM IR UDAs are not supported as of Impala 2.2
+        with self.assertRaises(com.IbisError):
+            self._conforming_wrapper(self.uda_ll, ['double'], 'double',
+                                     'Variance')
+
+    def _conforming_wrapper(self, where, inputs, output, prefix,
+                            serialize=True, name=None):
+        kwds = {
+            'name': name
+        }
+        if serialize:
+            kwds['serialize_fn'] = '{0}Serialize'.format(prefix)
+        return api.wrap_uda(where, inputs, output, '{0}Update'.format(prefix),
+                            init_fn='{0}Init'.format(prefix),
+                            merge_fn='{0}Merge'.format(prefix),
+                            finalize_fn='{0}Finalize'.format(prefix),
+                            **kwds)
+
+    @pytest.mark.udf
+    def test_count_uda(self):
+        func = self._wrap_count_uda()
+        func.register(func.name, self.test_data_db)
+        self.con.create_function(func, database=self.test_data_db)
+
+        # it works!
+        func(self.alltypes.int_col).execute()
+        self.temp_udas.append((func.name, ['int32']))
+
+    @pytest.mark.udf
+    def test_list_udas(self):
+        db = '__ibis_tmp_{0}'.format(util.guid())
+        self.con.create_database(db)
+        self.temp_databases.append(db)
+
+        func = self._wrap_count_uda()
+        self.con.create_function(func, database=db)
+
+        funcs = self.con.list_udas(database=db)
+
+        f = funcs[0]
+        assert f.name == func.name
+        assert f.inputs == func.inputs
+        assert f.output == func.output
+
+    @pytest.mark.udf
+    def test_drop_database_with_udfs_and_udas(self):
+        uda1 = self._wrap_count_uda()
+        uda2 = self._wrap_count_uda()
+
+        udf1 = api.wrap_udf(self.udf_ll, ['boolean'], 'boolean', 'Identity',
+                            'udf_{0}'.format(util.guid()))
+
+        db = '__ibis_tmp_{0}'.format(util.guid())
+
+        self.con.create_database(db)
+
+        self.con.create_function(uda1, database=db)
+        self.con.create_function(uda2, database=db)
+
+        self.con.create_function(udf1, database=db)
+
+        self.con.drop_database(db, force=True)
+
+        assert not self.con.exists_database(db)
+
+    def _wrap_count_uda(self, name=None):
+        if name is None:
+            name = 'user_count_{0}'.format(util.guid())
+        func = api.wrap_uda(self.uda_so, ['int32'], 'int64',
+                            'CountUpdate', name=name)
+        return func
+
+
+class TestUDFDDL(unittest.TestCase):
+
+    def setUp(self):
+        self.con = MockConnection()
+        self.name = 'test_name'
+        self.inputs = ['string', 'string']
+        self.output = 'int64'
+
+    def test_create_udf(self):
+        stmt = ddl.CreateFunction('/foo/bar.so', 'testFunc', self.inputs,
+                                  self.output, self.name)
+        result = stmt.compile()
+        expected = ("CREATE FUNCTION `test_name`(string, string) "
+                    "returns bigint "
+                    "location '/foo/bar.so' symbol='testFunc'")
+        assert result == expected
+
+    def test_create_udf_type_conversions(self):
+        stmt = ddl.CreateFunction('/foo/bar.so', 'testFunc',
+                                  ['string', 'int8', 'int16', 'int32'],
+                                  self.output, self.name)
+        result = stmt.compile()
+        expected = ("CREATE FUNCTION `test_name`(string, tinyint, "
+                    "smallint, int) returns bigint "
+                    "location '/foo/bar.so' symbol='testFunc'")
+        assert result == expected
+
+    def test_delete_udf_simple(self):
+        stmt = ddl.DropFunction(self.name, self.inputs)
+        result = stmt.compile()
+        expected = "DROP FUNCTION `test_name`(string, string)"
+        assert result == expected
+
+    def test_delete_udf_if_exists(self):
+        stmt = ddl.DropFunction(self.name, self.inputs, must_exist=False)
+        result = stmt.compile()
+        expected = "DROP FUNCTION IF EXISTS `test_name`(string, string)"
+        assert result == expected
+
+    def test_delete_udf_aggregate(self):
+        stmt = ddl.DropFunction(self.name, self.inputs, aggregate=True)
+        result = stmt.compile()
+        expected = "DROP AGGREGATE FUNCTION `test_name`(string, string)"
+        assert result == expected
+
+    def test_delete_udf_db(self):
+        stmt = ddl.DropFunction(self.name, self.inputs, database='test')
+        result = stmt.compile()
+        expected = "DROP FUNCTION test.`test_name`(string, string)"
+        assert result == expected
+
+    def test_create_uda(self):
+        def make_ex(serialize=False):
+            if serialize:
+                serialize = "\nserialize_fn='Serialize'"
+            else:
+                serialize = ""
+            return (("CREATE AGGREGATE FUNCTION "
+                     "bar.`test_name`(string, string)"
+                     " returns bigint location '/foo/bar.so'"
+                     "\ninit_fn='Init'"
+                     "\nupdate_fn='Update'"
+                     "\nmerge_fn='Merge'") +
+                    serialize +
+                    ("\nfinalize_fn='Finalize'"))
+
+        for ser in [True, False]:
+            stmt = ddl.CreateAggregateFunction('/foo/bar.so', self.inputs,
+                                               self.output, 'Update', 'Init',
+                                               'Merge',
+                                               'Serialize' if ser else None,
+                                               'Finalize', self.name, 'bar')
+            result = stmt.compile()
+            expected = make_ex(ser)
+            assert result == expected
+
+    def test_list_udf(self):
+        stmt = ddl.ListFunction('test')
+        result = stmt.compile()
+        expected = 'SHOW FUNCTIONS IN test'
+        assert result == expected
+
+    def test_list_udfs_like(self):
+        stmt = ddl.ListFunction('test', like='identity')
+        result = stmt.compile()
+        expected = "SHOW FUNCTIONS IN test LIKE 'identity'"
+        assert result == expected
+
+    def test_list_udafs(self):
+        stmt = ddl.ListFunction('test', aggregate=True)
+        result = stmt.compile()
+        expected = 'SHOW AGGREGATE FUNCTIONS IN test'
+        assert result == expected
+
+    def test_list_udafs_like(self):
+        stmt = ddl.ListFunction('test', like='identity', aggregate=True)
+        result = stmt.compile()
+        expected = "SHOW AGGREGATE FUNCTIONS IN test LIKE 'identity'"
+        assert result == expected
```

### Comparing `ibis_framework-9.0.0.dev686/ibis/backends/pandas/executor.py` & `ibis-framework-v0.6.0/ibis/expr/analysis.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,789 +1,855 @@
-from __future__ import annotations
+# Copyright 2014 Cloudera Inc.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+# http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from ibis.common import RelationError, ExpressionError
+from ibis.expr.datatypes import HasSchema
+from ibis.expr.window import window
+import ibis.expr.types as ir
+import ibis.expr.operations as ops
+import ibis.util as util
 
-import operator
-from functools import reduce
+# ---------------------------------------------------------------------
+# Some expression metaprogramming / graph transformations to support
+# compilation later
 
-import numpy as np
-import pandas as pd
-from packaging.version import parse as vparse
 
-import ibis.backends.pandas.kernels as pandas_kernels
-import ibis.expr.operations as ops
-from ibis.backends.pandas.convert import PandasConverter
-from ibis.backends.pandas.helpers import (
-    GroupedFrame,
-    PandasUtils,
-    RangeFrame,
-    RowsFrame,
-    UngroupedFrame,
-)
-from ibis.backends.pandas.rewrites import (
-    PandasAggregate,
-    PandasAsofJoin,
-    PandasJoin,
-    PandasLimit,
-    PandasRename,
-    PandasResetIndex,
-    PandasScalarSubquery,
-    PandasWindowFrame,
-    PandasWindowFunction,
-    plan,
-)
-from ibis.common.dispatch import Dispatched
-from ibis.common.exceptions import OperationNotDefinedError, UnboundExpressionError
-from ibis.formats.pandas import PandasData, PandasType
-from ibis.util import any_of, gen_name
-
-# ruff: noqa: F811
-
-
-class PandasExecutor(Dispatched, PandasUtils):
-    name = "pandas"
-    kernels = pandas_kernels
-
-    @classmethod
-    def visit(cls, op: ops.Node, **kwargs):
-        raise OperationNotDefinedError(
-            f"Operation {op!r} is not implemented for the pandas backend"
-        )
-
-    @classmethod
-    def visit(cls, op: ops.Literal, value, dtype):
-        if dtype.is_interval():
-            value = pd.Timedelta(value, dtype.unit.short)
-        elif dtype.is_array():
-            value = np.array(value)
-        elif dtype.is_date():
-            value = pd.Timestamp(value, tz="UTC").tz_localize(None)
-        return value
-
-    @classmethod
-    def visit(cls, op: ops.Field, rel, name):
-        return rel[name]
-
-    @classmethod
-    def visit(cls, op: ops.Alias, arg, name):
-        try:
-            return arg.rename(name)
-        except AttributeError:
-            return arg
-
-    @classmethod
-    def visit(cls, op: ops.SortKey, expr, ascending):
-        return expr
+def sub_for(expr, substitutions):
+    helper = _Substitutor(expr, substitutions)
+    return helper.get_result()
 
-    @classmethod
-    def visit(cls, op: ops.Cast, arg, to):
-        if arg is None:
-            return None
-        elif isinstance(arg, pd.Series):
-            return PandasConverter.convert_column(arg, to)
-        else:
-            return PandasConverter.convert_scalar(arg, to)
 
-    @classmethod
-    def visit(cls, op: ops.Greatest, arg):
-        return cls.columnwise(lambda df: df.max(axis=1), arg)
-
-    @classmethod
-    def visit(cls, op: ops.Least, arg):
-        return cls.columnwise(lambda df: df.min(axis=1), arg)
-
-    @classmethod
-    def visit(cls, op: ops.Coalesce, arg):
-        return cls.columnwise(lambda df: df.bfill(axis=1).iloc[:, 0], arg)
-
-    @classmethod
-    def visit(cls, op: ops.Value, **operands):
-        # automatically pick the correct kernel based on the operand types
-        typ = type(op)
-        name = op.name
-        dtype = PandasType.from_ibis(op.dtype)
-        kwargs = {"operands": operands, "name": name, "dtype": dtype}
-
-        # decimal operations have special implementations
-        if op.dtype.is_decimal():
-            func = cls.kernels.elementwise_decimal[typ]
-            return cls.elementwise(func, **kwargs)
-
-        # prefer generic implementations if available
-        if func := cls.kernels.generic.get(typ):
-            return cls.generic(func, **kwargs)
-
-        if len(operands) < 1:
-            raise OperationNotDefinedError(
-                f"No implementation found for operation {typ}"
-            )
-        _, *rest = operands.values()
-        is_multi_arg = bool(rest)
-        is_multi_column = any_of(rest, pd.Series)
-
-        if is_multi_column:
-            if func := cls.kernels.columnwise.get(typ):
-                return cls.columnwise(func, **kwargs)
-            elif func := cls.kernels.rowwise.get(typ):
-                return cls.rowwise(func, **kwargs)
-            else:
-                raise OperationNotDefinedError(
-                    "No columnwise or rowwise implementation found for "
-                    f"multi-column operation {typ}"
-                )
-        elif is_multi_arg:
-            if func := cls.kernels.columnwise.get(typ):
-                return cls.columnwise(func, **kwargs)
-            elif func := cls.kernels.serieswise.get(typ):
-                return cls.serieswise(func, **kwargs)
-            elif func := cls.kernels.rowwise.get(typ):
-                return cls.rowwise(func, **kwargs)
-            elif func := cls.kernels.elementwise.get(typ):
-                return cls.elementwise(func, **kwargs)
-            else:
-                raise OperationNotDefinedError(
-                    "No columnwise, serieswise, rowwise or elementwise "
-                    f"implementation found for multi-argument operation {typ}"
-                )
-        else:  # noqa: PLR5501
-            if func := cls.kernels.serieswise.get(typ):
-                return cls.serieswise(func, **kwargs)
-            elif func := cls.kernels.elementwise.get(typ):
-                return cls.elementwise(func, **kwargs)
-            else:
-                raise OperationNotDefinedError(
-                    "No serieswise or elementwise implementation found for "
-                    f"single-argument operation {typ}"
-                )
-
-    @classmethod
-    def visit(cls, op: ops.IsNan, arg):
-        try:
-            return np.isnan(arg)
-        except (TypeError, ValueError):
-            # if `arg` contains `None` np.isnan will complain
-            # so we take advantage of NaN not equaling itself
-            # to do the correct thing
-            return arg != arg
-
-    @classmethod
-    def visit(
-        cls, op: ops.SearchedCase | ops.SimpleCase, cases, results, default, base=None
-    ):
-        if base is not None:
-            cases = tuple(base == case for case in cases)
-        cases, _ = cls.asframe(cases, concat=False)
-        results, _ = cls.asframe(results, concat=False)
-        out = np.select(cases, results, default)
-        return pd.Series(out)
-
-    @classmethod
-    def visit(cls, op: ops.TimestampTruncate | ops.DateTruncate, arg, unit):
-        # TODO(kszucs): should use serieswise()
-        if vparse(pd.__version__) >= vparse("2.2"):
-            units = {"m": "min"}
-        else:
-            units = {"m": "Min", "ms": "L"}
-
-        unit = units.get(unit.short, unit.short)
-
-        if unit in "YMWD":
-            return arg.dt.to_period(unit).dt.to_timestamp()
-        try:
-            return arg.dt.floor(unit)
-        except ValueError:
-            return arg.dt.to_period(unit).dt.to_timestamp()
-
-    @classmethod
-    def visit(cls, op: ops.IntervalFromInteger, unit, **kwargs):
-        if unit.short in {"Y", "Q", "M", "W"}:
-            return cls.elementwise(lambda v: pd.DateOffset(**{unit.plural: v}), kwargs)
-        else:
-            return cls.serieswise(
-                lambda arg: arg.astype(f"timedelta64[{unit.short}]"), kwargs
-            )
-
-    @classmethod
-    def visit(cls, op: ops.BetweenTime, arg, lower_bound, upper_bound):
-        idx = pd.DatetimeIndex(arg)
-        if idx.tz is not None:
-            idx = idx.tz_convert(None)  # make naive because times are naive
-        indexer = idx.indexer_between_time(lower_bound, upper_bound)
-        result = np.zeros(len(arg), dtype=np.bool_)
-        result[indexer] = True
-        return pd.Series(result)
-
-    @classmethod
-    def visit(cls, op: ops.FindInSet, needle, values):
-        (needle, *haystack), _ = cls.asframe((needle, *values), concat=False)
-        condlist = [needle == col for col in haystack]
-        choicelist = [i for i, _ in enumerate(haystack)]
-        result = np.select(condlist, choicelist, default=-1)
-        return pd.Series(result, name=op.name)
-
-    @classmethod
-    def visit(cls, op: ops.Array, exprs):
-        return cls.rowwise(lambda row: np.array(row, dtype=object), exprs)
-
-    @classmethod
-    def visit(cls, op: ops.ArrayConcat, arg):
-        return cls.rowwise(lambda row: np.concatenate(row.values), arg)
-
-    @classmethod
-    def visit(cls, op: ops.Unnest, arg):
-        arg = cls.asseries(arg)
-        mask = arg.map(lambda v: bool(len(v)), na_action="ignore")
-        return arg[mask].explode()
-
-    @classmethod
-    def visit(
-        cls, op: ops.ElementWiseVectorizedUDF, func, func_args, input_type, return_type
-    ):
-        """Execute an elementwise UDF."""
-
-        res = func(*func_args)
-        if isinstance(res, pd.DataFrame):
-            # it is important otherwise it is going to fill up the memory
-            res = res.apply(lambda row: row.to_dict(), axis=1)
-
-        return res
-
-    ############################# Reductions ##################################
-
-    @classmethod
-    def visit(cls, op: ops.Reduction, arg, where):
-        func = cls.kernels.reductions[type(op)]
-        return cls.agg(func, arg, where)
-
-    @classmethod
-    def visit(cls, op: ops.CountStar, arg, where):
-        def agg(df):
-            if where is None:
-                return len(df)
-            else:
-                return df[where.name].sum()
+class _Substitutor(object):
 
-        return agg
+    def __init__(self, expr, substitutions, sub_memo=None):
+        self.expr = expr
 
-    @classmethod
-    def visit(cls, op: ops.CountDistinctStar, arg, where):
-        def agg(df):
-            if where is None:
-                return df.nunique()
-            else:
-                return df[where.name].nunique()
+        self.substitutions = substitutions
 
-        return agg
+        self._id_to_expr = {}
+        for k, v in substitutions:
+            self._id_to_expr[self._key(k)] = v
 
-    @classmethod
-    def visit(cls, op: ops.Arbitrary, arg, where):
-        return cls.agg(cls.kernels.reductions[ops.Arbitrary], arg, where)
+        self.sub_memo = sub_memo or {}
+        self.unchanged = True
 
-    @classmethod
-    def visit(cls, op: ops.ArgMin | ops.ArgMax, arg, key, where):
-        func = operator.methodcaller(op.__class__.__name__.lower())
+    def get_result(self):
+        expr = self.expr
+        node = expr.op()
 
-        if where is None:
+        if getattr(node, 'blocking', False):
+            return expr
 
-            def agg(df):
-                indices = func(df[key.name])
-                return df[arg.name].iloc[indices]
+        subbed_args = []
+        for arg in node.args:
+            if isinstance(arg, (tuple, list)):
+                subbed_arg = [self._sub_arg(x) for x in arg]
+            else:
+                subbed_arg = self._sub_arg(arg)
+            subbed_args.append(subbed_arg)
+
+        # Do not modify unnecessarily
+        if self.unchanged:
+            return expr
+
+        subbed_node = type(node)(*subbed_args)
+        if isinstance(expr, ir.ValueExpr):
+            result = expr._factory(subbed_node, name=expr._name)
+        else:
+            result = expr._factory(subbed_node)
+
+        return result
+
+    def _sub_arg(self, arg):
+        if isinstance(arg, ir.Expr):
+            subbed_arg = self.sub(arg)
+            if subbed_arg is not arg:
+                self.unchanged = False
         else:
+            # a string or some other thing
+            subbed_arg = arg
+
+        return subbed_arg
 
-            def agg(df):
-                mask = df[where.name]
-                filtered = df[mask]
-                indices = func(filtered[key.name])
-                return filtered[arg.name].iloc[indices]
+    def _key(self, expr):
+        return repr(expr.op())
 
-        return agg
+    def sub(self, expr):
+        key = self._key(expr)
 
-    @classmethod
-    def visit(cls, op: ops.Variance, arg, where, how):
-        ddof = {"pop": 0, "sample": 1}[how]
-        return cls.agg(lambda x: x.var(ddof=ddof), arg, where)
+        if key in self.sub_memo:
+            return self.sub_memo[key]
 
-    @classmethod
-    def visit(cls, op: ops.StandardDev, arg, where, how):
-        ddof = {"pop": 0, "sample": 1}[how]
-        return cls.agg(lambda x: x.std(ddof=ddof), arg, where)
+        if key in self._id_to_expr:
+            return self._id_to_expr[key]
 
-    @classmethod
-    def visit(cls, op: ops.Correlation, left, right, where, how):
-        if where is None:
+        result = self._sub(expr)
 
-            def agg(df):
-                return df[left.name].corr(df[right.name])
+        self.sub_memo[key] = result
+        return result
+
+    def _sub(self, expr):
+        helper = _Substitutor(expr, self.substitutions,
+                              sub_memo=self.sub_memo)
+        return helper.get_result()
+
+
+def substitute_parents(expr, lift_memo=None, past_projection=True):
+    rewriter = ExprSimplifier(expr, lift_memo=lift_memo,
+                              block_projection=not past_projection)
+    return rewriter.get_result()
+
+
+class ExprSimplifier(object):
+
+    """
+    Rewrite the input expression by replacing any table expressions part of a
+    "commutative table operation unit" (for lack of scientific term, a set of
+    operations that can be written down in any order and still yield the same
+    semantic result)
+    """
+
+    def __init__(self, expr, lift_memo=None, block_projection=False):
+        self.expr = expr
+        self.lift_memo = lift_memo or {}
+
+        self.block_projection = block_projection
+
+    def get_result(self):
+        expr = self.expr
+        node = expr.op()
+        if isinstance(node, ir.Literal):
+            return expr
+
+        # For table column references, in the event that we're on top of a
+        # projection, we need to check whether the ref comes from the base
+        # table schema or is a derived field. If we've projected out of
+        # something other than a physical table, then lifting should not occur
+        if isinstance(node, ops.TableColumn):
+            result = self._lift_TableColumn(expr, block=self.block_projection)
+            if result is not expr:
+                return result
+        # Temporary hacks around issues addressed in #109
+        elif isinstance(node, ops.Projection):
+            return self._lift_Projection(expr, block=self.block_projection)
+        elif isinstance(node, ops.Aggregation):
+            return self._lift_Aggregation(expr, block=self.block_projection)
+
+        unchanged = True
+
+        lifted_args = []
+        for arg in node.args:
+            lifted_arg, unch_arg = self._lift_arg(
+                arg, block=self.block_projection)
+            lifted_args.append(lifted_arg)
+
+            unchanged = unchanged and unch_arg
+
+        # Do not modify unnecessarily
+        if unchanged:
+            return expr
+
+        lifted_node = type(node)(*lifted_args)
+        if isinstance(expr, ir.ValueExpr):
+            result = expr._factory(lifted_node, name=expr._name)
         else:
+            result = expr._factory(lifted_node)
 
-            def agg(df):
-                mask = df[where.name]
-                lhs = df[left.name][mask]
-                rhs = df[right.name][mask]
-                return lhs.corr(rhs)
+        return result
 
-        return agg
+    def _lift_arg(self, arg, block=None):
+        unchanged = [True]
 
-    @classmethod
-    def visit(cls, op: ops.Covariance, left, right, where, how):
-        ddof = {"pop": 0, "sample": 1}[how]
-        if where is None:
+        def _lift(x):
+            if isinstance(x, ir.Expr):
+                lifted_arg = self.lift(x, block=block)
+                if lifted_arg is not x:
+                    unchanged[0] = False
+            else:
+                # a string or some other thing
+                lifted_arg = x
+            return lifted_arg
+
+        if arg is None:
+            return arg, True
 
-            def agg(df):
-                return df[left.name].cov(df[right.name], ddof=ddof)
+        if isinstance(arg, (tuple, list)):
+            result = [_lift(x) for x in arg]
         else:
+            result = _lift(arg)
 
-            def agg(df):
-                mask = df[where.name]
-                lhs = df[left.name][mask]
-                rhs = df[right.name][mask]
-                return lhs.cov(rhs, ddof=ddof)
+        return result, unchanged[0]
 
-        return agg
+    def lift(self, expr, block=None):
+        # This use of id() is OK since only for memoization
+        key = id(expr.op()), block
 
-    @classmethod
-    def visit(cls, op: ops.GroupConcat, arg, sep, where):
-        if where is None:
+        if key in self.lift_memo:
+            return self.lift_memo[key]
 
-            def agg(df):
-                return sep.join(df[arg.name].astype(str))
+        op = expr.op()
+
+        if isinstance(op, ops.ValueNode):
+            return self._sub(expr, block=block)
+        elif isinstance(op, ops.Filter):
+            result = self.lift(op.table, block=block)
+        elif isinstance(op, ops.Projection):
+            result = self._lift_Projection(expr, block=block)
+        elif isinstance(op, ops.Join):
+            result = self._lift_Join(expr, block=block)
+        elif isinstance(op, (ops.TableNode, HasSchema)):
+            return expr
         else:
+            raise NotImplementedError
+
+        # If we get here, time to record the modified expression in our memo to
+        # avoid excessive graph-walking
+        self.lift_memo[key] = result
+        return result
+
+    def _lift_TableColumn(self, expr, block=None):
+        node = expr.op()
 
-            def agg(df):
-                mask = df[where.name]
-                group = df[arg.name][mask]
-                if group.empty:
-                    return pd.NA
-                return sep.join(group)
+        tnode = node.table.op()
+        root = _base_table(tnode)
 
-        return agg
+        result = expr
+        if isinstance(root, ops.Projection):
+            can_lift = False
+
+            for val in root.selections:
+                if (isinstance(val.op(), ops.PhysicalTable) and
+                        node.name in val.schema()):
+
+                    can_lift = True
+                    lifted_root = self.lift(val)
+                elif (isinstance(val.op(), ops.TableColumn) and
+                      val.op().name == val.get_name() and
+                      node.name == val.get_name()):
+                    can_lift = True
+                    lifted_root = self.lift(val.op().table)
+
+                # XXX
+                # can_lift = False
+
+            # HACK: If we've projected a join, do not lift the children
+            # TODO: what about limits and other things?
+            # if isinstance(root.table.op(), Join):
+            #     can_lift = False
+
+            if can_lift and not block:
+                lifted_node = ops.TableColumn(node.name, lifted_root)
+                result = expr._factory(lifted_node, name=expr._name)
 
-    @classmethod
-    def visit(cls, op: ops.Quantile, arg, quantile, where):
-        return cls.agg(lambda x: x.quantile(quantile), arg, where)
+        return result
 
-    @classmethod
-    def visit(cls, op: ops.MultiQuantile, arg, quantile, where):
-        return cls.agg(lambda x: list(x.quantile(quantile)), arg, where)
+    def _lift_Aggregation(self, expr, block=None):
+        if block is None:
+            block = self.block_projection
 
-    @classmethod
-    def visit(
-        cls, op: ops.ReductionVectorizedUDF, func, func_args, input_type, return_type
-    ):
-        def agg(df):
-            args = [df[col.name] for col in func_args]
-            return func(*args)
+        op = expr.op()
 
-        return agg
+        # as exposed in #544, do not lift the table inside (which may be
+        # filtered or otherwise altered in some way) if blocking
+
+        if block:
+            lifted_table = op.table
+        else:
+            lifted_table = self.lift(op.table, block=True)
 
-    ############################# Analytic ####################################
+        unch = lifted_table is op.table
 
-    @classmethod
-    def visit(cls, op: ops.RowNumber):
-        def agg(df, order_keys):
-            return pd.Series(np.arange(len(df)), index=df.index)
+        lifted_aggs, unch1 = self._lift_arg(op.agg_exprs, block=True)
+        lifted_by, unch2 = self._lift_arg(op.by, block=True)
+        lifted_having, unch3 = self._lift_arg(op.having, block=True)
 
-        return agg
+        unchanged = unch and unch1 and unch2 and unch3
 
-    @classmethod
-    def visit(cls, op: ops.Lag | ops.Lead, arg, offset, default):
-        if isinstance(op, ops.Lag):
-            sign = operator.pos
+        if not unchanged:
+            lifted_op = ops.Aggregation(lifted_table, lifted_aggs,
+                                        by=lifted_by, having=lifted_having)
+            result = ir.TableExpr(lifted_op)
         else:
-            sign = operator.neg
+            result = expr
 
-        if op.offset is not None and op.offset.dtype.is_interval():
+        return result
+
+    def _lift_Projection(self, expr, block=None):
+        if block is None:
+            block = self.block_projection
 
-            def agg(df, order_keys):
-                df = df.set_index(order_keys)
-                col = df[arg.name].shift(freq=sign(offset))
-                res = col.reindex(df.index)
-                if not pd.isnull(default):
-                    res = res.fillna(default)
-                return res.reset_index(drop=True)
+        op = expr.op()
 
+        if block:
+            # GH #549: dig no further
+            return expr
         else:
-            offset = 1 if offset is None else offset
+            lifted_table, unch = self._lift_arg(op.table, block=True)
 
-            def agg(df, order_keys):
-                res = df[arg.name].shift(sign(offset))
-                if not pd.isnull(default):
-                    res = res.fillna(default)
-                return res
+        lifted_selections, unch_sel = self._lift_arg(op.selections, block=True)
+        unchanged = unch and unch_sel
+        if not unchanged:
+            lifted_projection = ops.Projection(lifted_table, lifted_selections)
+            result = ir.TableExpr(lifted_projection)
+        else:
+            result = expr
 
-        return agg
+        return result
 
-    @classmethod
-    def visit(cls, op: ops.MinRank | ops.DenseRank):
-        method = "dense" if isinstance(op, ops.DenseRank) else "min"
+    def _lift_Join(self, expr, block=None):
+        op = expr.op()
 
-        def agg(df, order_keys):
-            if len(order_keys) == 0:
-                raise ValueError("order_by argument is required for rank functions")
-            elif len(order_keys) == 1:
-                s = df[order_keys[0]]
-            else:
-                s = df[order_keys].apply(tuple, axis=1)
+        left_lifted = self.lift(op.left, block=block)
+        right_lifted = self.lift(op.right, block=block)
 
-            return s.rank(method=method).astype("int64") - 1
+        unchanged = (left_lifted is op.left and
+                     right_lifted is op.right)
 
-        return agg
+        # Fix predicates
+        lifted_preds = []
+        for x in op.predicates:
+            subbed = self._sub(x, block=True)
+            if subbed is not x:
+                unchanged = False
+            lifted_preds.append(subbed)
+
+        if not unchanged:
+            lifted_join = type(op)(left_lifted, right_lifted, lifted_preds)
+            result = ir.TableExpr(lifted_join)
+        else:
+            result = expr
+
+        return result
 
-    @classmethod
-    def visit(cls, op: ops.PercentRank):
-        def agg(df, order_keys):
-            if len(order_keys) == 0:
-                raise ValueError("order_by argument is required for rank functions")
-            elif len(order_keys) == 1:
-                s = df[order_keys[0]]
+    def _sub(self, expr, block=None):
+        # catchall recursive rewriter
+        if block is None:
+            block = self.block_projection
+
+        helper = ExprSimplifier(expr, lift_memo=self.lift_memo,
+                                block_projection=block)
+        return helper.get_result()
+
+
+def _base_table(table_node):
+    # Find the aggregate or projection root. Not proud of this
+    if isinstance(table_node, ir.BlockingTableNode):
+        return table_node
+    else:
+        return _base_table(table_node.table.op())
+
+
+def apply_filter(expr, predicates):
+    # This will attempt predicate pushdown in the cases where we can do it
+    # easily and safely
+
+    op = expr.op()
+
+    if isinstance(op, ops.Filter):
+        # Potential fusion opportunity. The predicates may need to be rewritten
+        # in terms of the child table. This prevents the broken ref issue
+        # (described in more detail in #59)
+        predicates = [sub_for(x, [(expr, op.table)]) for x in predicates]
+        return ops.Filter(op.table, op.predicates + predicates)
+
+    elif isinstance(op, (ops.Projection, ops.Aggregation)):
+        # if any of the filter predicates have the parent expression among
+        # their roots, then pushdown (at least of that predicate) is not
+        # possible
+
+        # It's not unusual for the filter to reference the projection
+        # itself. If a predicate can be pushed down, in this case we must
+        # rewrite replacing the table refs with the roots internal to the
+        # projection we are referencing
+        #
+        # If the filter references any new or derived aliases in the
+        #
+        # in pseudocode
+        # c = Projection(Join(a, b, jpreds), ppreds)
+        # filter_pred = c.field1 == c.field2
+        # Filter(c, [filter_pred])
+        #
+        # Assuming that the fields referenced by the filter predicate originate
+        # below the projection, we need to rewrite the predicate referencing
+        # the parent tables in the join being projected
+
+        # TODO: is partial pushdown (one or more, but not all of the passed
+        # predicates) something we should consider doing? Could be reasonable
+
+        # if isinstance(op, ops.Projection):
+        # else:
+        #     # Aggregation
+        #     can_pushdown = op.table.is_an
+
+        can_pushdown = _can_pushdown(op, predicates)
+
+        if can_pushdown:
+            predicates = [substitute_parents(x) for x in predicates]
+
+            # this will further fuse, if possible
+            filtered = op.table.filter(predicates)
+            result = op.substitute_table(filtered)
+        else:
+            result = ops.Filter(expr, predicates)
+    else:
+        result = ops.Filter(expr, predicates)
+
+    return result
+
+
+def _can_pushdown(op, predicates):
+    # Per issues discussed in #173
+    #
+    # The only case in which pushdown is possible is that all table columns
+    # referenced must meet all of the following (not that onerous in practice)
+    # criteria
+    #
+    # 1) Is a table column, not any other kind of expression
+    # 2) Is unaliased. So, if you project t3.foo AS bar, then filter on bar,
+    #    this cannot be pushed down (until we implement alias rewriting if
+    #    necessary)
+    # 3) Appears in the selections in the projection (either is part of one of
+    #    the entire tables or a single column selection)
+
+    can_pushdown = True
+    for pred in predicates:
+        validator = _PushdownValidate(op, pred)
+        predicate_is_valid = validator.get_result()
+        can_pushdown = can_pushdown and predicate_is_valid
+    return can_pushdown
+
+
+class _PushdownValidate(object):
+
+    def __init__(self, parent, predicate):
+        self.parent = parent
+        self.pred = predicate
+
+        self.validator = ExprValidator([self.parent.table])
+
+        self.valid = True
+
+    def get_result(self):
+        self._walk(self.pred)
+        return self.valid
+
+    def _walk(self, expr):
+        node = expr.op()
+        if isinstance(node, ops.TableColumn):
+            is_valid = self._validate_column(expr)
+            self.valid = self.valid and is_valid
+
+        for arg in node.flat_args():
+            if isinstance(arg, ir.ValueExpr):
+                self._walk(arg)
+
+            # Skip other types of exprs
+
+    def _validate_column(self, expr):
+        if isinstance(self.parent, ops.Projection):
+            return self._validate_projection(expr)
+        else:
+            validator = ExprValidator([self.parent.table])
+            return validator.validate(expr)
+
+    def _validate_projection(self, expr):
+        is_valid = False
+        node = expr.op()
+
+        # Has a different alias, invalid
+        if _is_aliased(expr):
+            return False
+
+        for val in self.parent.selections:
+            if (isinstance(val.op(), ops.PhysicalTable) and
+                    node.name in val.schema()):
+                is_valid = True
+            elif (isinstance(val.op(), ops.TableColumn) and
+                  node.name == val.get_name() and
+                  not _is_aliased(val)):
+                # Aliased table columns are no good
+                col_table = val.op().table.op()
+
+                lifted_node = substitute_parents(expr).op()
+
+                is_valid = (col_table.is_ancestor(node.table) or
+                            col_table.is_ancestor(lifted_node.table))
+
+                # is_valid = True
+
+        return is_valid
+
+
+def _is_aliased(col_expr):
+    return col_expr.op().name != col_expr.get_name()
+
+
+def windowize_function(expr, w=None):
+    def _check_window(x):
+        # Hmm
+        arg, window = x.op().args
+        if isinstance(arg.op(), ops.RowNumber):
+            if len(window._order_by) == 0:
+                raise ExpressionError('RowNumber requires explicit '
+                                      'window sort')
+
+        return x
+
+    def _windowize(x, w):
+        if not isinstance(x.op(), ops.WindowOp):
+            walked = _walk(x, w)
+        else:
+            window_arg, window_w = x.op().args
+            walked_child = _walk(window_arg, w)
+
+            if walked_child is not window_arg:
+                walked = x._factory(ops.WindowOp(walked_child, window_w),
+                                    name=x._name)
             else:
-                s = df[order_keys].apply(tuple, axis=1)
+                walked = x
 
-            return s.rank(method="min").sub(1).div(len(df) - 1)
+        op = walked.op()
+        if (isinstance(op, ops.AnalyticOp) or
+                getattr(op, '_reduction', False)):
+            if w is None:
+                w = window()
+            return _check_window(walked.over(w))
+        elif isinstance(op, ops.WindowOp):
+            if w is not None:
+                return _check_window(walked.over(w))
+            else:
+                return _check_window(walked)
+        else:
+            return walked
 
-        return agg
+    def _walk(x, w):
+        op = x.op()
 
-    @classmethod
-    def visit(cls, op: ops.CumeDist):
-        def agg(df, order_keys):
-            if len(order_keys) == 0:
-                raise ValueError("order_by argument is required for rank functions")
-            elif len(order_keys) == 1:
-                s = df[order_keys[0]]
+        unchanged = True
+        windowed_args = []
+        for arg in op.args:
+            if not isinstance(arg, ir.ValueExpr):
+                windowed_args.append(arg)
+                continue
+
+            new_arg = _windowize(arg, w)
+            unchanged = unchanged and arg is new_arg
+            windowed_args.append(new_arg)
+
+        if not unchanged:
+            new_op = type(op)(*windowed_args)
+            return x._factory(new_op, name=x._name)
+        else:
+            return x
+
+    return _windowize(expr, w)
+
+
+class Projector(object):
+
+    """
+    Analysis and validation of projection operation, taking advantage of
+    "projection fusion" opportunities where they exist, i.e. combining
+    compatible projections together rather than nesting them. Translation /
+    evaluation later will not attempt to do any further fusion /
+    simplification.
+    """
+
+    def __init__(self, parent, proj_exprs):
+        self.parent = parent
+        self.input_exprs = proj_exprs
+
+        node = self.parent.op()
+
+        if isinstance(node, ops.Projection):
+            roots = [node]
+        else:
+            roots = node.root_tables()
+
+        self.parent_roots = roots
+
+        clean_exprs = []
+        # validator = ExprValidator([parent])
+
+        for expr in proj_exprs:
+            # Perform substitution only if we share common roots
+            # if validator.shares_one_root(expr):
+            #     expr = substitute_parents(expr, past_projection=False)
+            expr = windowize_function(expr)
+            clean_exprs.append(expr)
+
+        self.clean_exprs = clean_exprs
+
+    def get_result(self):
+        roots = self.parent_roots
+
+        if len(roots) == 1 and isinstance(roots[0], ops.Projection):
+            fused_op = self._check_fusion(roots[0])
+            if fused_op is not None:
+                return fused_op
+
+        return ops.Projection(self.parent, self.clean_exprs)
+
+    def _check_fusion(self, root):
+        roots = root.table._root_tables()
+        validator = ExprValidator([root.table])
+        fused_exprs = []
+        can_fuse = False
+        for val in self.clean_exprs:
+            # XXX
+            lifted_val = substitute_parents(val)
+
+            # a * projection
+            if (isinstance(val, ir.TableExpr) and
+                (self.parent.op().is_ancestor(val) or
+                 # gross we share the same table root. Better way to
+                 # detect?
+                 len(roots) == 1 and val._root_tables()[0] is roots[0])):
+                can_fuse = True
+                fused_exprs.extend(root.selections)
+            elif validator.validate(lifted_val):
+                fused_exprs.append(lifted_val)
+            elif not validator.validate(val):
+                can_fuse = False
+                break
             else:
-                s = df[order_keys].apply(tuple, axis=1)
+                fused_exprs.append(val)
 
-            return s.rank(method="average", pct=True)
+        if can_fuse:
+            return ops.Projection(root.table, fused_exprs)
+        else:
+            return None
 
-        return agg
 
-    @classmethod
-    def visit(
-        cls, op: ops.AnalyticVectorizedUDF, func, func_args, input_type, return_type
-    ):
-        def agg(df, order_keys):
-            args = [df[col.name] for col in func_args]
-            res = func(*args)
-            if isinstance(res, pd.DataFrame):
-                # it is important otherwise it is going to fill up the memory
-                res = res.apply(lambda row: row.to_dict(), axis=1)
-            return res
-
-        return agg
-
-    ############################ Window functions #############################
-
-    @classmethod
-    def visit(cls, op: ops.WindowBoundary, value, preceding):
-        return value
-
-    @classmethod
-    def visit(cls, op: PandasWindowFrame, table, how, start, end, group_by, order_by):
-        if start is not None and op.start.preceding:
-            start = -start
-        if end is not None and op.end.preceding:
-            end = -end
-
-        table = table.assign(__start__=start, __end__=end)
-
-        # TODO(kszucs): order by ibis.random() is not supported because it is
-        # excluded from the group by keys due to its scalar shape
-        group_keys = [group.name for group in op.group_by]
-        order_keys = [key.name for key in op.order_by if key.shape.is_columnar()]
-        ascending = [key.ascending for key in op.order_by if key.shape.is_columnar()]
-
-        if order_by:
-            table = table.sort_values(order_keys, ascending=ascending, kind="mergesort")
-
-        if group_by:
-            frame = GroupedFrame(df=table, group_keys=group_keys)
-        else:
-            frame = UngroupedFrame(df=table)
-
-        if start is None and end is None:
-            return frame
-        elif how == "rows":
-            return RowsFrame(parent=frame)
-        elif how == "range":
-            if len(order_keys) != 1:
-                raise NotImplementedError(
-                    "Only single column order by is supported for range window frames"
-                )
-            return RangeFrame(parent=frame, order_key=order_keys[0])
-        else:
-            raise NotImplementedError(f"Unsupported window frame type: {how}")
-
-    @classmethod
-    def visit(cls, op: PandasWindowFunction, func, frame):
-        if isinstance(op.func, ops.Analytic):
-            order_keys = [key.name for key in op.frame.order_by]
-            return frame.apply_analytic(func, order_keys=order_keys)
-        else:
-            return frame.apply_reduction(func)
-
-    ############################ Relational ###################################
-
-    @classmethod
-    def visit(cls, op: ops.DatabaseTable, name, schema, source, namespace):
-        try:
-            return source.dictionary[name]
-        except KeyError:
-            raise UnboundExpressionError(
-                f"{name} is not a table in the {source.name!r} backend, you "
-                "probably tried to execute an expression without a data source"
-            )
-
-    @classmethod
-    def visit(cls, op: ops.InMemoryTable, name, schema, data):
-        return data.to_frame()
-
-    @classmethod
-    def visit(cls, op: ops.DummyTable, values):
-        df, _ = cls.asframe(values)
-        return df
-
-    @classmethod
-    def visit(cls, op: ops.Reference, parent, **kwargs):
-        return parent
-
-    @classmethod
-    def visit(cls, op: PandasRename, parent, mapping):
-        return parent.rename(columns=mapping)
-
-    @classmethod
-    def visit(cls, op: PandasLimit, parent, n, offset):
-        n = n.iat[0, 0]
-        offset = offset.iat[0, 0]
-        if n is None:
-            return parent.iloc[offset:]
-        else:
-            return parent.iloc[offset : offset + n]
-
-    @classmethod
-    def visit(cls, op: PandasResetIndex, parent):
-        return parent.reset_index(drop=True)
-
-    @classmethod
-    def visit(cls, op: ops.Sample, parent, fraction, method, seed):
-        return parent.sample(frac=fraction, random_state=seed)
-
-    @classmethod
-    def visit(cls, op: ops.Project, parent, values):
-        df, all_scalars = cls.asframe(values)
-        if all_scalars and len(parent) != len(df):
-            df = cls.concat([df] * len(parent))
-        return df
-
-    @classmethod
-    def visit(cls, op: ops.Filter, parent, predicates):
-        if predicates:
-            pred = reduce(operator.and_, predicates)
-            if len(pred) != len(parent):
-                raise RuntimeError(
-                    "Selection predicate length does not match underlying table"
-                )
-            parent = parent.loc[pred].reset_index(drop=True)
-        return parent
-
-    @classmethod
-    def visit(cls, op: ops.Sort, parent, keys):
-        # 1. add sort key columns to the dataframe if they are not already present
-        # 2. sort the dataframe using those columns
-        # 3. drop the sort key columns
-        ascending = [key.ascending for key in op.keys]
-        newcols = {gen_name("sort_key"): col for col in keys}
-        names = list(newcols.keys())
-        df = parent.assign(**newcols)
-        df = df.sort_values(
-            by=names, ascending=ascending, ignore_index=True, kind="mergesort"
-        )
-        return df.drop(columns=names)
-
-    @classmethod
-    def visit(cls, op: PandasAggregate, parent, groups, metrics):
-        if groups:
-            parent = parent.groupby([col.name for col in groups.values()])
-            metrics = {k: parent.apply(v) for k, v in metrics.items()}
-            result = cls.concat(metrics, axis=1).reset_index()
-            renames = {v.name: k for k, v in op.groups.items()}
-            return result.rename(columns=renames)
-        else:
-            results = {k: v(parent) for k, v in metrics.items()}
-            combined, _ = cls.asframe(results)
-            return combined
-
-    @classmethod
-    def visit(cls, op: PandasJoin, how, left, right, left_on, right_on):
-        # broadcast predicates if they are scalar values
-        left_on = [cls.asseries(v, like=left) for v in left_on]
-        right_on = [cls.asseries(v, like=right) for v in right_on]
-
-        if how == "cross":
-            assert not left_on and not right_on
-            return cls.merge(left, right, how="cross")
-        elif how == "anti":
-            df = cls.merge(
-                left,
-                right,
-                how="outer",
-                left_on=left_on,
-                right_on=right_on,
-                indicator=True,
-            )
-            df = df[df["_merge"] == "left_only"]
-            return df.drop(columns=["_merge"])
-        elif how == "semi":
-            mask = cls.asseries(True, like=left)
-            for left_pred, right_pred in zip(left_on, right_on):
-                mask = mask & left_pred.isin(right_pred)
-            return left[mask]
-        else:
-            left_columns = {gen_name("left"): s for s in left_on}
-            right_columns = {gen_name("right"): s for s in right_on}
-            left_keys = list(left_columns.keys())
-            right_keys = list(right_columns.keys())
-            left = left.assign(**left_columns)
-            right = right.assign(**right_columns)
-            df = left.merge(right, how=how, left_on=left_keys, right_on=right_keys)
-            return df
-
-    @classmethod
-    def visit(
-        cls,
-        op: PandasAsofJoin,
-        how,
-        left,
-        right,
-        left_on,
-        right_on,
-        left_by,
-        right_by,
-        operator,
-    ):
-        # broadcast predicates if they are scalar values
-        left_on = [cls.asseries(v, like=left) for v in left_on]
-        left_by = [cls.asseries(v, like=left) for v in left_by]
-        right_on = [cls.asseries(v, like=right) for v in right_on]
-        right_by = [cls.asseries(v, like=right) for v in right_by]
-
-        # merge_asof only works with column names not with series
-        left_on = {gen_name("left"): s for s in left_on}
-        left_by = {gen_name("left"): s for s in left_by}
-        right_on = {gen_name("right"): s for s in right_on}
-        right_by = {gen_name("right"): s for s in right_by}
-
-        left = left.assign(**left_on, **left_by)
-        right = right.assign(**right_on, **right_by)
-
-        # construct the appropriate flags for merge_asof
-        if operator == ops.LessEqual:
-            direction = "forward"
-            allow_exact_matches = True
-        elif operator == ops.GreaterEqual:
-            direction = "backward"
-            allow_exact_matches = True
-        elif operator == ops.Less:
-            direction = "forward"
-            allow_exact_matches = False
-        elif operator == ops.Greater:
-            direction = "backward"
-            allow_exact_matches = False
-        elif operator == ops.Equals:
-            direction = "nearest"
-            allow_exact_matches = True
-        else:
-            raise NotImplementedError(
-                f"Operator {operator} not supported for asof join"
-            )
-
-        # merge_asof requires the left side to be sorted by the join keys
-        left = left.sort_values(by=list(left_on.keys()))
-        df = cls.merge_asof(
-            left,
-            right,
-            left_on=list(left_on.keys()),
-            right_on=list(right_on.keys()),
-            left_by=list(left_by.keys()) or None,
-            right_by=list(right_by.keys()) or None,
-            direction=direction,
-            allow_exact_matches=allow_exact_matches,
-        )
-        return df
-
-    @classmethod
-    def visit(cls, op: ops.Union, left, right, distinct):
-        result = cls.concat([left, right], axis=0)
-        return result.drop_duplicates() if distinct else result
-
-    @classmethod
-    def visit(cls, op: ops.Intersection, left, right, distinct):
-        if not distinct:
-            raise NotImplementedError(
-                "`distinct=False` is not supported by the pandas backend"
-            )
-        return left.merge(right, on=list(left.columns), how="inner")
-
-    @classmethod
-    def visit(cls, op: ops.Difference, left, right, distinct):
-        if not distinct:
-            raise NotImplementedError(
-                "`distinct=False` is not supported by the pandas backend"
-            )
-        merged = left.merge(right, on=list(left.columns), how="outer", indicator=True)
-        result = merged[merged["_merge"] == "left_only"].drop("_merge", axis=1)
-        return result
+class ExprValidator(object):
+
+    def __init__(self, exprs):
+        self.parent_exprs = exprs
 
-    @classmethod
-    def visit(cls, op: ops.Distinct, parent):
-        return parent.drop_duplicates()
-
-    @classmethod
-    def visit(cls, op: ops.DropNa, parent, how, subset):
-        if op.subset is not None:
-            subset = [col.name for col in op.subset]
-        else:
-            subset = None
-        return parent.dropna(how=how, subset=subset)
-
-    @classmethod
-    def visit(cls, op: ops.FillNa, parent, replacements):
-        return parent.fillna(replacements)
-
-    @classmethod
-    def visit(cls, op: ops.InValues, value, options):
-        if isinstance(value, pd.Series):
-            return value.isin(options)
-        else:
-            return value in options
-
-    @classmethod
-    def visit(cls, op: ops.InSubquery, rel, needle):
-        first_column = rel.iloc[:, 0]
-        if isinstance(needle, pd.Series):
-            return needle.isin(first_column)
-        else:
-            return needle in first_column
-
-    @classmethod
-    def visit(cls, op: PandasScalarSubquery, rel):
-        return rel.iat[0, 0]
-
-    @classmethod
-    def execute(cls, node, backend, params):
-        def fn(node, _, **kwargs):
-            return cls.visit(node, **kwargs)
-
-        original = node
-        node = node.to_expr().as_table().op()
-        node = plan(node, backend=backend, params=params)
-        df = node.map_clear(fn)
-
-        # TODO(kszucs): add a flag to disable this conversion because it can be
-        # expensive for columns with object dtype
-        df = PandasData.convert_table(df, node.schema)
-        if isinstance(original, ops.Value):
-            if original.shape.is_scalar():
-                return df.iloc[0, 0]
-            elif original.shape.is_columnar():
-                return df.iloc[:, 0]
+        self.roots = []
+        for expr in self.parent_exprs:
+            self.roots.extend(expr._root_tables())
+
+    def has_common_roots(self, expr):
+        return self.validate(expr)
+
+    def validate(self, expr):
+        op = expr.op()
+        if isinstance(op, ops.TableColumn):
+            if self._among_roots(op.table.op()):
+                return True
+        elif isinstance(op, ops.Projection):
+            if self._among_roots(op):
+                return True
+
+        expr_roots = expr._root_tables()
+        for root in expr_roots:
+            if not self._among_roots(root):
+                return False
+        return True
+
+    def _among_roots(self, node):
+        return self.roots_shared(node) > 0
+
+    def roots_shared(self, node):
+        count = 0
+        for root in self.roots:
+            if root.is_ancestor(node):
+                count += 1
+        return count
+
+    def shares_some_roots(self, expr):
+        expr_roots = expr._root_tables()
+        return any(self._among_roots(root)
+                   for root in expr_roots)
+
+    def shares_one_root(self, expr):
+        expr_roots = expr._root_tables()
+        total = sum(self.roots_shared(root)
+                    for root in expr_roots)
+        return total == 1
+
+    def shares_multiple_roots(self, expr):
+        expr_roots = expr._root_tables()
+        total = sum(self.roots_shared(expr_roots)
+                    for root in expr_roots)
+        return total > 1
+
+    def validate_all(self, exprs):
+        for expr in exprs:
+            self.assert_valid(expr)
+
+    def assert_valid(self, expr):
+        if not self.validate(expr):
+            msg = self._error_message(expr)
+            raise RelationError(msg)
+
+    def _error_message(self, expr):
+        return ('The expression %s does not fully originate from '
+                'dependencies of the table expression.' % repr(expr))
+
+
+class CommonSubexpr(object):
+
+    def __init__(self, exprs):
+        self.parent_exprs = exprs
+
+    def validate(self, expr):
+        if isinstance(expr, ir.TableExpr):
+            if not self._check(expr):
+                return False
+
+        op = expr.op()
+
+        for arg in op.flat_args():
+            if not isinstance(arg, ir.Expr):
+                continue
+            elif not isinstance(arg, ir.TableExpr):
+                if not self.validate(arg):
+                    return False
             else:
-                raise TypeError(f"Unexpected shape: {original.shape}")
+                # Table expression. Must be found in a parent table expr a
+                # blocking root of one of the parent tables
+                if not self._check(arg):
+                    return False
+
+        return True
+
+    def _check(self, expr):
+        # Table dependency matches one of the parent exprs
+        is_valid = False
+        for parent in self.parent_exprs:
+            is_valid = is_valid or self._check_table(parent, expr)
+        return is_valid
+
+    def _check_table(self, parent, needle):
+        def _matches(expr):
+            op = expr.op()
+
+            if expr.equals(needle):
+                return True
+
+            if isinstance(op, ir.BlockingTableNode):
+                return False
+
+            for arg in op.flat_args():
+                if not isinstance(arg, ir.Expr):
+                    continue
+                if _matches(arg):
+                    return True
+
+            return True
+
+        return _matches(parent)
+
+    def validate_all(self, exprs):
+        for expr in exprs:
+            self.assert_valid(expr)
+
+    def assert_valid(self, expr):
+        if not self.validate(expr):
+            msg = self._error_message(expr)
+            raise RelationError(msg)
+
+    def _error_message(self, expr):
+        return ('The expression %s does not fully originate from '
+                'dependencies of the table expression.' % repr(expr))
+
+
+class FilterValidator(ExprValidator):
+
+    """
+    Filters need not necessarily originate fully from the ancestors of the
+    table being filtered. The key cases for this are
+
+    - Scalar reductions involving some other tables
+    - Array expressions involving other tables only (mapping to "uncorrelated
+      subqueries" in SQL-land)
+    - Reductions or array expressions like the above, but containing some
+      predicate with a record-specific interdependency ("correlated subqueries"
+      in SQL)
+    """
+
+    def validate(self, expr):
+        op = expr.op()
+
+        is_valid = True
+
+        if isinstance(op, ops.Contains):
+            value_valid = ExprValidator.validate(self, op.value)
+            is_valid = value_valid
+        else:
+            roots_valid = []
+            for arg in op.flat_args():
+                if isinstance(arg, ir.ScalarExpr):
+                    # arg_valid = True
+                    pass
+                elif isinstance(arg, (ir.ArrayExpr, ir.AnalyticExpr)):
+                    roots_valid.append(self.shares_some_roots(arg))
+                elif isinstance(arg, ir.Expr):
+                    raise NotImplementedError
+                else:
+                    # arg_valid = True
+                    pass
+
+            is_valid = any(roots_valid)
+
+        return is_valid
+
+
+def find_source_table(expr):
+    # A more complex version of _find_base_table.
+    # TODO: Revisit/refactor this all at some point
+    node = expr.op()
+
+    # First table expression observed for each argument that the expr
+    # depends on
+    first_tables = []
+
+    def push_first(arg):
+        if not isinstance(arg, ir.Expr):
+            return
+        if isinstance(arg, ir.TableExpr):
+            first_tables.append(arg)
         else:
-            return df
+            collect(arg.op())
+
+    def collect(node):
+        for arg in node.flat_args():
+            push_first(arg)
+
+    collect(node)
+    options = util.unique_by_key(first_tables, id)
+
+    if len(options) > 1:
+        raise NotImplementedError
+
+    return options[0]
+
+
+def unwrap_ands(expr):
+    out_exprs = []
+
+    def walk(expr):
+        op = expr.op()
+        if isinstance(op, ops.Comparison):
+            out_exprs.append(expr)
+        elif isinstance(op, ops.And):
+            walk(op.left)
+            walk(op.right)
+        else:
+            raise Exception('Invalid predicate: {0!s}'
+                            .format(expr._repr()))
+
+    walk(expr)
+    return out_exprs
```

### Comparing `ibis_framework-9.0.0.dev686/ibis/backends/pandas/tests/test_functions.py` & `ibis-framework-v0.6.0/ibis/impala/tests/test_pandas_interop.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,291 +1,250 @@
-from __future__ import annotations
-
-import decimal
-import functools
-import math
-import operator
-from operator import methodcaller
+# Copyright 2015 Cloudera Inc.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+# http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 import numpy as np
-import pandas as pd
 import pytest
-from pytest import param
 
-import ibis
+from pandas.util.testing import assert_frame_equal
+import pandas as pd
+
+from ibis.compat import unittest
+from ibis.common import IbisTypeError
+from ibis.impala.pandas_interop import pandas_to_ibis_schema, DataFrameWriter
+from ibis.impala.tests.common import ImpalaE2E
 import ibis.expr.datatypes as dt
-from ibis.backends.pandas.tests.conftest import TestConf as tm
-from ibis.backends.pandas.udf import udf
+import ibis.expr.types as ir
+import ibis.util as util
+import ibis
+
+
+class TestPandasTypeInterop(unittest.TestCase):
+
+    def test_series_to_ibis_literal(self):
+        values = [1, 2, 3, 4]
+        s = pd.Series(values)
+
+        expr = ir.as_value_expr(s)
+        expected = ir.sequence(list(s))
+        assert expr.equals(expected)
+
+
+class TestPandasSchemaInference(unittest.TestCase):
+
+    def test_dtype_bool(self):
+        df = pd.DataFrame({'col': [True, False, False]})
+        inferred = pandas_to_ibis_schema(df)
+        expected = ibis.schema([('col', 'boolean')])
+        assert inferred == expected
+
+    def test_dtype_int8(self):
+        df = pd.DataFrame({'col': np.int8([-3, 9, 17])})
+        inferred = pandas_to_ibis_schema(df)
+        expected = ibis.schema([('col', 'int8')])
+        assert inferred == expected
+
+    def test_dtype_int16(self):
+        df = pd.DataFrame({'col': np.int16([-5, 0, 12])})
+        inferred = pandas_to_ibis_schema(df)
+        expected = ibis.schema([('col', 'int16')])
+        assert inferred == expected
+
+    def test_dtype_int32(self):
+        df = pd.DataFrame({'col': np.int32([-12, 3, 25000])})
+        inferred = pandas_to_ibis_schema(df)
+        expected = ibis.schema([('col', 'int32')])
+        assert inferred == expected
+
+    def test_dtype_int64(self):
+        df = pd.DataFrame({'col': np.int64([102, 67228734, -0])})
+        inferred = pandas_to_ibis_schema(df)
+        expected = ibis.schema([('col', 'int64')])
+        assert inferred == expected
+
+    def test_dtype_float32(self):
+        df = pd.DataFrame({'col': np.float32([45e-3, -0.4, 99.])})
+        inferred = pandas_to_ibis_schema(df)
+        expected = ibis.schema([('col', 'float')])
+        assert inferred == expected
+
+    def test_dtype_float64(self):
+        df = pd.DataFrame({'col': np.float64([-3e43, 43., 10000000.])})
+        inferred = pandas_to_ibis_schema(df)
+        expected = ibis.schema([('col', 'double')])
+        assert inferred == expected
+
+    def test_dtype_uint8(self):
+        df = pd.DataFrame({'col': np.uint8([3, 0, 16])})
+        inferred = pandas_to_ibis_schema(df)
+        expected = ibis.schema([('col', 'int16')])
+        assert inferred == expected
+
+    def test_dtype_uint16(self):
+        df = pd.DataFrame({'col': np.uint16([5569, 1, 33])})
+        inferred = pandas_to_ibis_schema(df)
+        expected = ibis.schema([('col', 'int32')])
+        assert inferred == expected
+
+    def test_dtype_uint32(self):
+        df = pd.DataFrame({'col': np.uint32([100, 0, 6])})
+        inferred = pandas_to_ibis_schema(df)
+        expected = ibis.schema([('col', 'int64')])
+        assert inferred == expected
+
+    def test_dtype_uint64(self):
+        df = pd.DataFrame({'col': np.uint64([666, 2, 3])})
+        with self.assertRaises(IbisTypeError):
+            inferred = pandas_to_ibis_schema(df)  # noqa
+
+    def test_dtype_datetime64(self):
+        df = pd.DataFrame({
+            'col': [pd.Timestamp('2010-11-01 00:01:00'),
+                    pd.Timestamp('2010-11-01 00:02:00.1000'),
+                    pd.Timestamp('2010-11-01 00:03:00.300000')]})
+        inferred = pandas_to_ibis_schema(df)
+        expected = ibis.schema([('col', 'timestamp')])
+        assert inferred == expected
+
+    def test_dtype_timedelta64(self):
+        df = pd.DataFrame({
+            'col': [pd.Timedelta('1 days'),
+                    pd.Timedelta('-1 days 2 min 3us'),
+                    pd.Timedelta('-2 days +23:57:59.999997')]})
+        inferred = pandas_to_ibis_schema(df)
+        expected = ibis.schema([('col', 'int64')])
+        assert inferred == expected
+
+    def test_dtype_string(self):
+        df = pd.DataFrame({'col': ['foo', 'bar', 'hello']})
+        inferred = pandas_to_ibis_schema(df)
+        expected = ibis.schema([('col', 'string')])
+        assert inferred == expected
+
+    def test_dtype_categorical(self):
+        df = pd.DataFrame({'col': ['a', 'b', 'c', 'a']}, dtype='category')
+        inferred = pandas_to_ibis_schema(df)
+        expected = ibis.schema([('col', dt.Category(3))])
+        assert inferred == expected
+
+
+exhaustive_df = pd.DataFrame({
+    'bigint_col': np.array([0, 10, 20, 30, 40, 50, 60, 70, 80, 90],
+                           dtype='i8'),
+    'bool_col': np.array([True, False, True, False, True, None,
+                          True, False, True, False], dtype=np.bool_),
+    'bool_obj_col': np.array([True, False, np.nan, False, True, np.nan,
+                              True, np.nan, True, False], dtype=np.object_),
+    'date_string_col': ['11/01/10', None, '11/01/10', '11/01/10',
+                        '11/01/10', '11/01/10', '11/01/10', '11/01/10',
+                        '11/01/10', '11/01/10'],
+    'double_col': np.array([0.0, 10.1, np.nan, 30.299999999999997,
+                            40.399999999999999, 50.5, 60.599999999999994,
+                            70.700000000000003, 80.799999999999997,
+                            90.899999999999991], dtype=np.float64),
+    'float_col': np.array([np.nan, 1.1000000238418579, 2.2000000476837158,
+                           3.2999999523162842, 4.4000000953674316, 5.5,
+                           6.5999999046325684, 7.6999998092651367,
+                           8.8000001907348633,
+                           9.8999996185302734], dtype='f4'),
+    'int_col': np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype='i4'),
+    'month': [11, 11, 11, 11, 2, 11, 11, 11, 11, 11],
+    'smallint_col': np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype='i2'),
+    'string_col': ['0', '1', None, 'double , whammy', '4', '5',
+                   '6', '7', '8', '9'],
+    'timestamp_col': [pd.Timestamp('2010-11-01 00:00:00'),
+                      None,
+                      pd.Timestamp('2010-11-01 00:02:00.100000'),
+                      pd.Timestamp('2010-11-01 00:03:00.300000'),
+                      pd.Timestamp('2010-11-01 00:04:00.600000'),
+                      pd.Timestamp('2010-11-01 00:05:00.100000'),
+                      pd.Timestamp('2010-11-01 00:06:00.150000'),
+                      pd.Timestamp('2010-11-01 00:07:00.210000'),
+                      pd.Timestamp('2010-11-01 00:08:00.280000'),
+                      pd.Timestamp('2010-11-01 00:09:00.360000')],
+    'tinyint_col': np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype='i1'),
+    'year': [2010, 2010, 2010, 2010, 2010, 2009, 2009, 2009, 2009, 2009]})
+
+
+class TestPandasInterop(ImpalaE2E, unittest.TestCase):
+
+    @classmethod
+    def setUpClass(cls):
+        super(TestPandasInterop, cls).setUpClass()
+        cls.alltypes = cls.alltypes.execute()
+
+    def test_alltypes_roundtrip(self):
+        self._check_roundtrip(self.alltypes)
+
+    def test_writer_cleanup_deletes_hdfs_dir(self):
+        writer = DataFrameWriter(self.con, self.alltypes)
+
+        path = writer.write_temp_csv()
+        assert self.con.hdfs.exists(path)
+
+        writer.cleanup()
+        assert not self.con.hdfs.exists(path)
+
+        # noop
+        writer.cleanup()
+        assert not self.con.hdfs.exists(path)
+
+    @pytest.mark.superuser
+    def test_create_table_from_dataframe(self):
+        tname = 'tmp_pandas_{0}'.format(util.guid())
+        self.con.create_table(tname, self.alltypes, database=self.tmp_db,
+                              location=self._create_777_tmp_dir())
+        self.temp_tables.append(tname)
+
+        table = self.con.table(tname, database=self.tmp_db)
+        df = table.execute()
+        assert_frame_equal(df, self.alltypes)
+
+    @pytest.mark.superuser
+    def test_insert(self):
+        schema = pandas_to_ibis_schema(exhaustive_df)
+
+        table_name = 'tmp_pandas_{0}'.format(util.guid())
+        self.con.create_table(table_name, database=self.tmp_db,
+                              schema=schema,
+                              location=self._create_777_tmp_dir())
+        self.temp_tables.append(table_name)
+
+        self.con.insert(table_name, exhaustive_df.iloc[:4],
+                        database=self.tmp_db)
+        self.con.insert(table_name, exhaustive_df.iloc[4:],
+                        database=self.tmp_db)
+
+        table = self.con.table(table_name, database=self.tmp_db)
+
+        result = (table.execute()
+                  .sort_index(by='tinyint_col')
+                  .reset_index(drop=True))
+        assert_frame_equal(result, exhaustive_df)
+
+    def test_insert_partition(self):
+        # overwrite
+
+        # no overwrite
+        pass
+
+    def test_round_trip_exhaustive(self):
+        self._check_roundtrip(exhaustive_df)
+
+    def _check_roundtrip(self, df):
+        writer = DataFrameWriter(self.con, df)
+        path = writer.write_temp_csv()
 
+        table = writer.delimited_table(path)
+        df2 = table.execute()
 
-@pytest.mark.parametrize(
-    "op",
-    [
-        # comparison
-        operator.eq,
-        operator.ne,
-        operator.lt,
-        operator.le,
-        operator.gt,
-        operator.ge,
-    ],
-)
-def test_binary_operations(t, df, op):
-    expr = op(t.plain_float64, t.plain_int64)
-    result = expr.execute()
-    expected = op(df.plain_float64, df.plain_int64)
-    tm.assert_series_equal(result, expected)
-
-
-@pytest.mark.parametrize("op", [operator.and_, operator.or_, operator.xor])
-def test_binary_boolean_operations(t, df, op):
-    expr = op(t.plain_int64 == 1, t.plain_int64 == 2)
-    result = expr.execute()
-    expected = op(df.plain_int64 == 1, df.plain_int64 == 2)
-    tm.assert_series_equal(result, expected)
-
-
-def operate(func):
-    @functools.wraps(func)
-    def wrapper(*args, **kwargs):
-        try:
-            return func(*args, **kwargs)
-        except decimal.InvalidOperation:
-            return decimal.Decimal("NaN")
-
-    return wrapper
-
-
-@pytest.mark.parametrize(
-    ("ibis_func", "pandas_func"),
-    [
-        param(methodcaller("round"), round, id="round"),
-        param(
-            methodcaller("round", 2),
-            lambda x: x.quantize(decimal.Decimal(".00")),
-            id="round_2",
-        ),
-        param(
-            methodcaller("round", 0),
-            lambda x: x.quantize(decimal.Decimal("0.")),
-            id="round_0",
-        ),
-        param(methodcaller("ceil"), lambda x: decimal.Decimal(math.ceil(x)), id="ceil"),
-        param(
-            methodcaller("floor"), lambda x: decimal.Decimal(math.floor(x)), id="floor"
-        ),
-        param(
-            methodcaller("sign"),
-            lambda x: x if not x else decimal.Decimal(1).copy_sign(x),
-            id="sign",
-        ),
-        param(methodcaller("sqrt"), operate(lambda x: x.sqrt()), id="sqrt"),
-        param(
-            methodcaller("log", 2),
-            operate(lambda x: x.ln() / decimal.Decimal(2).ln()),
-            id="log_2",
-        ),
-        param(methodcaller("ln"), operate(lambda x: x.ln()), id="ln"),
-        param(
-            methodcaller("log2"),
-            operate(lambda x: x.ln() / decimal.Decimal(2).ln()),
-            id="log2",
-        ),
-        param(methodcaller("log10"), operate(lambda x: x.log10()), id="log10"),
-    ],
-)
-def test_math_functions_decimal(t, df, ibis_func, pandas_func):
-    dtype = dt.Decimal(12, 3)
-    context = decimal.Context(prec=dtype.precision)
-
-    def normalize(x):
-        x = context.create_decimal(x)
-        p = decimal.Decimal(
-            f"{'0' * (dtype.precision - dtype.scale)}.{'0' * dtype.scale}"
-        )
-        return x.quantize(p)
-
-    expr = ibis_func(t.float64_as_strings.cast(dtype))
-    result = expr.execute()
-
-    expected = (
-        df.float64_as_strings.apply(normalize).apply(pandas_func).apply(normalize)
-    )
-    tm.assert_series_equal(result, expected.astype(expr.type().to_pandas()))
-
-
-def test_round_decimal_with_negative_places(t):
-    type = dt.Decimal(12, 3)
-    expr = t.float64_as_strings.cast(type).round(-1)
-    result = expr.execute()
-    expected = pd.Series(
-        list(map(decimal.Decimal, ["1.0E+2", "2.3E+2", "-1.00E+3"])),
-        name="float64_as_strings",
-    )
-    tm.assert_series_equal(result, expected)
-
-
-@pytest.mark.parametrize(
-    ("ibis_func", "pandas_func"),
-    [
-        (lambda x: x.quantile(0), lambda x: x.quantile(0)),
-        (lambda x: x.quantile(1), lambda x: x.quantile(1)),
-        (lambda x: x.quantile(0.5), lambda x: x.quantile(0.5)),
-    ],
-)
-def test_quantile(t, df, ibis_func, pandas_func):
-    result = ibis_func(t.float64_with_zeros).execute()
-    expected = pandas_func(df.float64_with_zeros)
-    assert result == expected
-
-    assert result == expected
-
-    result = ibis_func(t.int64_with_zeros).execute()
-    expected = pandas_func(df.int64_with_zeros)
-    assert result == expected
-
-
-@pytest.mark.parametrize(
-    ("ibis_func", "pandas_func"),
-    [
-        (
-            lambda x: x.quantile([0.25, 0.75]),
-            lambda x: np.array(x.quantile([0.25, 0.75])),
-        )
-    ],
-)
-@pytest.mark.parametrize("column", ["float64_with_zeros", "int64_with_zeros"])
-def test_quantile_multi(t, df, ibis_func, pandas_func, column):
-    expr = ibis_func(t[column])
-    result = expr.execute()
-    expected = pandas_func(df[column])
-    np.testing.assert_array_equal(result, expected)
-
-
-@pytest.mark.parametrize(
-    ("ibis_func", "exc"),
-    [
-        # no lower/upper specified
-        (lambda x: x.clip(), ValueError),
-        # out of range on quantile
-        (lambda x: x.quantile(5.0), ValueError),
-    ],
-)
-def test_arraylike_functions_transform_errors(t, ibis_func, exc):
-    with pytest.raises(exc):
-        ibis_func(t.float64_with_zeros).execute()
-
-
-def test_quantile_multi_array_access(client, t, df):
-    quantile = t.float64_with_zeros.quantile([0.25, 0.5])
-    expr = quantile[0], quantile[1]
-    result = tuple(map(client.execute, expr))
-    expected = tuple(df.float64_with_zeros.quantile([0.25, 0.5]))
-    assert result == expected
-
-
-@pytest.mark.parametrize(
-    (
-        "left",
-        "right",
-        "expected_value",
-        "expected_type",
-        "left_dtype",
-        "right_dtype",
-    ),
-    [
-        (True, 1, True, bool, dt.boolean, dt.int64),
-        (True, 1.0, True, bool, dt.boolean, dt.float64),
-        (True, True, True, bool, dt.boolean, dt.boolean),
-        (False, 0, False, bool, dt.boolean, dt.int64),
-        (False, 0.0, False, bool, dt.boolean, dt.float64),
-        (False, False, False, bool, dt.boolean, dt.boolean),
-        (1, True, 1, int, dt.int64, dt.boolean),
-        (1, 1.0, 1, int, dt.int64, dt.float64),
-        (1, 1, 1, int, dt.int64, dt.int64),
-        (0, False, 0, int, dt.int64, dt.boolean),
-        (0, 0.0, 0, int, dt.int64, dt.float64),
-        (0, 0, 0, int, dt.int64, dt.int64),
-        (1.0, True, 1.0, float, dt.float64, dt.boolean),
-        (1.0, 1, 1.0, float, dt.float64, dt.int64),
-        (1.0, 1.0, 1.0, float, dt.float64, dt.float64),
-        (0.0, False, 0.0, float, dt.float64, dt.boolean),
-        (0.0, 0, 0.0, float, dt.float64, dt.int64),
-        (0.0, 0.0, 0.0, float, dt.float64, dt.float64),
-    ],
-)
-def test_execute_with_same_hash_value_in_scope(
-    left, right, expected_value, expected_type, left_dtype, right_dtype
-):
-    with pytest.warns(FutureWarning, match="v9.0"):
-
-        @udf.elementwise([left_dtype, right_dtype], left_dtype)
-        def my_func(x, _):
-            return x
-
-    df = pd.DataFrame({"left": [left], "right": [right]})
-    con = ibis.pandas.connect()
-    table = con.from_dataframe(df)
-
-    expr = my_func(table.left, table.right)
-    result = con.execute(expr)
-    assert isinstance(result, pd.Series)
-
-    result = result.tolist()
-    assert result == [expected_value]
-    assert type(result[0]) is expected_type
-
-
-def test_ifelse_returning_bool():
-    one = ibis.literal(1)
-    two = ibis.literal(2)
-    true = ibis.literal(True)
-    false = ibis.literal(False)
-    expr = ibis.ifelse(one + one == two, true, false)
-    result = ibis.pandas.connect().execute(expr)
-    assert result is True or result is np.True_
-
-
-@pytest.mark.parametrize(
-    ("dtype", "value"),
-    [
-        pytest.param(dt.float64, 1, id="float_int"),
-        pytest.param(dt.float64, True, id="float_bool"),
-        pytest.param(dt.int64, 1.0, id="int_float"),
-        pytest.param(dt.int64, True, id="int_bool"),
-        pytest.param(dt.boolean, 1.0, id="bool_float"),
-        pytest.param(dt.boolean, 1, id="bool_int"),
-    ],
-)
-def test_signature_does_not_match_input_type(dtype, value):
-    with pytest.warns(FutureWarning, match="v9.0"):
-
-        @udf.elementwise([dtype], dtype)
-        def func(x):
-            return x
-
-    df = pd.DataFrame({"col": [value]})
-    table = ibis.pandas.connect().from_dataframe(df)
-
-    result = table.col.execute()
-    assert isinstance(result, pd.Series)
-
-    result = result.tolist()
-    assert result == [value]
-    assert type(result[0]) is type(value)
-
-
-@pytest.mark.parametrize(
-    ("ibis_func", "pandas_func"),
-    [
-        (
-            lambda x: x.approx_median(),
-            lambda x: x.median(),
-        )
-    ],
-)
-@pytest.mark.parametrize("column", ["float64_with_zeros", "int64_with_zeros"])
-def test_approx_median(t, df, ibis_func, pandas_func, column):
-    expr = ibis_func(t[column])
-    result = expr.execute()
-    expected = pandas_func(df[column])
-    assert expected == result
+        assert_frame_equal(df2, df)
```

### Comparing `ibis_framework-9.0.0.dev686/ibis/backends/polars/compiler.py` & `ibis-framework-v0.6.0/ibis/impala/compiler.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,1361 +1,1315 @@
-from __future__ import annotations
+# Copyright 2014 Cloudera Inc.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+# http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-import calendar
+from six import StringIO
 import datetime
-import math
-import operator
-from collections.abc import Mapping
-from functools import partial, reduce, singledispatch
-from math import isnan
-
-import numpy as np
-import pandas as pd
-import polars as pl
-from packaging.version import parse as vparse
 
-import ibis.common.exceptions as com
+import ibis
+import ibis.expr.analysis as L
 import ibis.expr.datatypes as dt
+import ibis.expr.types as ir
 import ibis.expr.operations as ops
-from ibis.backends.pandas.rewrites import PandasAsofJoin, PandasJoin, PandasRename
-from ibis.expr.operations.udf import InputType
-from ibis.formats.polars import PolarsType
-from ibis.util import gen_name
+import ibis.expr.temporal as tempo
 
+import ibis.sql.compiler as comp
+import ibis.sql.transforms as transforms
 
-def _expr_method(expr, op, methods):
-    for m in methods:
-        if hasattr(expr, m):
-            return getattr(expr, m)
-    raise com.TranslationError(
-        f"Failed to translate {op}; expected expression method(s) not found:\n{methods!r}"
-    )
+import ibis.impala.identifiers as identifiers
 
+import ibis.common as com
+import ibis.util as util
 
-def _literal_value(op, nan_as_none=False):
-    # TODO(kszucs): broadcast and apply UDF on two columns using concat_list
-    # TODO(kszucs): better error message
-    if op is None:
-        return None
-    elif not isinstance(op, ops.Literal):
-        raise com.UnsupportedArgumentError(
-            f"Polars does not support columnar argument {op.name}"
-        )
-    else:
-        value = op.value
-        return None if nan_as_none and isnan(value) else value
 
+def build_ast(expr, context=None):
+    builder = ImpalaQueryBuilder(expr, context=context)
+    return builder.get_result()
 
-@singledispatch
-def translate(expr, *, ctx):
-    raise NotImplementedError(expr)
 
+def _get_query(expr, context):
+    ast = build_ast(expr, context)
+    query = ast.queries[0]
 
-@translate.register(ops.Node)
-def operation(op, **_):
-    raise com.OperationNotDefinedError(f"No translation rule for {type(op)}")
+    return query
 
 
-@translate.register(ops.DatabaseTable)
-def table(op, **_):
-    return op.source._tables[op.name]
+def to_sql(expr, context=None):
+    query = _get_query(expr, context)
+    return query.compile()
 
 
-@translate.register(ops.DummyTable)
-def dummy_table(op, **kw):
-    selections = [translate(arg, **kw) for name, arg in op.values.items()]
-    return pl.DataFrame().lazy().select(selections)
+# ----------------------------------------------------------------------
+# Select compilation
 
+class ImpalaSelectBuilder(comp.SelectBuilder):
 
-@translate.register(ops.InMemoryTable)
-def in_memory_table(op, **_):
-    return op.data.to_polars(op.schema).lazy()
+    @property
+    def _select_class(self):
+        return ImpalaSelect
 
 
-@translate.register(ops.Alias)
-def alias(op, **kw):
-    arg = translate(op.arg, **kw)
-    return arg.alias(op.name)
+class ImpalaQueryBuilder(comp.QueryBuilder):
 
+    select_builder = ImpalaSelectBuilder
 
-def _make_duration(value, dtype):
-    kwargs = {f"{dtype.resolution}s": value}
-    return pl.duration(**kwargs)
+    @property
+    def _make_context(self):
+        return ImpalaContext
 
+    @property
+    def _union_class(self):
+        return ImpalaUnion
 
-@translate.register(ops.Literal)
-def literal(op, **_):
-    value = op.value
-    dtype = op.dtype
 
-    if dtype.is_array():
-        value = pl.Series("", value)
-        typ = PolarsType.from_ibis(dtype)
-        val = pl.lit(value, dtype=typ)
-        return val.implode()
-    elif dtype.is_struct():
-        values = [
-            pl.lit(v, dtype=PolarsType.from_ibis(dtype[k])).alias(k)
-            for k, v in value.items()
-        ]
-        return pl.struct(values)
-    elif dtype.is_interval():
-        return _make_duration(value, dtype)
-    elif dtype.is_null():
-        return pl.lit(value)
-    elif dtype.is_binary():
-        return pl.lit(value)
-    else:
-        typ = PolarsType.from_ibis(dtype)
-        return pl.lit(op.value, dtype=typ)
+class ImpalaContext(comp.QueryContext):
 
+    def _to_sql(self, expr, ctx):
+        return to_sql(expr, context=ctx)
 
-_TIMESTAMP_SCALE_TO_UNITS = {
-    0: "s",
-    1: "ms",
-    2: "ms",
-    3: "ms",
-    4: "us",
-    5: "us",
-    6: "us",
-    7: "ns",
-    8: "ns",
-    9: "ns",
-}
 
+class ImpalaSelect(comp.Select):
+
+    """
+    A SELECT statement which, after execution, might yield back to the user a
+    table, array/list, or scalar value, depending on the expression that
+    generated it
+    """
+
+    def compile(self):
+        """
+        This method isn't yet idempotent; calling multiple times may yield
+        unexpected results
+        """
+        # Can't tell if this is a hack or not. Revisit later
+        self.context.set_query(self)
+
+        # If any subqueries, translate them and add to beginning of query as
+        # part of the WITH section
+        with_frag = self.format_subqueries()
+
+        # SELECT
+        select_frag = self.format_select_set()
+
+        # FROM, JOIN, UNION
+        from_frag = self.format_table_set()
+
+        # WHERE
+        where_frag = self.format_where()
+
+        # GROUP BY and HAVING
+        groupby_frag = self.format_group_by()
+
+        # ORDER BY and LIMIT
+        order_frag = self.format_postamble()
+
+        # Glue together the query fragments and return
+        query = _join_not_none('\n', [with_frag, select_frag, from_frag,
+                                      where_frag, groupby_frag, order_frag])
+
+        return query
 
-@translate.register(ops.Cast)
-def cast(op, **_):
-    return _cast(op, strict=True)
-
-
-@translate.register(ops.TryCast)
-def try_cast(op, **_):
-    return _cast(op, strict=False)
-
-
-def _cast(op, strict=True, **kw):
-    arg = translate(op.arg, **kw)
-    dtype = op.arg.dtype
-    to = op.to
-
-    if to.is_interval():
-        if not strict:
-            raise NotImplementedError(f"Unsupported try_cast to type: {to!r}")
-        return _make_duration(arg, to)
-    elif to.is_date():
-        if not strict:
-            raise NotImplementedError(f"Unsupported try_cast to type: {to!r}")
-        if dtype.is_string():
-            return arg.str.strptime(pl.Date, "%Y-%m-%d")
-    elif to.is_timestamp():
-        if not strict:
-            raise NotImplementedError(f"Unsupported try_cast to type: {to!r}")
-
-        time_zone = to.timezone
-        time_unit = _TIMESTAMP_SCALE_TO_UNITS.get(to.scale, "us")
-
-        if dtype.is_integer():
-            typ = pl.Datetime(time_unit="us", time_zone=time_zone)
-            arg = (arg * 1_000_000).cast(typ)
-            if time_unit != "us":
-                arg = arg.dt.truncate(f"1{time_unit}")
-            return arg.alias(op.name)
-        elif dtype.is_string():
-            typ = pl.Datetime(time_unit=time_unit, time_zone=time_zone)
-            arg = arg.str.strptime(typ)
-            if time_unit == "s":
-                return arg.dt.truncate("1s")
-            return arg
-
-    typ = PolarsType.from_ibis(to)
-    return arg.cast(typ, strict=strict)
-
-
-@translate.register(ops.Field)
-def column(op, **_):
-    return pl.col(op.name)
-
-
-@translate.register(ops.SortKey)
-def sort_key(op, **kw):
-    arg = translate(op.expr, **kw)
-    descending = op.descending
-    try:
-        return arg.sort(descending=descending)
-    except TypeError:  # pragma: no cover
-        return arg.sort(reverse=descending)  # pragma: no cover
-
-
-@translate.register(ops.Project)
-def project(op, **kw):
-    lf = translate(op.parent, **kw)
-
-    selections = []
-    unnests = []
-    scalars = []
-    for name, arg in op.values.items():
-        if isinstance(arg, ops.Unnest):
-            translated = translate(arg.arg, **kw).alias(name)
-            unnests.append(name)
-            selections.append(translated)
-        elif isinstance(arg, ops.Value):
-            translated = translate(arg, **kw).alias(name)
-            if arg.shape.is_scalar():
-                scalars.append(translated)
-                selections.append(pl.col(name))
+    def format_subqueries(self):
+        if len(self.subqueries) == 0:
+            return
+
+        context = self.context
+
+        buf = StringIO()
+        buf.write('WITH ')
+
+        for i, expr in enumerate(self.subqueries):
+            if i > 0:
+                buf.write(',\n')
+            formatted = util.indent(context.get_compiled_expr(expr), 2)
+            alias = context.get_ref(expr)
+            buf.write('{0} AS (\n{1}\n)'.format(alias, formatted))
+
+        return buf.getvalue()
+
+    def format_select_set(self):
+        # TODO:
+        context = self.context
+        formatted = []
+        for expr in self.select_set:
+            if isinstance(expr, ir.ValueExpr):
+                expr_str = self._translate(expr, named=True)
+            elif isinstance(expr, ir.TableExpr):
+                # A * selection, possibly prefixed
+                if context.need_aliases():
+                    alias = context.get_ref(expr)
+
+                    # materialized join will not have an alias. see #491
+                    expr_str = '{0}.*'.format(alias) if alias else '*'
+                else:
+                    expr_str = '*'
+            formatted.append(expr_str)
+
+        buf = StringIO()
+        line_length = 0
+        max_length = 70
+        tokens = 0
+        for i, val in enumerate(formatted):
+            # always line-break for multi-line expressions
+            if val.count('\n'):
+                if i:
+                    buf.write(',')
+                buf.write('\n')
+                indented = util.indent(val, self.indent)
+                buf.write(indented)
+
+                # set length of last line
+                line_length = len(indented.split('\n')[-1])
+                tokens = 1
+            elif (tokens > 0 and line_length and
+                  len(val) + line_length > max_length):
+                # There is an expr, and adding this new one will make the line
+                # too long
+                buf.write(',\n       ') if i else buf.write('\n')
+                buf.write(val)
+                line_length = len(val) + 7
+                tokens = 1
             else:
-                selections.append(translated)
+                if i:
+                    buf.write(',')
+                buf.write(' ')
+                buf.write(val)
+                tokens += 1
+                line_length += len(val) + 2
+
+        if self.distinct:
+            select_key = 'SELECT DISTINCT'
         else:
-            raise com.TranslationError(
-                "Polars backend is unable to compile selection with "
-                f"operation type of {type(arg)}"
-            )
+            select_key = 'SELECT'
+
+        return '{0}{1}'.format(select_key, buf.getvalue())
 
-    if scalars:
-        # Scalars need to first be projected to columns with `with_columns`,
-        # otherwise if a downstream select only selects out the scalar-backed
-        # columns they'll return with only a single row.
-        lf = lf.with_columns(scalars)
+    def format_table_set(self):
+        if self.table_set is None:
+            return None
+
+        fragment = 'FROM '
+
+        helper = _TableSetFormatter(self, self.table_set)
+        fragment += helper.get_result()
+
+        return fragment
+
+    def format_group_by(self):
+        if not len(self.group_by):
+            # There is no aggregation, nothing to see here
+            return None
+
+        lines = []
+        if len(self.group_by) > 0:
+            clause = 'GROUP BY {0}'.format(', '.join([
+                str(x + 1) for x in self.group_by]))
+            lines.append(clause)
+
+        if len(self.having) > 0:
+            trans_exprs = []
+            for expr in self.having:
+                translated = self._translate(expr)
+                trans_exprs.append(translated)
+            lines.append('HAVING {0}'.format(' AND '.join(trans_exprs)))
+
+        return '\n'.join(lines)
+
+    def format_where(self):
+        if len(self.where) == 0:
+            return None
+
+        buf = StringIO()
+        buf.write('WHERE ')
+        fmt_preds = [self._translate(pred, permit_subquery=True)
+                     for pred in self.where]
+        conj = ' AND\n{0}'.format(' ' * 6)
+        buf.write(conj.join(fmt_preds))
+        return buf.getvalue()
+
+    def format_postamble(self):
+        buf = StringIO()
+        lines = 0
+
+        if len(self.order_by) > 0:
+            buf.write('ORDER BY ')
+            formatted = []
+            for expr in self.order_by:
+                key = expr.op()
+                translated = self._translate(key.expr)
+                if not key.ascending:
+                    translated += ' DESC'
+                formatted.append(translated)
+            buf.write(', '.join(formatted))
+            lines += 1
+
+        if self.limit is not None:
+            if lines:
+                buf.write('\n')
+            n, offset = self.limit['n'], self.limit['offset']
+            buf.write('LIMIT {0}'.format(n))
+            if offset is not None and offset != 0:
+                buf.write(' OFFSET {0}'.format(offset))
+            lines += 1
+
+        if not lines:
+            return None
+
+        return buf.getvalue()
+
+    @property
+    def translator(self):
+        return ImpalaExprTranslator
+
+
+def _join_not_none(sep, pieces):
+    pieces = [x for x in pieces if x is not None]
+    return sep.join(pieces)
+
+
+class _TableSetFormatter(comp.TableSetFormatter):
+
+    def get_result(self):
+        # Got to unravel the join stack; the nesting order could be
+        # arbitrary, so we do a depth first search and push the join tokens
+        # and predicates onto a flat list, then format them
+        op = self.expr.op()
 
-    if selections:
-        lf = lf.select(selections)
+        if isinstance(op, ops.Join):
+            self._walk_join_tree(op)
+        else:
+            self.join_tables.append(self._format_table(self.expr))
 
-        if unnests:
-            lf = lf.explode(*unnests)
+        # TODO: Now actually format the things
+        buf = StringIO()
+        buf.write(self.join_tables[0])
+        for jtype, table, preds in zip(self.join_types, self.join_tables[1:],
+                                       self.join_predicates):
+            buf.write('\n')
+            buf.write(util.indent('{0} {1}'.format(jtype, table), self.indent))
+
+            if len(preds):
+                buf.write('\n')
+                fmt_preds = [self._translate(pred) for pred in preds]
+                conj = ' AND\n{0}'.format(' ' * 3)
+                fmt_preds = util.indent('ON ' + conj.join(fmt_preds),
+                                        self.indent * 2)
+                buf.write(fmt_preds)
+
+        return buf.getvalue()
+
+    _join_names = {
+        ops.InnerJoin: 'INNER JOIN',
+        ops.LeftJoin: 'LEFT OUTER JOIN',
+        ops.RightJoin: 'RIGHT OUTER JOIN',
+        ops.OuterJoin: 'FULL OUTER JOIN',
+        ops.LeftAntiJoin: 'LEFT ANTI JOIN',
+        ops.LeftSemiJoin: 'LEFT SEMI JOIN',
+        ops.CrossJoin: 'CROSS JOIN'
+    }
+
+    def _get_join_type(self, op):
+        jname = self._join_names[type(op)]
+
+        # Impala requires this
+        if len(op.predicates) == 0:
+            jname = self._join_names[ops.CrossJoin]
+
+        return jname
+
+    def _format_table(self, expr):
+        # TODO: This could probably go in a class and be significantly nicer
+        ctx = self.context
+
+        ref_expr = expr
+        op = ref_op = expr.op()
+        if isinstance(op, ops.SelfReference):
+            ref_expr = op.table
+            ref_op = ref_expr.op()
+
+        if isinstance(ref_op, ops.PhysicalTable):
+            name = ref_op.name
+            if name is None:
+                raise com.RelationError('Table did not have a name: {0!r}'
+                                        .format(expr))
+            result = quote_identifier(name)
+            is_subquery = False
+        else:
+            # A subquery
+            if ctx.is_extracted(ref_expr):
+                # Was put elsewhere, e.g. WITH block, we just need to grab its
+                # alias
+                alias = ctx.get_ref(expr)
 
-    return lf
+                # HACK: self-references have to be treated more carefully here
+                if isinstance(op, ops.SelfReference):
+                    return '{0} {1}'.format(ctx.get_ref(ref_expr), alias)
+                else:
+                    return alias
 
+            subquery = ctx.get_compiled_expr(expr)
+            result = '(\n{0}\n)'.format(util.indent(subquery, self.indent))
+            is_subquery = True
 
-@translate.register(ops.Sort)
-def sort(op, **kw):
-    lf = translate(op.parent, **kw)
-    if not op.keys:
-        return lf
+        if is_subquery or ctx.need_aliases():
+            result += ' {0}'.format(ctx.get_ref(expr))
 
-    newcols = {gen_name("sort_key"): translate(col.expr, **kw) for col in op.keys}
-    lf = lf.with_columns(**newcols)
+        return result
 
-    by = list(newcols.keys())
-    descending = [key.descending for key in op.keys]
-    try:
-        lf = lf.sort(by, descending=descending, nulls_last=True)
-    except TypeError:  # pragma: no cover
-        lf = lf.sort(by, reverse=descending, nulls_last=True)  # pragma: no cover
 
-    return lf.drop(*by)
+class ImpalaUnion(comp.Union):
 
+    def compile(self):
+        context = self.context
 
-@translate.register(ops.Filter)
-def filter_(op, **kw):
-    lf = translate(op.parent, **kw)
+        if self.distinct:
+            union_keyword = 'UNION'
+        else:
+            union_keyword = 'UNION ALL'
 
-    if op.predicates:
-        predicates = map(partial(translate, **kw), op.predicates)
-        predicate = reduce(operator.and_, predicates)
-        lf = lf.filter(predicate)
+        left_set = context.get_compiled_expr(self.left)
+        right_set = context.get_compiled_expr(self.right)
 
-    return lf
+        query = '{0}\n{1}\n{2}'.format(left_set, union_keyword, right_set)
+        return query
 
 
-@translate.register(ops.Limit)
-def limit(op, **kw):
-    if (n := op.n) is not None and not isinstance(n, int):
-        raise NotImplementedError("Dynamic limit not supported")
+# ---------------------------------------------------------------------
+# Scalar and array expression formatting
+
+_sql_type_names = {
+    'int8': 'tinyint',
+    'int16': 'smallint',
+    'int32': 'int',
+    'int64': 'bigint',
+    'float': 'float',
+    'double': 'double',
+    'string': 'string',
+    'boolean': 'boolean',
+    'timestamp': 'timestamp',
+    'decimal': 'decimal',
+}
 
-    if not isinstance(offset := op.offset, int):
-        raise NotImplementedError("Dynamic offset not supported")
 
-    lf = translate(op.parent, **kw)
-    return lf.slice(offset, n)
+def _cast(translator, expr):
+    op = expr.op()
+    arg, target_type = op.args
+    arg_formatted = translator.translate(arg)
 
+    if isinstance(arg, ir.CategoryValue) and target_type == 'int32':
+        return arg_formatted
+    else:
+        sql_type = _type_to_sql_string(target_type)
+        return 'CAST({0!s} AS {1!s})'.format(arg_formatted, sql_type)
 
-@translate.register(ops.Aggregate)
-def aggregation(op, **kw):
-    lf = translate(op.parent, **kw)
 
-    if op.groups:
-        # project first to handle computed group by columns
-        lf = (
-            lf.with_columns(
-                [translate(arg, **kw).alias(name) for name, arg in op.groups.items()]
-            )
-            .group_by(list(op.groups.keys()))
-            .agg
-        )
+def _type_to_sql_string(tval):
+    if isinstance(tval, dt.Decimal):
+        return 'decimal({0},{1})'.format(tval.precision, tval.scale)
     else:
-        lf = lf.select
+        return _sql_type_names[tval.name()]
 
-    if op.metrics:
-        metrics = [translate(arg, **kw).alias(name) for name, arg in op.metrics.items()]
-        lf = lf(metrics)
 
-    return lf
+def _between(translator, expr):
+    op = expr.op()
+    comp, lower, upper = [translator.translate(x) for x in op.args]
+    return '{0!s} BETWEEN {1!s} AND {2!s}'.format(comp, lower, upper)
 
 
-@translate.register(PandasRename)
-def rename(op, **kw):
-    parent = translate(op.parent, **kw)
-    return parent.rename(op.mapping)
+def _is_null(translator, expr):
+    formatted_arg = translator.translate(expr.op().args[0])
+    return '{0!s} IS NULL'.format(formatted_arg)
 
 
-@translate.register(PandasJoin)
-def join(op, **kw):
-    how = op.how
-    left = translate(op.left, **kw)
-    right = translate(op.right, **kw)
+def _not_null(translator, expr):
+    formatted_arg = translator.translate(expr.op().args[0])
+    return '{0!s} IS NOT NULL'.format(formatted_arg)
 
-    # workaround required for https://github.com/pola-rs/polars/issues/13130
-    prefix = gen_name("on")
-    left_on = {f"{prefix}_{i}": translate(v, **kw) for i, v in enumerate(op.left_on)}
-    right_on = {f"{prefix}_{i}": translate(v, **kw) for i, v in enumerate(op.right_on)}
-    left = left.with_columns(**left_on)
-    right = right.with_columns(**right_on)
-    on = list(left_on.keys())
 
-    if how == "right":
-        how = "left"
-        left, right = right, left
+_cumulative_to_reduction = {
+    ops.CumulativeSum: ops.Sum,
+    ops.CumulativeMin: ops.Min,
+    ops.CumulativeMax: ops.Max,
+    ops.CumulativeMean: ops.Mean,
+    ops.CumulativeAny: ops.Any,
+    ops.CumulativeAll: ops.All,
+}
 
-    joined = left.join(right, on=on, how=how)
 
-    try:
-        joined = joined.drop(*on)
-    except TypeError:
-        joined = joined.drop(columns=on)
+def _cumulative_to_window(translator, expr, window):
+    win = ibis.cumulative_window()
+    win = (win.group_by(window._group_by)
+           .order_by(window._order_by))
 
-    return joined
+    op = expr.op()
 
+    klass = _cumulative_to_reduction[type(op)]
+    new_op = klass(*op.args)
+    new_expr = expr._factory(new_op, name=expr._name)
 
-@translate.register(PandasAsofJoin)
-def asof_join(op, **kw):
-    left = translate(op.left, **kw)
-    right = translate(op.right, **kw)
+    if type(new_op) in translator._rewrites:
+        new_expr = translator._rewrites[type(new_op)](new_expr)
 
-    # workaround required for https://github.com/pola-rs/polars/issues/13130
-    on, by = gen_name("on"), gen_name("by")
-    left_on = {f"{on}_{i}": translate(v, **kw) for i, v in enumerate(op.left_on)}
-    right_on = {f"{on}_{i}": translate(v, **kw) for i, v in enumerate(op.right_on)}
-    left_by = {f"{by}_{i}": translate(v, **kw) for i, v in enumerate(op.left_by)}
-    right_by = {f"{by}_{i}": translate(v, **kw) for i, v in enumerate(op.right_by)}
+    new_expr = L.windowize_function(new_expr, win)
+    return new_expr
 
-    left = left.with_columns(**left_on, **left_by)
-    right = right.with_columns(**right_on, **right_by)
 
-    on = list(left_on.keys())
-    by = list(left_by.keys())
+def _window(translator, expr):
+    op = expr.op()
 
-    if op.operator in {ops.Less, ops.LessEqual}:
-        direction = "forward"
-    elif op.operator in {ops.Greater, ops.GreaterEqual}:
-        direction = "backward"
-    elif op.operator == ops.Equals:
-        direction = "nearest"
-    else:
-        raise NotImplementedError(f"Operator {operator} not supported for asof join")
+    arg, window = op.args
+    window_op = arg.op()
+
+    _require_order_by = (ops.Lag,
+                         ops.Lead,
+                         ops.DenseRank,
+                         ops.MinRank,
+                         ops.FirstValue,
+                         ops.LastValue)
+
+    _unsupported_reductions = (
+        ops.CMSMedian,
+        ops.GroupConcat,
+        ops.HLLCardinality,
+    )
 
-    assert len(on) == 1
-    joined = left.join_asof(right, on=on[0], by=by, strategy=direction)
-    try:
-        joined = joined.drop(*(on + by))
-    except TypeError:
-        joined = joined.drop(columns=on + by)
-    return joined
-
-
-@translate.register(ops.DropNa)
-def dropna(op, **kw):
-    lf = translate(op.parent, **kw)
-
-    if op.subset is None:
-        subset = None
-    elif not len(op.subset):
-        return lf.clear() if op.how == "all" else lf
+    if isinstance(window_op, _unsupported_reductions):
+        raise com.TranslationError('{0!s} is not supported in '
+                                   'window functions'
+                                   .format(type(window_op)))
+
+    if isinstance(window_op, ops.CumulativeOp):
+        arg = _cumulative_to_window(translator, arg, window)
+        return translator.translate(arg)
+
+    # Some analytic functions need to have the expression of interest in
+    # the ORDER BY part of the window clause
+    if (isinstance(window_op, _require_order_by) and
+            len(window._order_by) == 0):
+        window = window.order_by(window_op.args[0])
+
+    window_formatted = _format_window(translator, window)
+
+    arg_formatted = translator.translate(arg)
+    result = '{0} {1}'.format(arg_formatted, window_formatted)
+
+    if type(window_op) in _expr_transforms:
+        return _expr_transforms[type(window_op)](result)
     else:
-        subset = [arg.name for arg in op.subset]
+        return result
+
 
-    if op.how == "all":
-        cols = pl.col(subset) if subset else pl.all()
-        return lf.filter(~pl.all_horizontal(cols.is_null()))
+def _format_window(translator, window):
+    components = []
 
-    return lf.drop_nulls(subset)
+    if len(window._group_by) > 0:
+        partition_args = [translator.translate(x)
+                          for x in window._group_by]
+        components.append('PARTITION BY {0}'.format(', '.join(partition_args)))
+
+    if len(window._order_by) > 0:
+        order_args = []
+        for expr in window._order_by:
+            key = expr.op()
+            translated = translator.translate(key.expr)
+            if not key.ascending:
+                translated += ' DESC'
+            order_args.append(translated)
+
+        components.append('ORDER BY {0}'.format(', '.join(order_args)))
+
+    p, f = window.preceding, window.following
+
+    def _prec(p):
+        return '{0} PRECEDING'.format(p) if p > 0 else 'CURRENT ROW'
+
+    def _foll(f):
+        return '{0} FOLLOWING'.format(f) if f > 0 else 'CURRENT ROW'
+
+    if p is not None and f is not None:
+        frame = ('ROWS BETWEEN {0} AND {1}'
+                 .format(_prec(p), _foll(f)))
+    elif p is not None:
+        if isinstance(p, tuple):
+            start, end = p
+            frame = ('ROWS BETWEEN {0} AND {1}'
+                     .format(_prec(start), _prec(end)))
+        else:
+            kind = 'ROWS' if p > 0 else 'RANGE'
+            frame = ('{0} BETWEEN {1} AND UNBOUNDED FOLLOWING'
+                     .format(kind, _prec(p)))
+    elif f is not None:
+        if isinstance(f, tuple):
+            start, end = f
+            frame = ('ROWS BETWEEN {0} AND {1}'
+                     .format(_foll(start), _foll(end)))
+        else:
+            kind = 'ROWS' if f > 0 else 'RANGE'
+            frame = ('{0} BETWEEN UNBOUNDED PRECEDING AND {1}'
+                     .format(kind, _foll(f)))
+    else:
+        # no-op, default is full sample
+        frame = None
 
+    if frame is not None:
+        components.append(frame)
 
-@translate.register(ops.FillNa)
-def fillna(op, **kw):
-    table = translate(op.parent, **kw)
+    return 'OVER ({0})'.format(' '.join(components))
 
-    columns = []
 
-    repls = op.replacements
+def _shift_like(name):
 
-    if isinstance(repls, Mapping):
+    def formatter(translator, expr):
+        op = expr.op()
+        arg, offset, default = op.args
 
-        def get_replacement(name):
-            repl = repls.get(name)
-            if repl is not None:
-                return repl.value
+        arg_formatted = translator.translate(arg)
+
+        if default is not None:
+            if offset is None:
+                offset_formatted = '1'
             else:
-                return None
+                offset_formatted = translator.translate(offset)
+
+            default_formatted = translator.translate(default)
+
+            return '{0}({1}, {2}, {3})'.format(name, arg_formatted,
+                                               offset_formatted,
+                                               default_formatted)
+        elif offset is not None:
+            offset_formatted = translator.translate(offset)
+            return '{0}({1}, {2})'.format(name, arg_formatted,
+                                          offset_formatted)
+        else:
+            return '{0}({1})'.format(name, arg_formatted)
+
+    return formatter
+
+
+def _nth_value(translator, expr):
+    op = expr.op()
+    arg, rank = op.args
 
+    arg_formatted = translator.translate(arg)
+    rank_formatted = translator.translate(rank - 1)
+
+    return 'first_value(lag({0}, {1}))'.format(arg_formatted,
+                                               rank_formatted)
+
+
+def _negate(translator, expr):
+    arg = expr.op().args[0]
+    formatted_arg = translator.translate(arg)
+    if isinstance(expr, ir.BooleanValue):
+        return 'NOT {0!s}'.format(formatted_arg)
     else:
-        value = repls.value
+        if _needs_parens(arg):
+            formatted_arg = _parenthesize(formatted_arg)
+        return '-{0!s}'.format(formatted_arg)
 
-        def get_replacement(_):
-            return value
 
-    for name, dtype in op.parent.schema.items():
-        column = pl.col(name)
-        if isinstance(op.replacements, Mapping):
-            value = op.replacements.get(name)
-        else:
-            value = _literal_value(op.replacements)
+def _parenthesize(what):
+    return '({0!s})'.format(what)
 
-        if value is not None:
-            if dtype.is_floating():
-                column = column.fill_nan(value)
-            column = column.fill_null(value)
 
-        # requires special treatment if the fill value has different datatype
-        if dtype.is_timestamp():
-            column = column.cast(pl.Datetime)
+def unary(func_name):
+    return fixed_arity(func_name, 1)
 
-        columns.append(column)
 
-    return table.select(columns)
+def _reduction_format(translator, func_name, arg, where):
+    if where is not None:
+        case = where.ifelse(arg, ibis.NA)
+        arg = translator.translate(case)
+    else:
+        arg = translator.translate(arg)
 
+    return '{0!s}({1!s})'.format(func_name, arg)
 
-@translate.register(ops.IdenticalTo)
-def identical_to(op, **kw):
-    left = translate(op.left, **kw)
-    right = translate(op.right, **kw)
-    return left.eq_missing(right)
 
+def _reduction(func_name):
+    def formatter(translator, expr):
+        op = expr.op()
 
-@translate.register(ops.NullIf)
-def nullif(op, **kw):
-    arg = translate(op.arg, **kw)
-    null_if_expr = translate(op.null_if_expr, **kw)
-    return pl.when(arg == null_if_expr).then(None).otherwise(arg)
+        # HACK: support trailing arguments
+        arg, where = op.args[:2]
 
+        return _reduction_format(translator, func_name, arg, where)
+    return formatter
 
-@translate.register(ops.IfElse)
-def ifelse(op, **kw):
-    bool_expr = translate(op.bool_expr, **kw)
-    true_expr = translate(op.true_expr, **kw)
-    false_null_expr = translate(op.false_null_expr, **kw)
-    return pl.when(bool_expr).then(true_expr).otherwise(false_null_expr)
 
+def _variance_like(func_name):
+    func_names = {
+        'sample': func_name,
+        'pop': '{0}_pop'.format(func_name)
+    }
 
-@translate.register(ops.SimpleCase)
-def simple_case(op, **kw):
-    base = translate(op.base, **kw)
-    default = translate(op.default, **kw)
-    for case, result in reversed(list(zip(op.cases, op.results))):
-        case = base == translate(case, **kw)
-        result = translate(result, **kw)
-        default = pl.when(case).then(result).otherwise(default)
-    return default
+    def formatter(translator, expr):
+        arg, where, how = expr.op().args
+        return _reduction_format(translator, func_names[how], arg, where)
+    return formatter
 
 
-@translate.register(ops.SearchedCase)
-def searched_case(op, **kw):
-    default = translate(op.default, **kw)
-    for case, result in reversed(list(zip(op.cases, op.results))):
-        case = translate(case, **kw)
-        result = translate(result, **kw)
-        default = pl.when(case).then(result).otherwise(default)
-    return default
+def fixed_arity(func_name, arity):
 
+    def formatter(translator, expr):
+        op = expr.op()
+        if arity != len(op.args):
+            raise com.IbisError('incorrect number of args')
+        return _format_call(translator, func_name, *op.args)
 
-@translate.register(ops.Coalesce)
-def coalesce(op, **kw):
-    arg = [translate(expr, **kw) for expr in op.arg]
-    return pl.coalesce(arg)
+    return formatter
 
 
-@translate.register(ops.Least)
-def least(op, **kw):
-    arg = [translate(arg, **kw) for arg in op.arg]
-    return pl.min_horizontal(arg)
+def _ifnull_workaround(translator, expr):
+    op = expr.op()
+    a, b = op.args
 
+    # work around per #345, #360
+    if (isinstance(a, ir.DecimalValue) and
+            isinstance(b, ir.IntegerValue)):
+        b = b.cast(a.type())
 
-@translate.register(ops.Greatest)
-def greatest(op, **kw):
-    arg = [translate(arg, **kw) for arg in op.arg]
-    return pl.max_horizontal(arg)
+    return _format_call(translator, 'isnull', a, b)
 
 
-@translate.register(ops.InSubquery)
-def in_column(op, **kw):
-    value = translate(op.value, **kw)
-    needle = translate(op.needle, **kw)
-    return needle.is_in(value)
+def _format_call(translator, func, *args):
+    formatted_args = []
+    for arg in args:
+        fmt_arg = translator.translate(arg)
+        formatted_args.append(fmt_arg)
 
+    return '{0!s}({1!s})'.format(func, ', '.join(formatted_args))
 
-@translate.register(ops.InValues)
-def in_values(op, **kw):
-    value = translate(op.value, **kw)
-    options = list(map(translate, op.options))
-    if not options:
-        return pl.lit(False)
-    return pl.any_horizontal([value == option for option in options])
 
+def _binary_infix_op(infix_sym):
+    def formatter(translator, expr):
+        op = expr.op()
 
-_string_unary = {
-    ops.Strip: "strip_chars",
-    ops.LStrip: "strip_chars_start",
-    ops.RStrip: "strip_chars_end",
-    ops.Lowercase: "to_lowercase",
-    ops.Uppercase: "to_uppercase",
-}
+        left, right = op.args
 
+        left_arg = translator.translate(left)
+        right_arg = translator.translate(right)
 
-@translate.register(ops.StringLength)
-def string_length(op, **kw):
-    arg = translate(op.arg, **kw)
-    typ = PolarsType.from_ibis(op.dtype)
-    return arg.str.len_bytes().cast(typ)
+        if _needs_parens(left):
+            left_arg = _parenthesize(left_arg)
 
+        if _needs_parens(right):
+            right_arg = _parenthesize(right_arg)
 
-@translate.register(ops.Capitalize)
-def capitalize(op, **kw):
-    arg = translate(op.arg, **kw)
-    typ = PolarsType.from_ibis(op.dtype)
-    first = arg.str.slice(0, 1).str.to_uppercase()
-    rest = arg.str.slice(1, None).str.to_lowercase()
-    return (first + rest).cast(typ)
+        return '{0!s} {1!s} {2!s}'.format(left_arg, infix_sym, right_arg)
+    return formatter
 
 
-@translate.register(ops.StringUnary)
-def string_unary(op, **kw):
-    arg = translate(op.arg, **kw)
-    func = _string_unary.get(type(op))
-    if func is None:
-        raise com.OperationNotDefinedError(f"{type(op).__name__} not supported")
+def _xor(translator, expr):
+    op = expr.op()
 
-    method = getattr(arg.str, func)
-    return method()
+    left_arg = translator.translate(op.left)
+    right_arg = translator.translate(op.right)
 
+    if _needs_parens(op.left):
+        left_arg = _parenthesize(left_arg)
 
-@translate.register(ops.Reverse)
-def reverse(op, **kw):
-    arg = translate(op.arg, **kw)
-    return arg.map_elements(lambda x: x[::-1])
+    if _needs_parens(op.right):
+        right_arg = _parenthesize(right_arg)
 
+    return ('{0} AND NOT {1}'
+            .format('({0} {1} {2})'.format(left_arg, 'OR', right_arg),
+                    '({0} {1} {2})'.format(left_arg, 'AND', right_arg)))
 
-@translate.register(ops.StringSplit)
-def string_split(op, **kw):
-    arg = translate(op.arg, **kw)
-    delim = _literal_value(op.delimiter)
-    return arg.str.split(by=delim)
 
+def _name_expr(formatted_expr, quoted_name):
+    return '{0!s} AS {1!s}'.format(formatted_expr, quoted_name)
 
-@translate.register(ops.StringReplace)
-def string_replace(op, **kw):
-    arg = translate(op.arg, **kw)
-    pat = translate(op.pattern, **kw)
-    rep = translate(op.replacement, **kw)
-    return arg.str.replace(pat, rep, literal=True)
 
+def _needs_parens(op):
+    if isinstance(op, ir.Expr):
+        op = op.op()
+    op_klass = type(op)
+    # function calls don't need parens
+    return (op_klass in _binary_infix_ops or
+            op_klass in [ops.Negate])
 
-@translate.register(ops.StartsWith)
-def string_startswith(op, **kw):
-    arg = translate(op.arg, **kw)
-    start = _literal_value(op.start)
-    return arg.str.starts_with(start)
 
+def _need_parenthesize_args(op):
+    if isinstance(op, ir.Expr):
+        op = op.op()
+    op_klass = type(op)
+    return (op_klass in _binary_infix_ops or
+            op_klass in [ops.Negate])
 
-@translate.register(ops.EndsWith)
-def string_endswith(op, **kw):
-    arg = translate(op.arg, **kw)
-    end = _literal_value(op.end)
-    return arg.str.ends_with(end)
 
+def _boolean_literal_format(expr):
+    value = expr.op().value
+    return 'TRUE' if value else 'FALSE'
 
-@translate.register(ops.StringConcat)
-def string_concat(op, **kw):
-    args = [translate(arg, **kw) for arg in op.arg]
-    return pl.concat_str(args)
 
+def _number_literal_format(expr):
+    value = expr.op().value
+    return repr(value)
 
-@translate.register(ops.StringJoin)
-def string_join(op, **kw):
-    args = [translate(arg, **kw) for arg in op.arg]
-    sep = _literal_value(op.sep)
-    return pl.concat_str(args, separator=sep)
 
+def _string_literal_format(expr):
+    value = expr.op().value
+    return "'{0!s}'".format(value.replace("'", "\\'"))
 
-@translate.register(ops.Substring)
-def string_substring(op, **kw):
-    arg = translate(op.arg, **kw)
-    return arg.str.slice(
-        offset=_literal_value(op.start),
-        length=_literal_value(op.length),
-    )
 
+def _timestamp_literal_format(expr):
+    value = expr.op().value
+    if isinstance(value, datetime.datetime):
+        if value.microsecond != 0:
+            raise ValueError(value)
+        value = value.strftime('%Y-%m-%d %H:%M:%S')
 
-@translate.register(ops.StringContains)
-def string_contains(op, **kw):
-    haystack = translate(op.haystack, **kw)
-    return haystack.str.contains(
-        pattern=_literal_value(op.needle),
-        literal=True,
-    )
+    return "'{0!s}'".format(value)
 
 
-@translate.register(ops.RegexSearch)
-def regex_search(op, **kw):
-    arg = translate(op.arg, **kw)
-    return arg.str.contains(
-        pattern=_literal_value(op.pattern),
-        literal=False,
-    )
+def quote_identifier(name, quotechar='`', force=False):
+    if force or name.count(' ') or name in identifiers.impala_identifiers:
+        return '{0}{1}{0}'.format(quotechar, name)
+    else:
+        return name
 
 
-@translate.register(ops.RegexExtract)
-def regex_extract(op, **kw):
-    arg = translate(op.arg, **kw)
-    return arg.str.extract(
-        pattern=_literal_value(op.pattern),
-        group_index=_literal_value(op.index),
-    )
+class CaseFormatter(object):
 
+    def __init__(self, translator, base, cases, results, default):
+        self.translator = translator
+        self.base = base
+        self.cases = cases
+        self.results = results
+        self.default = default
+
+        # HACK
+        self.indent = 2
+        self.multiline = len(cases) > 1
+        self.buf = StringIO()
+
+    def _trans(self, expr):
+        return self.translator.translate(expr)
+
+    def get_result(self):
+        self.buf.seek(0)
+
+        self.buf.write('CASE')
+        if self.base is not None:
+            base_str = self._trans(self.base)
+            self.buf.write(' {0}'.format(base_str))
+
+        for case, result in zip(self.cases, self.results):
+            self._next_case()
+            case_str = self._trans(case)
+            result_str = self._trans(result)
+            self.buf.write('WHEN {0} THEN {1}'.format(case_str, result_str))
+
+        if self.default is not None:
+            self._next_case()
+            default_str = self._trans(self.default)
+            self.buf.write('ELSE {0}'.format(default_str))
 
-@translate.register(ops.RegexReplace)
-def regex_replace(op, **kw):
-    arg = translate(op.arg, **kw)
-    pattern = translate(op.pattern, **kw)
-    replacement = translate(op.replacement, **kw)
-    return arg.str.replace_all(
-        pattern=pattern,
-        value=replacement,
-    )
+        if self.multiline:
+            self.buf.write('\nEND')
+        else:
+            self.buf.write(' END')
 
+        return self.buf.getvalue()
 
-@translate.register(ops.LPad)
-def lpad(op, **kw):
-    arg = translate(op.arg, **kw)
-    _lpad = _expr_method(arg.str, "lpad", ["pad_start", "rjust"])
-    return _lpad(_literal_value(op.length), _literal_value(op.pad))
-
-
-@translate.register(ops.RPad)
-def rpad(op, **kw):
-    arg = translate(op.arg, **kw)
-    _rpad = _expr_method(arg.str, "rpad", ["pad_end", "ljust"])
-    return _rpad(_literal_value(op.length), _literal_value(op.pad))
-
-
-@translate.register(ops.StrRight)
-def str_right(op, **kw):
-    arg = translate(op.arg, **kw)
-    nchars = _literal_value(op.nchars)
-    return arg.str.slice(-nchars, None)
-
-
-@translate.register(ops.Round)
-def round(op, **kw):
-    arg = translate(op.arg, **kw)
-    typ = PolarsType.from_ibis(op.dtype)
-    digits = _literal_value(op.digits)
-    return arg.round(digits or 0).cast(typ)
-
-
-@translate.register(ops.Radians)
-def radians(op, **kw):
-    arg = translate(op.arg, **kw)
-    return arg * math.pi / 180
-
-
-@translate.register(ops.Degrees)
-def degrees(op, **kw):
-    arg = translate(op.arg, **kw)
-    return arg * 180 / math.pi
-
-
-@translate.register(ops.Clip)
-def clip(op, **kw):
-    arg = translate(op.arg, **kw)
-
-    lower = _literal_value(op.lower)
-    upper = _literal_value(op.upper)
-
-    if vparse(pl.__version__) >= vparse("0.19.12"):
-        if not (lower is None and upper is None):
-            return arg.clip(lower, upper)
-    elif lower is not None and upper is not None:
-        return arg.clip(lower, upper)
-    elif lower is not None:
-        return arg.clip_min(lower)
-    elif upper is not None:
-        return arg.clip_max(upper)
-
-    raise com.TranslationError("No lower or upper bound specified")
-
-
-@translate.register(ops.Log)
-def log(op, **kw):
-    arg = translate(op.arg, **kw)
-    return arg.log(base=_literal_value(op.base))
-
-
-@translate.register(ops.Repeat)
-def repeat(op, **kw):
-    arg = translate(op.arg, **kw)
-    n_times = _literal_value(op.times)
-    return pl.concat_str([arg] * n_times, separator="")
-
-
-@translate.register(ops.Sign)
-def sign(op, **kw):
-    arg = translate(op.arg, **kw)
-    typ = PolarsType.from_ibis(op.dtype)
-    return arg.sign().cast(typ)
-
-
-@translate.register(ops.Power)
-def power(op, **kw):
-    left = translate(op.left, **kw)
-    right = translate(op.right, **kw)
-    return left.pow(right)
-
-
-@translate.register(ops.StructField)
-def struct_field(op, **kw):
-    arg = translate(op.arg, **kw)
-    return arg.struct.field(op.name)
-
-
-@translate.register(ops.StructColumn)
-def struct_column(op, **kw):
-    fields = [translate(v, **kw).alias(k) for k, v in zip(op.names, op.values)]
-    return pl.struct(fields)
-
-
-_reductions = {
-    ops.All: "all",
-    ops.Any: "any",
-    ops.ApproxMedian: "median",
-    ops.Arbitrary: "first",
-    ops.Count: "count",
-    ops.CountDistinct: "n_unique",
-    ops.First: "first",
-    ops.Last: "last",
-    ops.Max: "max",
-    ops.Mean: "mean",
-    ops.Median: "median",
-    ops.Min: "min",
-    ops.Mode: "mode",
-    ops.StandardDev: "std",
-    ops.Sum: "sum",
-    ops.Variance: "var",
+    def _next_case(self):
+        if self.multiline:
+            self.buf.write('\n{0}'.format(' ' * self.indent))
+        else:
+            self.buf.write(' ')
+
+
+def _simple_case(translator, expr):
+    op = expr.op()
+    formatter = CaseFormatter(translator, op.base, op.cases, op.results,
+                              op.default)
+    return formatter.get_result()
+
+
+def _searched_case(translator, expr):
+    op = expr.op()
+    formatter = CaseFormatter(translator, None, op.cases, op.results,
+                              op.default)
+    return formatter.get_result()
+
+
+def _table_array_view(translator, expr):
+    ctx = translator.context
+    table = expr.op().table
+    query = ctx.get_compiled_expr(table)
+    return '(\n{0}\n)'.format(util.indent(query, ctx.indent))
+
+
+# ---------------------------------------------------------------------
+# Timestamp arithmetic and other functions
+
+def _timestamp_delta(translator, expr):
+    op = expr.op()
+    arg, offset = op.args
+    formatted_arg = translator.translate(arg)
+    return _timestamp_format_offset(offset, formatted_arg)
+
+
+_impala_delta_functions = {
+    tempo.Year: 'years_add',
+    tempo.Month: 'months_add',
+    tempo.Week: 'weeks_add',
+    tempo.Day: 'days_add',
+    tempo.Hour: 'hours_add',
+    tempo.Minute: 'minutes_add',
+    tempo.Second: 'seconds_add',
+    tempo.Millisecond: 'milliseconds_add',
+    tempo.Microsecond: 'microseconds_add',
+    tempo.Nanosecond: 'nanoseconds_add'
 }
 
-for reduction in _reductions.keys():
 
-    @translate.register(reduction)
-    def reduction(op, **kw):
-        args = [
-            translate(arg, **kw)
-            for name, arg in zip(op.argnames, op.args)
-            if name not in ("where", "how")
-        ]
-
-        agg = _reductions[type(op)]
-
-        predicates = [arg.is_not_null() for arg in args]
-        if (where := op.where) is not None:
-            predicates.append(translate(where, **kw))
-
-        first, *rest = args
-        method = operator.methodcaller(agg, *rest)
-        return method(first.filter(reduce(operator.and_, predicates))).cast(
-            PolarsType.from_ibis(op.dtype)
-        )
-
-
-@translate.register(ops.Quantile)
-def execute_quantile(op, **kw):
-    arg = translate(op.arg, **kw)
-    quantile = translate(op.quantile, **kw)
-    filt = arg.is_not_null() & quantile.is_not_null()
-    if (where := op.where) is not None:
-        filt &= translate(where, **kw)
-
-    # we can't throw quantile into the _reductions mapping because Polars'
-    # default interpolation of "nearest" doesn't match the rest of our backends
-    return arg.filter(filt).quantile(quantile, interpolation="linear")
-
-
-@translate.register(ops.Correlation)
-def correlation(op, **kw):
-    x = op.left
-    if (x_type := x.dtype).is_boolean():
-        x = ops.Cast(x, dt.Int32(nullable=x_type.nullable))
-
-    y = op.right
-    if (y_type := y.dtype).is_boolean():
-        y = ops.Cast(y, dt.Int32(nullable=y_type.nullable))
-
-    if (where := op.where) is not None:
-        x = ops.IfElse(where, x, None)
-        y = ops.IfElse(where, y, None)
-
-    return pl.corr(translate(x, **kw), translate(y, **kw))
-
-
-@translate.register(ops.Distinct)
-def distinct(op, **kw):
-    table = translate(op.parent, **kw)
-    return table.unique()
-
-
-@translate.register(ops.CountStar)
-def count_star(op, **kw):
-    if (where := op.where) is not None:
-        condition = translate(where, **kw)
-        result = condition.sum()
+def _timestamp_format_offset(offset, arg):
+    f = _impala_delta_functions[type(offset)]
+    return '{0}({1}, {2})'.format(f, arg, offset.n)
+
+
+# ---------------------------------------------------------------------
+# Semi/anti-join supports
+
+
+def _exists_subquery(translator, expr):
+    op = expr.op()
+    ctx = translator.context
+
+    expr = (op.foreign_table
+            .filter(op.predicates)
+            .projection([ir.literal(1).name(ir.unnamed)]))
+
+    subquery = ctx.get_compiled_expr(expr)
+
+    if isinstance(op, transforms.ExistsSubquery):
+        key = 'EXISTS'
+    elif isinstance(op, transforms.NotExistsSubquery):
+        key = 'NOT EXISTS'
     else:
-        try:
-            result = pl.len()
-        except AttributeError:
-            result = pl.count()
-    return result.cast(PolarsType.from_ibis(op.dtype))
+        raise NotImplementedError
 
+    return '{0} (\n{1}\n)'.format(key, util.indent(subquery, ctx.indent))
 
-@translate.register(ops.TimestampNow)
-def timestamp_now(op, **_):
-    return pl.lit(pd.Timestamp("now", tz="UTC").tz_localize(None))
 
+def _table_column(translator, expr):
+    op = expr.op()
+    field_name = op.name
+    quoted_name = quote_identifier(field_name, force=True)
 
-@translate.register(ops.DateNow)
-def date_now(op, **_):
-    return pl.lit(datetime.date.today())
+    table = op.table
+    ctx = translator.context
 
+    # If the column does not originate from the table set in the current SELECT
+    # context, we should format as a subquery
+    if translator.permit_subquery and ctx.is_foreign_expr(table):
+        proj_expr = table.projection([field_name]).to_array()
+        return _table_array_view(translator, proj_expr)
 
-@translate.register(ops.Strftime)
-def strftime(op, **kw):
-    arg = translate(op.arg, **kw)
-    fmt = _literal_value(op.format_str)
-    return arg.dt.strftime(format=fmt)
+    if ctx.need_aliases():
+        alias = ctx.get_ref(table)
+        if alias is not None:
+            quoted_name = '{0}.{1}'.format(alias, quoted_name)
 
+    return quoted_name
 
-@translate.register(ops.Date)
-def date(op, **kw):
-    arg = translate(op.arg, **kw)
-    return arg.cast(pl.Date)
 
+def _extract_field(sql_attr):
+    def extract_field_formatter(translator, expr):
+        op = expr.op()
+        arg = translator.translate(op.args[0])
 
-@translate.register(ops.DateTruncate)
-@translate.register(ops.TimestampTruncate)
-def temporal_truncate(op, **kw):
-    arg = translate(op.arg, **kw)
-    unit = "mo" if op.unit.short == "M" else op.unit.short
-    unit = f"1{unit.lower()}"
-    return arg.dt.truncate(unit).dt.offset_by("-1w")
+        # This is pre-2.0 Impala-style, which did not used to support the
+        # SQL-99 format extract($FIELD from expr)
+        return "extract({0!s}, '{1!s}')".format(arg, sql_attr)
+    return extract_field_formatter
 
 
-def _compile_literal_interval(op):
-    if not isinstance(op, ops.Literal):
-        raise com.UnsupportedOperationError(
-            "Only literal interval values are supported"
-        )
+def _truncate(translator, expr):
+    op = expr.op()
 
-    if op.dtype.unit.short == "M":
-        suffix = "mo"
-    else:
-        suffix = op.dtype.unit.short.lower()
+    arg = translator.translate(op.args[0])
 
-    return f"{op.value}{suffix}"
+    _impala_unit_names = {
+        'M': 'MONTH',
+        'D': 'J',
+        'J': 'D',
+        'H': 'HH'
+    }
 
+    unit = op.args[1]
+    unit = _impala_unit_names.get(unit, unit)
 
-@translate.register(ops.TimestampBucket)
-def timestamp_bucket(op, **kw):
-    arg = translate(op.arg, **kw)
-    interval = _compile_literal_interval(op.interval)
-    if op.offset is not None:
-        offset = _compile_literal_interval(op.offset)
-        neg_offset = offset[1:] if offset.startswith("-") else f"-{offset}"
-        arg = arg.dt.offset_by(neg_offset)
-    else:
-        offset = None
-    res = arg.dt.truncate(interval)
-    if offset is not None:
-        res = res.dt.offset_by(offset)
-    return res
+    return "trunc({0!s}, '{1!s}')".format(arg, unit)
 
 
-@translate.register(ops.DateFromYMD)
-def date_from_ymd(op, **kw):
-    return pl.date(
-        year=translate(op.year, **kw),
-        month=translate(op.month, **kw),
-        day=translate(op.day, **kw),
-    )
+def _timestamp_from_unix(translator, expr):
+    op = expr.op()
 
+    val, unit = op.args
 
-@translate.register(ops.Atan2)
-def atan2(op, **kw):
-    left = translate(op.left, **kw)
-    right = translate(op.right, **kw)
-    return pl.map_batches([left, right], lambda cols: np.arctan2(cols[0], cols[1]))
-
-
-@translate.register(ops.Modulus)
-def modulus(op, **kw):
-    left = translate(op.left, **kw)
-    right = translate(op.right, **kw)
-    return pl.map_batches([left, right], lambda cols: np.mod(cols[0], cols[1]))
-
-
-@translate.register(ops.TimestampFromYMDHMS)
-def timestamp_from_ymdhms(op, **kw):
-    return pl.datetime(
-        year=translate(op.year, **kw),
-        month=translate(op.month, **kw),
-        day=translate(op.day, **kw),
-        hour=translate(op.hours, **kw),
-        minute=translate(op.minutes, **kw),
-        second=translate(op.seconds, **kw),
-    )
+    if unit == 'ms':
+        val = (val / 1000).cast('int32')
+    elif unit == 'us':
+        val = (val / 1000000).cast('int32')
 
+    arg = _from_unixtime(translator, val)
+    return 'CAST({0} AS timestamp)'.format(arg)
 
-@translate.register(ops.TimestampFromUNIX)
-def timestamp_from_unix(op, **kw):
-    arg = translate(op.arg, **kw)
-    unit = op.unit.short
-    if unit == "s":
-        arg = arg.cast(pl.Int64) * 1_000
-        unit = "ms"
-    return arg.cast(pl.Datetime(time_unit=unit))
-
-
-@translate.register(ops.IntervalFromInteger)
-def interval_from_integer(op, **kw):
-    arg = translate(op.arg, **kw)
-    return _make_duration(arg, dt.Interval(unit=op.unit))
-
-
-@translate.register(ops.StringToDate)
-def string_to_date(op, **kw):
-    arg = translate(op.arg, **kw)
-    return arg.str.strptime(
-        dtype=pl.Date,
-        format=_literal_value(op.format_str),
-    )
 
+def _from_unixtime(translator, expr):
+    arg = translator.translate(expr)
+    return 'from_unixtime({0}, "yyyy-MM-dd HH:mm:ss")'.format(arg)
 
-@translate.register(ops.StringToTimestamp)
-def string_to_timestamp(op, **kw):
-    arg = translate(op.arg, **kw)
-    return arg.str.strptime(
-        dtype=pl.Datetime,
-        format=_literal_value(op.format_str),
-    )
 
+def varargs(func_name):
+    def varargs_formatter(translator, expr):
+        op = expr.op()
+        return _format_call(translator, func_name, *op.args)
+    return varargs_formatter
 
-@translate.register(ops.TimestampDiff)
-def timestamp_diff(op, **kw):
-    left = translate(op.left, **kw)
-    right = translate(op.right, **kw)
-    # TODO: truncating both to seconds is necessary to conform to the output
-    # type of the operation
-    return left.dt.truncate("1s") - right.dt.truncate("1s")
-
-
-@translate.register(ops.ArrayLength)
-def array_length(op, **kw):
-    arg = translate(op.arg, **kw)
-    return arg.list.len()
-
-
-@translate.register(ops.ArrayConcat)
-def array_concat(op, **kw):
-    result, *rest = map(partial(translate, **kw), op.arg)
-
-    for arg in rest:
-        result = result.list.concat(arg)
-
-    return result
-
-
-@translate.register(ops.Array)
-def array_column(op, **kw):
-    cols = [translate(col, **kw) for col in op.exprs]
-    return pl.concat_list(cols)
-
-
-@translate.register(ops.ArrayCollect)
-def array_collect(op, **kw):
-    arg = translate(op.arg, **kw)
-    if (where := op.where) is not None:
-        arg = arg.filter(translate(where, **kw))
-    return arg
-
-
-@translate.register(ops.ArrayFlatten)
-def array_flatten(op, **kw):
-    return pl.concat_list(translate(op.arg, **kw))
-
-
-_date_methods = {
-    ops.ExtractDay: "day",
-    ops.ExtractMonth: "month",
-    ops.ExtractYear: "year",
-    ops.ExtractQuarter: "quarter",
-    ops.ExtractDayOfYear: "ordinal_day",
-    ops.ExtractWeekOfYear: "week",
-    ops.ExtractHour: "hour",
-    ops.ExtractMinute: "minute",
-    ops.ExtractSecond: "second",
-    ops.ExtractMicrosecond: "microsecond",
-    ops.ExtractMillisecond: "millisecond",
-}
 
+def _substring(translator, expr):
+    op = expr.op()
+    arg, start, length = op.args
+    arg_formatted = translator.translate(arg)
+    start_formatted = translator.translate(start)
 
-@translate.register(ops.ExtractTemporalField)
-def extract_date_field(op, **kw):
-    arg = translate(op.arg, **kw)
-    method = operator.methodcaller(_date_methods[type(op)])
-    return method(arg.dt).cast(pl.Int32)
-
-
-@translate.register(ops.ExtractEpochSeconds)
-def extract_epoch_seconds(op, **kw):
-    arg = translate(op.arg, **kw)
-    return arg.dt.epoch("s").cast(pl.Int32)
-
-
-_day_of_week_offset = vparse(pl.__version__) >= vparse("0.15.1")
-
-
-_unary = {
-    # TODO(kszucs): factor out the lambdas
-    ops.Abs: operator.methodcaller("abs"),
-    ops.Acos: operator.methodcaller("arccos"),
-    ops.Asin: operator.methodcaller("arcsin"),
-    ops.Atan: operator.methodcaller("arctan"),
-    ops.Ceil: lambda arg: arg.ceil().cast(pl.Int64),
-    ops.Cos: operator.methodcaller("cos"),
-    ops.Cot: lambda arg: 1.0 / arg.tan(),
-    ops.DayOfWeekIndex: (
-        lambda arg: arg.dt.weekday().cast(pl.Int16) - _day_of_week_offset
-    ),
-    ops.Exp: operator.methodcaller("exp"),
-    ops.Floor: lambda arg: arg.floor().cast(pl.Int64),
-    ops.IsInf: operator.methodcaller("is_infinite"),
-    ops.IsNan: operator.methodcaller("is_nan"),
-    ops.IsNull: operator.methodcaller("is_null"),
-    ops.Ln: operator.methodcaller("log"),
-    ops.Log10: operator.methodcaller("log10"),
-    ops.Log2: lambda arg: arg.log(2),
-    ops.Negate: operator.neg,
-    ops.Not: operator.methodcaller("not_"),
-    ops.NotNull: operator.methodcaller("is_not_null"),
-    ops.Sin: operator.methodcaller("sin"),
-    ops.Sqrt: operator.methodcaller("sqrt"),
-    ops.Tan: operator.methodcaller("tan"),
-}
+    # Impala is 1-indexed
+    if length is None or isinstance(length.op(), ir.Literal):
+        lvalue = length.op().value if length else None
+        if lvalue:
+            return 'substr({0}, {1} + 1, {2})'.format(arg_formatted,
+                                                      start_formatted,
+                                                      lvalue)
+        else:
+            return 'substr({0}, {1} + 1)'.format(arg_formatted,
+                                                 start_formatted)
+    else:
+        length_formatted = translator.translate(length)
+        return 'substr({0}, {1} + 1, {2})'.format(arg_formatted,
+                                                  start_formatted,
+                                                  length_formatted)
+
+
+def _string_find(translator, expr):
+    op = expr.op()
+    arg, substr, start, _ = op.args
+    arg_formatted = translator.translate(arg)
+    substr_formatted = translator.translate(substr)
+
+    if start and not isinstance(start.op(), ir.Literal):
+        start_fmt = translator.translate(start)
+        return 'locate({0}, {1}, {2} + 1) - 1'.format(substr_formatted,
+                                                      arg_formatted,
+                                                      start_fmt)
+    elif start and start.op().value:
+        sval = start.op().value
+        return 'locate({0}, {1}, {2}) - 1'.format(substr_formatted,
+                                                  arg_formatted,
+                                                  sval + 1)
+    else:
+        return 'locate({0}, {1}) - 1'.format(substr_formatted, arg_formatted)
 
 
-@translate.register(ops.DayOfWeekName)
-def day_of_week_name(op, **kw):
-    index = translate(op.arg, **kw).dt.weekday() - _day_of_week_offset
-    arg = None
-    for i, name in enumerate(calendar.day_name):
-        arg = pl.when(index == i).then(pl.lit(name)).otherwise(arg)
-    return arg
-
-
-@translate.register(ops.Unary)
-def unary(op, **kw):
-    arg = translate(op.arg, **kw)
-    func = _unary.get(type(op))
-    if func is None:
-        raise com.OperationNotDefinedError(f"{type(op).__name__} not supported")
-    return func(arg)
-
-
-_comparisons = {
-    ops.Equals: operator.eq,
-    ops.Greater: operator.gt,
-    ops.GreaterEqual: operator.ge,
-    ops.Less: operator.lt,
-    ops.LessEqual: operator.le,
-    ops.NotEquals: operator.ne,
-}
+def _string_join(translator, expr):
+    op = expr.op()
+    arg, strings = op.args
+    return _format_call(translator, 'concat_ws', arg, *strings)
 
 
-@translate.register(ops.Comparison)
-def comparison(op, **kw):
-    left = translate(op.left, **kw)
-    right = translate(op.right, **kw)
-    func = _comparisons.get(type(op))
-    if func is None:
-        raise com.OperationNotDefinedError(f"{type(op).__name__} not supported")
-    return func(left, right)
-
-
-@translate.register(ops.Between)
-def between(op, **kw):
-    op_arg = op.arg
-    arg = translate(op_arg, **kw)
-    dtype = op_arg.dtype
-    lower = translate(ops.Cast(op.lower_bound, dtype), **kw)
-    upper = translate(ops.Cast(op.upper_bound, dtype), **kw)
-    return arg.is_between(lower, upper, closed="both")
-
-
-_bitwise_binops = {
-    ops.BitwiseRightShift: np.right_shift,
-    ops.BitwiseLeftShift: np.left_shift,
-    ops.BitwiseOr: np.bitwise_or,
-    ops.BitwiseAnd: np.bitwise_and,
-    ops.BitwiseXor: np.bitwise_xor,
-}
+def _parse_url(translator, expr):
+    op = expr.op()
 
+    arg, extract, key = op.args
+    arg_formatted = translator.translate(arg)
 
-@translate.register(ops.BitwiseBinary)
-def bitwise_binops(op, **kw):
-    ufunc = _bitwise_binops.get(type(op))
-    if ufunc is None:
-        raise com.OperationNotDefinedError(f"{type(op).__name__} not supported")
-    left = translate(op.left, **kw)
-    right = translate(op.right, **kw)
-
-    if isinstance(op.right, ops.Literal):
-        result = left.map_batches(lambda col: ufunc(col, op.right.value))
-    elif isinstance(op.left, ops.Literal):
-        result = right.map_batches(lambda col: ufunc(op.left.value, col))
+    if key is None:
+        return "parse_url({0}, '{1}')".format(arg_formatted, extract)
     else:
-        result = pl.map_batches([left, right], lambda cols: ufunc(cols[0], cols[1]))
+        key_fmt = translator.translate(key)
+        return "parse_url({0}, '{1}', {2})".format(arg_formatted,
+                                                   extract, key_fmt)
 
-    return result.cast(PolarsType.from_ibis(op.dtype))
 
+def _find_in_set(translator, expr):
+    op = expr.op()
 
-@translate.register(ops.BitwiseNot)
-def bitwise_not(op, **kw):
-    arg = translate(op.arg, **kw)
-    return arg.map_batches(lambda x: np.invert(x))
-
-
-_binops = {
-    ops.Add: operator.add,
-    ops.And: operator.and_,
-    ops.DateAdd: operator.add,
-    ops.DateSub: operator.sub,
-    ops.DateDiff: operator.sub,
-    ops.TimestampAdd: operator.add,
-    ops.TimestampSub: operator.sub,
-    ops.IntervalSubtract: operator.sub,
-    ops.Divide: operator.truediv,
-    ops.FloorDivide: operator.floordiv,
-    ops.Multiply: operator.mul,
-    ops.Or: operator.or_,
-    ops.Xor: operator.xor,
-    ops.Subtract: operator.sub,
-}
+    arg, str_list = op.args
+    arg_formatted = translator.translate(arg)
+    str_formatted = ','.join([x._arg.value for x in str_list])
+    return "find_in_set({0}, '{1}') - 1".format(arg_formatted, str_formatted)
 
 
-@translate.register(ops.Binary)
-def binop(op, **kw):
-    left = translate(op.left, **kw)
-    right = translate(op.right, **kw)
-    func = _binops.get(type(op))
-    if func is None:
-        raise com.OperationNotDefinedError(f"{type(op).__name__} not supported")
-    return func(left, right)
-
-
-@translate.register(ops.ElementWiseVectorizedUDF)
-def elementwise_udf(op, **kw):
-    func_args = [translate(arg, **kw) for arg in op.func_args]
-    return_type = PolarsType.from_ibis(op.return_type)
+def _round(translator, expr):
+    op = expr.op()
+    arg, digits = op.args
 
-    return pl.map_batches(
-        func_args, lambda args: op.func(*args), return_dtype=return_type
-    )
+    arg_formatted = translator.translate(arg)
+
+    if digits is not None:
+        digits_formatted = translator.translate(digits)
+        return 'round({0}, {1})'.format(arg_formatted,
+                                        digits_formatted)
+    else:
+        return 'round({0})'.format(arg_formatted)
 
 
-@translate.register(ops.E)
-def execute_e(op, **_):
-    return pl.lit(np.e)
+def _hash(translator, expr):
+    op = expr.op()
+    arg, how = op.args
 
+    arg_formatted = translator.translate(arg)
 
-@translate.register(ops.Pi)
-def execute_pi(op, **_):
-    return pl.lit(np.pi)
+    if how == 'fnv':
+        return 'fnv_hash({0})'.format(arg_formatted)
+    else:
+        raise NotImplementedError(how)
 
 
-@translate.register(ops.Time)
-def execute_time(op, **kw):
-    arg = translate(op.arg, **kw)
-    if op.arg.dtype.is_timestamp():
-        return arg.dt.truncate("1us").cast(pl.Time)
-    return arg
+def _log(translator, expr):
+    op = expr.op()
+    arg, base = op.args
+    arg_formatted = translator.translate(arg)
 
+    if base is None:
+        return 'ln({0})'.format(arg_formatted)
+    else:
+        return 'log({0}, {1})'.format(arg_formatted,
+                                      translator.translate(base))
 
-@translate.register(ops.Union)
-def execute_union(op, **kw):
-    result = pl.concat([translate(op.left, **kw), translate(op.right, **kw)])
-    if op.distinct:
-        return result.unique()
-    return result
 
+def _count_distinct(translator, expr):
+    op = expr.op()
+    arg_formatted = translator.translate(op.args[0])
+    return 'COUNT(DISTINCT {0})'.format(arg_formatted)
+
+
+def _literal(translator, expr):
+    if isinstance(expr, ir.BooleanValue):
+        typeclass = 'boolean'
+    elif isinstance(expr, ir.StringValue):
+        typeclass = 'string'
+    elif isinstance(expr, ir.NumericValue):
+        typeclass = 'number'
+    elif isinstance(expr, ir.TimestampValue):
+        typeclass = 'timestamp'
+    else:
+        raise NotImplementedError
 
-@translate.register(ops.Hash)
-def execute_hash(op, **kw):
-    return translate(op.arg, **kw).hash()
+    return _literal_formatters[typeclass](expr)
 
 
-def _arg_min_max(op, func, **kw):
-    key = op.key
-    arg = op.arg
+def _null_literal(translator, expr):
+    return 'NULL'
 
-    if (op_where := op.where) is not None:
-        key = ops.IfElse(op_where, key, None)
-        arg = ops.IfElse(op_where, arg, None)
 
-    translate_arg = translate(arg, **kw)
-    translate_key = translate(key, **kw)
+_literal_formatters = {
+    'boolean': _boolean_literal_format,
+    'number': _number_literal_format,
+    'string': _string_literal_format,
+    'timestamp': _timestamp_literal_format
+}
+
+
+def _value_list(translator, expr):
+    op = expr.op()
+    formatted = [translator.translate(x) for x in op.values]
+    return '({0})'.format(', '.join(formatted))
 
-    not_null_mask = translate_arg.is_not_null() & translate_key.is_not_null()
-    return translate_arg.filter(not_null_mask).gather(
-        func(translate_key.filter(not_null_mask))
-    )
 
+_subtract_one = '{0} - 1'.format
 
-@translate.register(ops.ArgMax)
-def execute_arg_max(op, **kw):
-    return _arg_min_max(op, pl.Expr.arg_max, **kw)
-
-
-@translate.register(ops.ArgMin)
-def execute_arg_min(op, **kw):
-    return _arg_min_max(op, pl.Expr.arg_min, **kw)
-
-
-@translate.register(ops.SQLStringView)
-def execute_sql_string_view(op, *, ctx: pl.SQLContext, **kw):
-    translate(op.child, ctx=ctx, **kw)
-    return ctx.execute(op.query)
-
-
-@translate.register(ops.View)
-def execute_view(op, *, ctx: pl.SQLContext, **kw):
-    child = translate(op.child, ctx=ctx, **kw)
-    ctx.register(op.name, child)
-    return child
-
-
-@translate.register(ops.Reference)
-def execute_reference(op, **kw):
-    return translate(op.parent, **kw)
-
-
-@translate.register(ops.CountDistinctStar)
-def execute_count_distinct_star(op, **kw):
-    arg = pl.struct(*op.arg.schema.names)
-    if op.where is not None:
-        arg = arg.filter(translate(op.where, **kw))
-    return arg.n_unique()
-
-
-_UDF_INVOKERS = {
-    # Convert polars series into a list
-    #   -> map the function element by element
-    #   -> convert back to a polars series
-    InputType.PYTHON: lambda func, dtype, args: pl.Series(
-        map(func, *(arg.to_list() for arg in args)),
-        dtype=PolarsType.from_ibis(dtype),
-    ),
-    # Convert polars series into a pyarrow array
-    #  -> invoke the function on the pyarrow array
-    #  -> cast the result to match the ibis dtype
-    #  -> convert back to a polars series
-    InputType.PYARROW: lambda func, dtype, args: pl.from_arrow(
-        func(*(arg.to_arrow() for arg in args)).cast(dtype.to_pyarrow()),
-    ),
+
+_expr_transforms = {
+    ops.RowNumber: _subtract_one,
+    ops.DenseRank: _subtract_one,
+    ops.MinRank: _subtract_one,
 }
 
 
-@translate.register(ops.ScalarUDF)
-def execute_scalar_udf(op, **kw):
-    input_type = op.__input_type__
-    if input_type in _UDF_INVOKERS:
-        dtype = op.dtype
-        return pl.map_batches(
-            exprs=[translate(arg, **kw) for arg in op.args],
-            function=partial(_UDF_INVOKERS[input_type], op.__func__, dtype),
-            return_dtype=PolarsType.from_ibis(dtype),
-        )
-    elif input_type == InputType.BUILTIN:
-        first, *rest = map(translate, op.args)
-        return getattr(first, op.__func_name__)(*rest)
-    else:
-        raise NotImplementedError(
-            f"UDF input type {input_type} not supported for Polars"
-        )
-
-
-@translate.register(ops.AggUDF)
-def execute_agg_udf(op, **kw):
-    args = (arg for name, arg in zip(op.argnames, op.args) if name != "where")
-    first, *rest = map(partial(translate, **kw), args)
-    if (where := op.where) is not None:
-        first = first.filter(translate(where, **kw))
-    return getattr(first, op.__func_name__)(*rest)
-
-
-@translate.register(ops.RegexSplit)
-def execute_regex_split(op, **kw):
-    import pyarrow.compute as pc
-
-    def split(args):
-        arg, patterns = args
-        if len(patterns) != 1:
-            raise com.IbisError(
-                "Only a single scalar pattern is supported for Polars re_split"
-            )
-        return pl.from_arrow(pc.split_pattern_regex(arg.to_arrow(), patterns[0]))
-
-    arg = translate(op.arg, **kw)
-    pattern = translate(op.pattern, **kw)
-    return pl.map_batches(
-        exprs=(arg, pattern),
-        function=split,
-        return_dtype=PolarsType.from_ibis(op.dtype),
-    )
+_binary_infix_ops = {
+    # Binary operations
+    ops.Add: _binary_infix_op('+'),
+    ops.Subtract: _binary_infix_op('-'),
+    ops.Multiply: _binary_infix_op('*'),
+    ops.Divide: _binary_infix_op('/'),
+    ops.Power: fixed_arity('pow', 2),
+    ops.Modulus: _binary_infix_op('%'),
+
+    # Comparisons
+    ops.Equals: _binary_infix_op('='),
+    ops.NotEquals: _binary_infix_op('!='),
+    ops.GreaterEqual: _binary_infix_op('>='),
+    ops.Greater: _binary_infix_op('>'),
+    ops.LessEqual: _binary_infix_op('<='),
+    ops.Less: _binary_infix_op('<'),
+
+    # Boolean comparisons
+    ops.And: _binary_infix_op('AND'),
+    ops.Or: _binary_infix_op('OR'),
+    ops.Xor: _xor,
+}
+
+
+_operation_registry = {
+    # Unary operations
+    ops.NotNull: _not_null,
+    ops.IsNull: _is_null,
+    ops.Negate: _negate,
+
+    ops.IfNull: _ifnull_workaround,
+    ops.NullIf: fixed_arity('nullif', 2),
+
+    ops.ZeroIfNull: unary('zeroifnull'),
+    ops.NullIfZero: unary('nullifzero'),
+
+    ops.Abs: unary('abs'),
+    ops.BaseConvert: fixed_arity('conv', 3),
+    ops.Ceil: unary('ceil'),
+    ops.Floor: unary('floor'),
+    ops.Exp: unary('exp'),
+    ops.Round: _round,
+
+    ops.Sign: unary('sign'),
+    ops.Sqrt: unary('sqrt'),
+
+    ops.Hash: _hash,
+
+    ops.Log: _log,
+    ops.Ln: unary('ln'),
+    ops.Log2: unary('log2'),
+    ops.Log10: unary('log10'),
+
+    ops.DecimalPrecision: unary('precision'),
+    ops.DecimalScale: unary('scale'),
+
+    # Unary aggregates
+    ops.CMSMedian: _reduction('appx_median'),
+    ops.HLLCardinality: _reduction('ndv'),
+    ops.Mean: _reduction('avg'),
+    ops.Sum: _reduction('sum'),
+    ops.Max: _reduction('max'),
+    ops.Min: _reduction('min'),
+
+    ops.StandardDev: _variance_like('stddev'),
+    ops.Variance: _variance_like('variance'),
+
+    ops.GroupConcat: fixed_arity('group_concat', 2),
+
+    ops.Count: _reduction('count'),
+    ops.CountDistinct: _count_distinct,
+
+    # string operations
+    ops.StringLength: unary('length'),
+    ops.StringAscii: unary('ascii'),
+    ops.Lowercase: unary('lower'),
+    ops.Uppercase: unary('upper'),
+    ops.Reverse: unary('reverse'),
+    ops.Strip: unary('trim'),
+    ops.LStrip: unary('ltrim'),
+    ops.RStrip: unary('rtrim'),
+    ops.Capitalize: unary('initcap'),
+    ops.Substring: _substring,
+    ops.StrRight: fixed_arity('strright', 2),
+    ops.Repeat: fixed_arity('repeat', 2),
+    ops.StringFind: _string_find,
+    ops.Translate: fixed_arity('translate', 3),
+    ops.FindInSet: _find_in_set,
+    ops.LPad: fixed_arity('lpad', 3),
+    ops.RPad: fixed_arity('rpad', 3),
+    ops.StringJoin: _string_join,
+    ops.StringSQLLike: _binary_infix_op('LIKE'),
+    ops.RegexSearch: _binary_infix_op('RLIKE'),
+    ops.RegexExtract: fixed_arity('regexp_extract', 3),
+    ops.RegexReplace: fixed_arity('regexp_replace', 3),
+    ops.ParseURL: _parse_url,
+
+    # Timestamp operations
+    ops.TimestampNow: lambda *args: 'now()',
+    ops.ExtractYear: _extract_field('year'),
+    ops.ExtractMonth: _extract_field('month'),
+    ops.ExtractDay: _extract_field('day'),
+    ops.ExtractHour: _extract_field('hour'),
+    ops.ExtractMinute: _extract_field('minute'),
+    ops.ExtractSecond: _extract_field('second'),
+    ops.ExtractMillisecond: _extract_field('millisecond'),
+    ops.Truncate: _truncate,
+
+    # Other operations
+    ops.E: lambda *args: 'e()',
+
+    ir.Literal: _literal,
+    ir.NullLiteral: _null_literal,
+
+    ir.ValueList: _value_list,
+
+    ops.Cast: _cast,
+
+    ops.Coalesce: varargs('coalesce'),
+    ops.Greatest: varargs('greatest'),
+    ops.Least: varargs('least'),
+
+    ops.Where: fixed_arity('if', 3),
+
+    ops.Between: _between,
+    ops.Contains: _binary_infix_op('IN'),
+    ops.NotContains: _binary_infix_op('NOT IN'),
+
+    ops.SimpleCase: _simple_case,
+    ops.SearchedCase: _searched_case,
+
+    ops.TableColumn: _table_column,
+
+    ops.TableArrayView: _table_array_view,
+
+    ops.TimestampDelta: _timestamp_delta,
+    ops.TimestampFromUNIX: _timestamp_from_unix,
+
+    transforms.ExistsSubquery: _exists_subquery,
+    transforms.NotExistsSubquery: _exists_subquery,
+
+    # RowNumber, and rank functions starts with 0 in Ibis-land
+    ops.RowNumber: lambda *args: 'row_number()',
+    ops.DenseRank: lambda *args: 'dense_rank()',
+    ops.MinRank: lambda *args: 'rank()',
+
+    ops.FirstValue: unary('first_value'),
+    ops.LastValue: unary('last_value'),
+    ops.NthValue: _nth_value,
+    ops.Lag: _shift_like('lag'),
+    ops.Lead: _shift_like('lead'),
+    ops.WindowOp: _window
+}
+
+_operation_registry.update(_binary_infix_ops)
+
+
+class ImpalaExprTranslator(comp.ExprTranslator):
+
+    _registry = _operation_registry
+    _context_class = ImpalaContext
+
+    def name(self, translated, name, force=True):
+        return _name_expr(translated,
+                          quote_identifier(name, force=force))
+
+compiles = ImpalaExprTranslator.compiles
+rewrites = ImpalaExprTranslator.rewrites
 
 
-@translate.register(ops.IntegerRange)
-def execute_integer_range(op, **kw):
-    if not isinstance(op.step, ops.Literal):
-        raise com.UnsupportedOperationError(
-            "Dynamic integer step not supported by Polars"
-        )
-    step = op.step.value
-
-    dtype = PolarsType.from_ibis(op.dtype.value_type)
-    empty = pl.int_ranges(0, 0, dtype=dtype)
-
-    if step == 0:
-        return empty
-
-    start = translate(op.start, **kw)
-    stop = translate(op.stop, **kw)
-    return pl.int_ranges(start, stop, step, dtype=dtype)
-
-
-@translate.register(ops.TimestampRange)
-def execute_timestamp_range(op, **kw):
-    if not isinstance(op.step, ops.Literal):
-        raise com.UnsupportedOperationError(
-            "Dynamic interval step not supported by Polars"
-        )
-    step = op.step.value
-    unit = op.step.dtype.unit.value
-
-    start = translate(op.start, **kw)
-    stop = translate(op.stop, **kw)
-    return pl.datetime_ranges(start, stop, f"{step}{unit}", closed="left")
+@rewrites(ops.FloorDivide)
+def _floor_divide(expr):
+    left, right = expr.op().args
+    return left.div(right).floor()
```

### Comparing `ibis_framework-9.0.0.dev686/ibis/backends/snowflake/compiler.py` & `ibis-framework-v0.6.0/ibis/expr/rules.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,624 +1,718 @@
-from __future__ import annotations
+# Copyright 2015 Cloudera Inc.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+# http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-import itertools
-from functools import partial
+from collections import defaultdict
+import operator
 
-import sqlglot as sg
-import sqlglot.expressions as sge
-from public import public
-
-import ibis.common.exceptions as com
+from ibis.common import IbisTypeError
+from ibis.compat import py_string
 import ibis.expr.datatypes as dt
-import ibis.expr.operations as ops
-from ibis import util
-from ibis.backends.sql.compiler import NULL, C, FuncGen, SQLGlotCompiler
-from ibis.backends.sql.datatypes import SnowflakeType
-from ibis.backends.sql.dialects import Snowflake
-from ibis.backends.sql.rewrites import (
-    exclude_unsupported_window_frame_from_ops,
-    exclude_unsupported_window_frame_from_row_number,
-    replace_log2,
-    replace_log10,
-    rewrite_empty_order_by_window,
-)
-from ibis.expr.rewrites import rewrite_stringslice
-
-
-class SnowflakeFuncGen(FuncGen):
-    udf = FuncGen(namespace="ibis_udfs.public")
-
-
-@public
-class SnowflakeCompiler(SQLGlotCompiler):
-    __slots__ = ()
-
-    dialect = Snowflake
-    type_mapper = SnowflakeType
-    no_limit_value = NULL
-    rewrites = (
-        exclude_unsupported_window_frame_from_row_number,
-        exclude_unsupported_window_frame_from_ops,
-        rewrite_empty_order_by_window,
-        rewrite_stringslice,
-        replace_log2,
-        replace_log10,
-        *SQLGlotCompiler.rewrites,
-    )
-
-    UNSUPPORTED_OPERATIONS = frozenset(
-        (
-            ops.ArrayMap,
-            ops.ArrayFilter,
-            ops.RowID,
-            ops.MultiQuantile,
-            ops.IntervalFromInteger,
-            ops.IntervalAdd,
-            ops.TimestampDiff,
-        )
-    )
-
-    SIMPLE_OPS = {
-        ops.All: "min",
-        ops.Any: "max",
-        ops.ArrayDistinct: "array_distinct",
-        ops.ArrayFlatten: "array_flatten",
-        ops.ArrayIndex: "get",
-        ops.ArrayIntersect: "array_intersection",
-        ops.ArrayRemove: "array_remove",
-        ops.BitAnd: "bitand_agg",
-        ops.BitOr: "bitor_agg",
-        ops.BitXor: "bitxor_agg",
-        ops.BitwiseAnd: "bitand",
-        ops.BitwiseLeftShift: "bitshiftleft",
-        ops.BitwiseNot: "bitnot",
-        ops.BitwiseOr: "bitor",
-        ops.BitwiseRightShift: "bitshiftright",
-        ops.BitwiseXor: "bitxor",
-        ops.EndsWith: "endswith",
-        ops.Hash: "hash",
-        ops.Median: "median",
-        ops.Mode: "mode",
-        ops.StringToDate: "to_date",
-        ops.StringToTimestamp: "to_timestamp_tz",
-        ops.TimeFromHMS: "time_from_parts",
-        ops.TimestampFromYMDHMS: "timestamp_from_parts",
-    }
+import ibis.expr.types as ir
+import ibis.common as com
+import ibis.util as util
+
+
+class BinaryPromoter(object):
+    # placeholder for type promotions for basic binary arithmetic
+
+    def __init__(self, left, right, op):
+        self.args = [left, right]
+        self.left = left
+        self.right = right
+        self.op = op
+
+        self._check_compatibility()
+
+    def get_result(self):
+        promoted_type = self._get_type()
+        return shape_like_args(self.args, promoted_type)
+
+    def _get_type(self):
+        if util.any_of(self.args, ir.FloatingValue):
+            if util.any_of(self.args, ir.DoubleValue):
+                return 'double'
+            else:
+                return 'float'
+        elif util.all_of(self.args, ir.IntegerValue):
+            return self._get_int_type()
+        elif util.any_of(self.args, ir.DecimalValue):
+            return _decimal_promoted_type(self.args)
+        else:
+            raise NotImplementedError
 
-    def __init__(self):
-        super().__init__()
-        self.f = SnowflakeFuncGen()
-
-    def _aggregate(self, funcname: str, *args, where):
-        if where is not None:
-            args = [self.if_(where, arg, NULL) for arg in args]
-
-        func = self.f[funcname]
-        return func(*args)
-
-    @staticmethod
-    def _minimize_spec(start, end, spec):
-        if (
-            start is None
-            and isinstance(getattr(end, "value", None), ops.Literal)
-            and end.value.value == 0
-            and end.following
-        ):
-            return None
-        return spec
-
-    def visit_Literal(self, op, *, value, dtype):
-        if value is None:
-            return super().visit_Literal(op, value=value, dtype=dtype)
-        elif dtype.is_timestamp():
-            args = (
-                value.year,
-                value.month,
-                value.day,
-                value.hour,
-                value.minute,
-                value.second,
-                value.microsecond * 1_000,
-            )
-            if value.tzinfo is not None:
-                return self.f.timestamp_tz_from_parts(*args, dtype.timezone)
+    def _get_int_type(self):
+        deps = [x.op() for x in self.args]
+
+        if util.all_of(deps, ir.Literal):
+            return _smallest_int_containing(
+                [self.op(deps[0].value, deps[1].value)])
+        elif util.any_of(deps, ir.Literal):
+            if isinstance(deps[0], ir.Literal):
+                val = deps[0].value
+                atype = self.args[1].type()
             else:
-                # workaround sqlglot not supporting more than 6 arguments by
-                # using an anonymous function
-                return self.f.anon.timestamp_from_parts(*args)
-        elif dtype.is_time():
-            nanos = value.microsecond * 1_000
-            return self.f.time_from_parts(value.hour, value.minute, value.second, nanos)
-        elif dtype.is_map():
-            key_type = dtype.key_type
-            value_type = dtype.value_type
-
-            pairs = []
-
-            for k, v in value.items():
-                pairs.append(
-                    self.visit_Literal(
-                        ops.Literal(k, key_type), value=k, dtype=key_type
-                    )
-                )
-                pairs.append(
-                    self.visit_Literal(
-                        ops.Literal(v, value_type), value=v, dtype=value_type
-                    )
-                )
-
-            return self.f.object_construct_keep_null(*pairs)
-        elif dtype.is_struct():
-            pairs = []
-            for k, v in value.items():
-                pairs.append(k)
-                pairs.append(
-                    self.visit_Literal(
-                        ops.Literal(v, dtype[k]), value=v, dtype=dtype[k]
-                    )
-                )
-            return self.f.object_construct_keep_null(*pairs)
-        elif dtype.is_uuid():
-            return sge.convert(str(value))
-        elif dtype.is_binary():
-            return sge.HexString(this=value.hex())
-        return super().visit_Literal(op, value=value, dtype=dtype)
-
-    def visit_Arbitrary(self, op, *, arg, where):
-        return self.f.get(self.agg.array_agg(arg, where=where), 0)
-
-    def visit_Cast(self, op, *, arg, to):
-        if to.is_struct() or to.is_map():
-            return self.if_(self.f.is_object(arg), arg, NULL)
-        elif to.is_array():
-            return self.if_(self.f.is_array(arg), arg, NULL)
-        return self.cast(arg, to)
-
-    def visit_ToJSONMap(self, op, *, arg):
-        return self.if_(self.f.is_object(arg), arg, NULL)
-
-    def visit_ToJSONArray(self, op, *, arg):
-        return self.if_(self.f.is_array(arg), arg, NULL)
-
-    def visit_UnwrapJSONString(self, op, *, arg):
-        return self.if_(self.f.is_varchar(arg), self.f.as_varchar(arg), NULL)
-
-    def visit_UnwrapJSONInt64(self, op, *, arg):
-        return self.if_(self.f.is_integer(arg), self.f.as_integer(arg), NULL)
-
-    def visit_UnwrapJSONFloat64(self, op, *, arg):
-        return self.if_(self.f.is_double(arg), self.f.as_double(arg), NULL)
-
-    def visit_UnwrapJSONBoolean(self, op, *, arg):
-        return self.if_(self.f.is_boolean(arg), self.f.as_boolean(arg), NULL)
-
-    def visit_IsNan(self, op, *, arg):
-        return arg.eq(self.NAN)
-
-    def visit_IsInf(self, op, *, arg):
-        return arg.isin(self.POS_INF, self.NEG_INF)
-
-    def visit_JSONGetItem(self, op, *, arg, index):
-        return self.f.get(arg, index)
-
-    def visit_StringFind(self, op, *, arg, substr, start, end):
-        args = [substr, arg]
-        if start is not None:
-            args.append(start + 1)
-        return self.f.position(*args)
-
-    def visit_RegexSplit(self, op, *, arg, pattern):
-        return self.f.udf.regexp_split(arg, pattern)
-
-    def visit_Map(self, op, *, keys, values):
-        return self.if_(
-            sg.and_(self.f.is_array(keys), self.f.is_array(values)),
-            self.f.arrays_to_object(keys, values),
-            NULL,
-        )
-
-    def visit_MapKeys(self, op, *, arg):
-        return self.if_(self.f.is_object(arg), self.f.object_keys(arg), NULL)
-
-    def visit_MapValues(self, op, *, arg):
-        return self.if_(self.f.is_object(arg), self.f.udf.object_values(arg), NULL)
-
-    def visit_MapGet(self, op, *, arg, key, default):
-        dtype = op.dtype
-        expr = self.f.coalesce(self.f.get(arg, key), self.f.to_variant(default))
-        if dtype.is_json() or dtype.is_null():
-            return expr
-        return self.cast(expr, dtype)
-
-    def visit_MapContains(self, op, *, arg, key):
-        return self.f.array_contains(
-            self.if_(self.f.is_object(arg), self.f.object_keys(arg), NULL),
-            self.f.to_variant(key),
-        )
-
-    def visit_MapMerge(self, op, *, left, right):
-        return self.if_(
-            sg.and_(self.f.is_object(left), self.f.is_object(right)),
-            self.f.udf.object_merge(left, right),
-            NULL,
-        )
-
-    def visit_MapLength(self, op, *, arg):
-        return self.if_(
-            self.f.is_object(arg), self.f.array_size(self.f.object_keys(arg)), NULL
-        )
-
-    def visit_Log(self, op, *, arg, base):
-        return self.f.log(base, arg, dialect=self.dialect)
-
-    def visit_RandomScalar(self, op, **kwargs):
-        return self.f.uniform(
-            self.f.to_double(0.0), self.f.to_double(1.0), self.f.random()
-        )
-
-    def visit_RandomUUID(self, op, **kwargs):
-        return self.f.uuid_string()
-
-    def visit_ApproxMedian(self, op, *, arg, where):
-        return self.agg.approx_percentile(arg, 0.5, where=where)
-
-    def visit_TimeDelta(self, op, *, part, left, right):
-        return self.f.timediff(part, right, left, dialect=self.dialect)
-
-    def visit_DateDelta(self, op, *, part, left, right):
-        return self.f.datediff(part, right, left, dialect=self.dialect)
-
-    def visit_TimestampDelta(self, op, *, part, left, right):
-        return self.f.timestampdiff(part, right, left, dialect=self.dialect)
-
-    def visit_TimestampDateAdd(self, op, *, left, right):
-        if not isinstance(op.right, ops.Literal):
-            raise com.OperationNotDefinedError(
-                f"right side of {type(op).__name__} operation must be an interval literal"
-            )
-        return sg.exp.Add(this=left, expression=right)
-
-    visit_DateAdd = visit_TimestampAdd = visit_TimestampDateAdd
-
-    def visit_IntegerRange(self, op, *, start, stop, step):
-        return self.if_(
-            step.neq(0), self.f.array_generate_range(start, stop, step), self.f.array()
-        )
-
-    def visit_StructColumn(self, op, *, names, values):
-        return self.f.object_construct_keep_null(
-            *itertools.chain.from_iterable(zip(names, values))
-        )
-
-    def visit_StructField(self, op, *, arg, field):
-        return self.cast(self.f.get(arg, field), op.dtype)
-
-    def visit_RegexSearch(self, op, *, arg, pattern):
-        return sge.RegexpLike(
-            this=arg,
-            expression=self.f.concat(".*", pattern, ".*"),
-            flag=sge.convert("cs"),
-        )
-
-    def visit_RegexReplace(self, op, *, arg, pattern, replacement):
-        return sge.RegexpReplace(this=arg, expression=pattern, replacement=replacement)
-
-    def visit_TypeOf(self, op, *, arg):
-        return self.f.typeof(self.f.to_variant(arg))
-
-    def visit_ArrayRepeat(self, op, *, arg, times):
-        return self.f.udf.array_repeat(arg, times)
-
-    def visit_ArrayUnion(self, op, *, left, right):
-        return self.f.array_distinct(self.f.array_cat(left, right))
-
-    def visit_ArrayContains(self, op, *, arg, other):
-        return self.f.array_contains(arg, self.f.to_variant(other))
-
-    def visit_ArrayCollect(self, op, *, arg, where):
-        return self.agg.array_agg(
-            self.f.ifnull(arg, self.f.parse_json("null")), where=where
-        )
-
-    def visit_ArrayConcat(self, op, *, arg):
-        # array_cat only accepts two arguments
-        return self.f.array_flatten(self.f.array(*arg))
-
-    def visit_ArrayPosition(self, op, *, arg, other):
-        # snowflake is zero-based here, so we don't need to subtract 1 from the
-        # result
-        return self.f.coalesce(
-            self.f.array_position(self.f.to_variant(other), arg) + 1, 0
-        )
-
-    def visit_RegexExtract(self, op, *, arg, pattern, index):
-        # https://docs.snowflake.com/en/sql-reference/functions/regexp_substr
-        return sge.RegexpExtract(
-            this=arg,
-            expression=pattern,
-            position=sge.convert(1),
-            group=index,
-            parameters=sge.convert("ce"),
-        )
-
-    def visit_ArrayZip(self, op, *, arg):
-        return self.if_(
-            sg.not_(sg.or_(*(arr.is_(NULL) for arr in arg))),
-            self.f.udf.array_zip(self.f.array(*arg)),
-            NULL,
-        )
-
-    def visit_DayOfWeekName(self, op, *, arg):
-        return sge.Case(
-            this=self.f.dayname(arg),
-            ifs=[
-                self.if_("Sun", "Sunday"),
-                self.if_("Mon", "Monday"),
-                self.if_("Tue", "Tuesday"),
-                self.if_("Wed", "Wednesday"),
-                self.if_("Thu", "Thursday"),
-                self.if_("Fri", "Friday"),
-                self.if_("Sat", "Saturday"),
-            ],
-            default=NULL,
-        )
-
-    def visit_TimestampFromUNIX(self, op, *, arg, unit):
-        timestamp_units_to_scale = {"s": 0, "ms": 3, "us": 6, "ns": 9}
-        return self.f.to_timestamp(arg, timestamp_units_to_scale[unit.short])
-
-    def visit_First(self, op, *, arg, where):
-        return self.f.get(self.agg.array_agg(arg, where=where), 0)
-
-    def visit_Last(self, op, *, arg, where):
-        expr = self.agg.array_agg(arg, where=where)
-        return self.f.get(expr, self.f.array_size(expr) - 1)
-
-    def visit_GroupConcat(self, op, *, arg, where, sep):
-        if where is None:
-            return self.f.listagg(arg, sep)
-
-        return self.if_(
-            self.f.count_if(where) > 0,
-            self.f.listagg(self.if_(where, arg, NULL), sep),
-            NULL,
-        )
-
-    def visit_TimestampBucket(self, op, *, arg, interval, offset):
-        if offset is not None:
-            raise com.UnsupportedOperationError(
-                "`offset` is not supported in the Snowflake backend for timestamp bucketing"
-            )
-
-        interval = op.interval
-        if not isinstance(interval, ops.Literal):
-            raise com.UnsupportedOperationError(
-                f"Interval must be a literal for the Snowflake backend, got {type(interval)}"
-            )
-
-        return self.f.time_slice(arg, interval.value, interval.dtype.unit.name)
-
-    def visit_ArraySlice(self, op, *, arg, start, stop):
-        if start is None:
-            start = 0
-
-        if stop is None:
-            stop = self.f.array_size(arg)
-        return self.f.array_slice(arg, start, stop)
-
-    def visit_ExtractEpochSeconds(self, op, *, arg):
-        return self.f.extract("epoch", arg)
-
-    def visit_ExtractMicrosecond(self, op, *, arg):
-        return self.f.extract("epoch_microsecond", arg) % 1_000_000
-
-    def visit_ExtractMillisecond(self, op, *, arg):
-        return self.f.extract("epoch_millisecond", arg) % 1_000
-
-    def visit_ExtractQuery(self, op, *, arg, key):
-        parsed_url = self.f.parse_url(arg, 1)
-        if key is not None:
-            r = self.f.get(self.f.get(parsed_url, "parameters"), key)
+                val = deps[1].value
+                atype = self.args[0].type()
+            return _int_one_literal_promotion(atype, val, self.op)
         else:
-            r = self.f.get(parsed_url, "query")
-        return self.f.nullif(self.f.as_varchar(r), "")
+            return _int_bounds_promotion(self.left.type(),
+                                         self.right.type(), self.op)
+
+    def _check_compatibility(self):
+        if (util.any_of(self.args, ir.StringValue) and
+                not util.all_of(self.args, ir.StringValue)):
+            raise TypeError('String and non-string incompatible')
+
+
+def _decimal_promoted_type(args):
+    precisions = []
+    scales = []
+    for arg in args:
+        if isinstance(arg, ir.DecimalValue):
+            precisions.append(arg.meta.precision)
+            scales.append(arg.meta.scale)
+    return dt.Decimal(max(precisions), max(scales))
+
+
+class PowerPromoter(BinaryPromoter):
+
+    def __init__(self, left, right):
+        super(PowerPromoter, self).__init__(left, right, operator.pow)
+
+    def _get_type(self):
+        rval = self.args[1].op()
+
+        if util.any_of(self.args, ir.FloatingValue):
+            if util.any_of(self.args, ir.DoubleValue):
+                return 'double'
+            else:
+                return 'float'
+        elif util.any_of(self.args, ir.DecimalValue):
+            return _decimal_promoted_type(self.args)
+        elif isinstance(rval, ir.Literal) and rval.value < 0:
+            return 'double'
+        elif util.all_of(self.args, ir.IntegerValue):
+            return self._get_int_type()
+        else:
+            raise NotImplementedError
+
+
+def highest_precedence_type(exprs):
+    # Return the highest precedence type from the passed expressions. Also
+    # verifies that there are valid implicit casts between any of the types and
+    # the selected highest precedence type
+    selector = _TypePrecedence(exprs)
+    return selector.get_result()
+
+
+class _TypePrecedence(object):
+    # Impala type precedence (more complex in other database implementations)
+    # timestamp
+    # double
+    # float
+    # decimal
+    # bigint
+    # int
+    # smallint
+    # tinyint
+    # boolean
+    # string
+
+    _precedence = {
+        'double': 9,
+        'float': 8,
+        'decimal': 7,
+        'int64': 6,
+        'int32': 5,
+        'int16': 4,
+        'int8': 3,
+        'boolean': 2,
+        'string': 1,
+        'null': 0
+    }
+
+    def __init__(self, exprs):
+        self.exprs = exprs
+
+        if len(exprs) == 0:
+            raise ValueError('Must pass at least one expression')
+
+        self.type_counts = defaultdict(lambda: 0)
+        self._count_types()
+
+    def get_result(self):
+        highest_type = self._get_highest_type()
+        self._check_casts(highest_type)
+        return highest_type
+
+    def _count_types(self):
+        for expr in self.exprs:
+            self.type_counts[expr.type()] += 1
+
+    def _get_highest_type(self):
+        scores = []
+        for k, v in self.type_counts.items():
+            if not v:
+                continue
+            score = self._precedence[k.name()]
+
+            scores.append((score, k))
+
+        scores.sort()
+        return scores[-1][1]
+
+    def _check_casts(self, typename):
+        for expr in self.exprs:
+            if not expr._can_cast_implicit(typename):
+                raise ValueError('Expression with type {0} cannot be '
+                                 'implicitly casted to {1}'
+                                 .format(expr.type(), typename))
+
+
+def _int_bounds_promotion(ltype, rtype, op):
+    lmin, lmax = ltype.bounds
+    rmin, rmax = rtype.bounds
+
+    values = [op(lmin, rmin), op(lmin, rmax),
+              op(lmax, rmin), op(lmax, rmax)]
+
+    return _smallest_int_containing(values, allow_overflow=True)
+
+
+def _int_one_literal_promotion(atype, lit_val, op):
+    amin, amax = atype.bounds
+    bound_type = _smallest_int_containing([op(amin, lit_val),
+                                           op(amax, lit_val)],
+                                          allow_overflow=True)
+    # In some cases, the bounding type might be int8, even though neither of
+    # the types are that small. We want to ensure the containing type is _at
+    # least_ as large as the smallest type in the expression
+    return _largest_int([bound_type, atype])
+
+
+def _smallest_int_containing(values, allow_overflow=False):
+    containing_types = [int_literal_class(x, allow_overflow=allow_overflow)
+                        for x in values]
+    return _largest_int(containing_types)
+
+
+def int_literal_class(value, allow_overflow=False):
+    if -128 <= value <= 127:
+        t = 'int8'
+    elif -32768 <= value <= 32767:
+        t = 'int16'
+    elif -2147483648 <= value <= 2147483647:
+        t = 'int32'
+    else:
+        if value < -9223372036854775808 or value > 9223372036854775807:
+            if not allow_overflow:
+                raise OverflowError(value)
+        t = 'int64'
+    return dt.validate_type(t)
+
+
+def _largest_int(int_types):
+    nbytes = max(t._nbytes for t in int_types)
+    return dt.validate_type('int%d' % (8 * nbytes))
+
+
+class ImplicitCast(object):
+
+    def __init__(self, value_type, implicit_targets):
+        self.value_type = value_type
+        self.implicit_targets = implicit_targets
+
+    def can_cast(self, target):
+        base_type = target.name()
+        return (base_type in self.implicit_targets or
+                target == self.value_type)
+
+
+# ----------------------------------------------------------------------
+# Input / output type rules and validation
+
+
+def shape_like(arg, out_type):
+    out_type = dt.validate_type(out_type)
+    if isinstance(arg, ir.ScalarExpr):
+        return out_type.scalar_type()
+    else:
+        return out_type.array_type()
+
+
+def shape_like_args(args, out_type):
+    out_type = dt.validate_type(out_type)
+    if util.any_of(args, ir.ArrayExpr):
+        return out_type.array_type()
+    else:
+        return out_type.scalar_type()
+
+
+def is_table(e):
+    return isinstance(e, ir.TableExpr)
+
+
+def is_array(e):
+    return isinstance(e, ir.ArrayExpr)
+
+
+def is_scalar(e):
+    return isinstance(e, ir.ScalarExpr)
+
+
+def is_collection(expr):
+    return isinstance(expr, (ir.ArrayExpr, ir.TableExpr))
+
+
+class Argument(object):
+
+    """
+
+    """
+
+    def __init__(self, name=None, default=None, optional=False,
+                 validator=None):
+        self.name = name
+        self.default = default
+        self.optional = optional
+
+        self.validator = validator
+
+    def validate(self, args, i):
+        arg = args[i]
+
+        if self.validator is not None:
+            arg = args[i] = self.validator(arg)
+
+        if arg is None:
+            if not self.optional:
+                return ir.as_value_expr(self.default)
+            elif self.optional:
+                return arg
+
+        return self._validate(args, i)
+
+    def _validate(self, args, i):
+        raise NotImplementedError
+
+
+def _to_argument(val):
+    if isinstance(val, dt.DataType):
+        val = value_typed_as(val)
+    elif not isinstance(val, Argument):
+        val = val()
+    return val
+
+
+class TypeSignature(object):
+
+    def __init__(self, type_specs):
+        types = []
+
+        for val in type_specs:
+            val = _to_argument(val)
+            types.append(val)
+
+        self.types = types
+
+    def __repr__(self):
+        types = '\n    '.join('arg {0}: {1}'.format(i, repr(x))
+                              for i, x in enumerate(self.types))
+        return '{0}\n    {1}'.format(type(self), types)
+
+    def validate(self, args):
+        n, k = len(args), len(self.types)
+        k_required = len([x for x in self.types if not x.optional])
+        if k != k_required:
+            if n < k_required:
+                raise com.IbisError('Expected at least {0} args, got {1}'
+                                    .format(k, k_required))
+        elif n != k:
+            raise com.IbisError('Expected {0} args, got {1}'.format(k, n))
+
+        if n < k:
+            args = list(args) + [t.default for t in self.types[n:]]
+
+        return self._validate(args, self.types)
+
+    def _validate(self, args, types):
+        clean_args = list(args)
+        for i, validator in enumerate(types):
+            try:
+                clean_args[i] = validator.validate(clean_args, i)
+            except IbisTypeError as e:
+                exc = e.args[0]
+                msg = ('Argument {0}: {1}'.format(i, exc) +
+                       '\nArgument was: {0}'.format(ir._safe_repr(args[i])))
+                raise IbisTypeError(msg)
+
+        return clean_args
+
+
+class VarArgs(TypeSignature):
+
+    def __init__(self, arg_type, min_length=1):
+        self.arg_type = _to_argument(arg_type)
+        self.min_length = min_length
+
+    def __repr__(self):
+        return '{0}\n    {1}'.format(type(self), repr(self.arg_type))
+
+    def validate(self, args):
+        n, k = len(args), self.min_length
+        if n < k:
+            raise com.IbisError('Expected at least {0} args, got {1}'
+                                .format(k, n))
+
+        return self._validate(args, [self.arg_type] * n)
+
+
+varargs = VarArgs
+
+
+def scalar_output(rule):
+    def f(self):
+        if isinstance(rule, dt.DataType):
+            t = rule
+        else:
+            t = dt.validate_type(rule(self))
+        return t.scalar_type()
+    return f
+
+
+def array_output(rule):
+    def f(self):
+        if isinstance(rule, dt.DataType):
+            t = rule
+        else:
+            t = dt.validate_type(rule(self))
+        return t.array_type()
+    return f
+
+
+def shape_like_flatargs(out_type):
+
+    def output_type(self):
+        flattened = []
+        for arg in self.args:
+            if isinstance(arg, (list, tuple)):
+                flattened.extend(arg)
+            else:
+                flattened.append(arg)
+        return shape_like_args(flattened, out_type)
+
+    return output_type
+
+
+def shape_like_arg(i, out_type):
+
+    def output_type(self):
+        return shape_like(self.args[i], out_type)
+
+    return output_type
+
+
+def numeric_highest_promote(i):
 
-    def visit_ExtractProtocol(self, op, *, arg):
-        return self.f.nullif(
-            self.f.as_varchar(self.f.get(self.f.parse_url(arg, 1), "scheme")), ""
-        )
-
-    def visit_ExtractAuthority(self, op, *, arg):
-        return self.f.concat_ws(
-            ":",
-            self.f.as_varchar(self.f.get(self.f.parse_url(arg, 1), "host")),
-            self.f.as_varchar(self.f.get(self.f.parse_url(arg, 1), "port")),
-        )
-
-    def visit_ExtractFile(self, op, *, arg):
-        return self.f.concat_ws(
-            "?",
-            self.visit_ExtractPath(op, arg=arg),
-            self.visit_ExtractQuery(op, arg=arg, key=None),
-        )
-
-    def visit_ExtractPath(self, op, *, arg):
-        return self.f.concat(
-            "/", self.f.as_varchar(self.f.get(self.f.parse_url(arg, 1), "path"))
-        )
-
-    def visit_ExtractFragment(self, op, *, arg):
-        return self.f.nullif(
-            self.f.as_varchar(self.f.get(self.f.parse_url(arg, 1), "fragment")), ""
-        )
-
-    def visit_Unnest(self, op, *, arg):
-        sep = sge.convert(util.guid())
-        split = self.f.split(
-            self.f.array_to_string(self.f.nullif(arg, self.f.array()), sep), sep
-        )
-        expr = self.f.nullif(self.f.explode(split), "")
-        return self.cast(expr, op.dtype)
-
-    def visit_Quantile(self, op, *, arg, quantile, where):
-        # can't use `self.agg` here because `quantile` must be a constant and
-        # the agg method filters using `where` for every argument which turns
-        # the constant into an expression
-        if where is not None:
-            arg = self.if_(where, arg, NULL)
-
-        # The Snowflake SQLGlot dialect rewrites calls to `percentile_cont` to
-        # include     WITHIN GROUP (ORDER BY ...)
-        # as per https://docs.snowflake.com/en/sql-reference/functions/percentile_cont
-        # using the rule `add_within_group_for_percentiles`
-        #
-        # If we have copy=False set in our call to `compile`, if there is more
-        # than one quantile, the rewrite rule fails on the second pass because
-        # of some mutation in the first pass. To avoid this error, we create the
-        # expression with the within group included already and skip the (now
-        # unneeded) rewrite rule.
-        order_by = sge.Order(expressions=[sge.Ordered(this=arg)])
-        quantile = self.f.percentile_cont(quantile)
-        return sge.WithinGroup(this=quantile, expression=order_by)
-
-    def visit_CountStar(self, op, *, arg, where):
-        if where is None:
-            return super().visit_CountStar(op, arg=arg, where=where)
-        return self.f.count_if(where)
-
-    def visit_CountDistinct(self, op, *, arg, where):
-        if where is not None:
-            arg = self.if_(where, arg, NULL)
-        return self.f.count(sge.Distinct(expressions=[arg]))
-
-    def visit_CountDistinctStar(self, op, *, arg, where):
-        columns = op.arg.schema.names
-        quoted = self.quoted
-        col = partial(sg.column, quoted=quoted)
-        if where is None:
-            expressions = list(map(col, columns))
+    def output_type(self):
+        arg = self.args[i]
+
+        if isinstance(arg, ir.DecimalValue):
+            return arg._factory
+        elif isinstance(arg, ir.FloatingValue):
+            # Impala upcasts float to double in this op
+            return shape_like(arg, 'double')
+        elif isinstance(arg, ir.IntegerValue):
+            return shape_like(arg, 'int64')
         else:
-            # any null columns will cause the entire row not to be counted
-            expressions = [self.if_(where, col(name), NULL) for name in columns]
-        return self.f.count(sge.Distinct(expressions=expressions))
-
-    def visit_Xor(self, op, *, left, right):
-        # boolxor accepts numerics ... and returns a boolean? wtf?
-        return self.f.boolxor(self.cast(left, dt.int8), self.cast(right, dt.int8))
-
-    def visit_WindowFunction(self, op, *, how, func, start, end, group_by, order_by):
-        if start is None:
-            start = {}
-        if end is None:
-            end = {}
-
-        start_value = start.get("value", "UNBOUNDED")
-        start_side = start.get("side", "PRECEDING")
-        end_value = end.get("value", "UNBOUNDED")
-        end_side = end.get("side", "FOLLOWING")
-
-        if getattr(start_value, "this", None) == "0":
-            start_value = "CURRENT ROW"
-            start_side = None
-
-        if getattr(end_value, "this", None) == "0":
-            end_value = "CURRENT ROW"
-            end_side = None
-
-        spec = sge.WindowSpec(
-            kind=how.upper(),
-            start=start_value,
-            start_side=start_side,
-            end=end_value,
-            end_side=end_side,
-            over="OVER",
-        )
-        order = sge.Order(expressions=order_by) if order_by else None
-
-        orig_spec = spec
-        spec = self._minimize_spec(op.start, op.end, orig_spec)
-
-        # due to https://docs.snowflake.com/en/sql-reference/functions-analytic#window-frame-usage-notes
-        # we need to make the default window rows (since range isn't supported)
-        # and we need to make the default frame unbounded preceding to current
-        # row
-        if spec is None and isinstance(op.func, (ops.First, ops.Last, ops.NthValue)):
-            spec = orig_spec
-            spec.args["kind"] = "ROWS"
-
-        return sge.Window(this=func, partition_by=group_by, order=order, spec=spec)
-
-    def visit_WindowBoundary(self, op, *, value, preceding):
-        if not isinstance(op.value, ops.Literal):
-            raise com.OperationNotDefinedError(
-                "Expressions in window bounds are not supported by Snowflake"
-            )
-        return super().visit_WindowBoundary(op, value=value, preceding=preceding)
-
-    def visit_Correlation(self, op, *, left, right, how, where):
-        if how == "sample":
-            raise com.UnsupportedOperationError(
-                f"{self.dialect} only implements `pop` correlation coefficient"
-            )
-
-        # TODO: rewrite rule?
-        if (left_type := op.left.dtype).is_boolean():
-            left = self.cast(left, dt.Int32(nullable=left_type.nullable))
-
-        if (right_type := op.right.dtype).is_boolean():
-            right = self.cast(right, dt.Int32(nullable=right_type.nullable))
-
-        return self.agg.corr(left, right, where=where)
-
-    def visit_TimestampRange(self, op, *, start, stop, step):
-        raw_step = op.step
-
-        if not isinstance(raw_step, ops.Literal):
-            raise com.UnsupportedOperationError("`step` argument must be a literal")
-
-        unit = raw_step.dtype.unit.name.lower()
-        step = raw_step.value
-
-        value_type = op.dtype.value_type
-
-        if step == 0:
-            return self.f.array()
-
-        return (
-            sg.select(
-                self.f.array_agg(
-                    self.f.replace(
-                        # conversion to varchar is necessary to control
-                        # the timestamp format
-                        #
-                        # otherwise, since timestamps in arrays become strings
-                        # anyway due to lack of parameterized type support in
-                        # Snowflake the format depends on a session parameter
-                        self.f.to_varchar(
-                            self.f.dateadd(unit, C.value, start, dialect=self.dialect),
-                            'YYYY-MM-DD"T"HH24:MI:SS.FF6'
-                            + (value_type.timezone is not None) * "TZH:TZM",
-                        ),
-                        # timezones are always hour:minute offsets from UTC, not
-                        # named, so replacing "Z" shouldn't be an issue
-                        "Z",
-                        "+00:00",
-                    ),
-                )
-            )
-            .from_(
-                sge.Table(
-                    this=sge.Unnest(
-                        expressions=[
-                            self.f.array_generate_range(
-                                0,
-                                self.f.datediff(
-                                    unit, start, stop, dialect=self.dialect
-                                ),
-                                step,
-                            )
-                        ]
-                    )
-                )
-            )
-            .subquery()
-        )
+            raise NotImplementedError
+
+    return output_type
+
+
+def type_of_arg(i):
+
+    def output_type(self):
+        return self.args[i]._factory
+
+    return output_type
+
+
+def signature(types):
+    if isinstance(types, TypeSignature):
+        return types
+
+    return TypeSignature(types)
+
+
+class ValueArgument(Argument):
+
+    def _validate(self, args, i):
+        arg = args[i]
+        if not isinstance(arg, ir.Expr):
+            arg = args[i] = ir.as_value_expr(arg)
+
+        return arg
+
+
+class AnyTyped(Argument):
+
+    def __init__(self, types, fail_message, **arg_kwds):
+        self.types = util.promote_list(types)
+        self.fail_message = fail_message
+        Argument.__init__(self, **arg_kwds)
+
+    def _validate(self, args, i):
+        arg = args[i]
+
+        if not self._type_matches(arg):
+            if isinstance(self.fail_message, py_string):
+                exc = self.fail_message
+            else:
+                exc = self.fail_message(self.types, arg)
+            raise IbisTypeError(exc)
+
+        return arg
+
+    def _type_matches(self, arg):
+        for t in self.types:
+            if (isinstance(t, dt.DataType) or
+                    isinstance(t, type) and issubclass(t, dt.DataType)):
+                if t.can_implicit_cast(arg.type()):
+                    return True
+            else:
+                if isinstance(arg, t):
+                    return True
+        return False
+
+
+class ValueTyped(AnyTyped, ValueArgument):
+
+    def __repr__(self):
+        return 'ValueTyped({0})'.format(repr(self.types))
+
+    def _validate(self, args, i):
+        ValueArgument._validate(self, args, i)
+        return AnyTyped._validate(self, args, i)
+
+
+class MultipleTypes(Argument):
+
+    def __init__(self, types, **arg_kwds):
+        self.types = [_to_argument(t) for t in types]
+        Argument.__init__(self, **arg_kwds)
+
+    def _validate(self, args, i):
+        for t in self.types:
+            arg = t.validate(args, i)
+        return arg
+
+
+class OneOf(Argument):
+
+    def __init__(self, types, **arg_kwds):
+        self.types = [_to_argument(t) for t in types]
+        Argument.__init__(self, **arg_kwds)
+
+    def _validate(self, args, i):
+        validated = False
+        for t in self.types:
+            try:
+                arg = t.validate(args, i)
+                validated = True
+            except:
+                pass
+            else:
+                break
+
+        if not validated:
+            raise IbisTypeError('No type options validated')
+
+        return arg
+
+
+class CastIfDecimal(ValueArgument):
+
+    def __init__(self, ref_j, **arg_kwds):
+        self.ref_j = ref_j
+        ValueArgument.__init__(self, **arg_kwds)
+
+    def _validate(self, args, i):
+        ValueArgument._validate(self, args, i)
+
+        ref_arg = args[self.ref_j]
+        if isinstance(ref_arg, ir.DecimalValue):
+            return args[i].cast(ref_arg.type())
+
+        return args[i]
+
+
+cast_if_decimal = CastIfDecimal
+
+
+def value_typed_as(types, **arg_kwds):
+    fail_message = 'Arg was not in types {0}'.format(repr(types))
+    return ValueTyped(types, fail_message, **arg_kwds)
+
+
+def array(value_type=None, name=None, optional=False):
+    array_checker = ValueTyped(ir.ArrayExpr, 'not an array expr',
+                               name=name,
+                               optional=optional)
+    if value_type is None:
+        return array_checker
+    else:
+        return MultipleTypes([array_checker, value_type],
+                             name=name,
+                             optional=optional)
+
+
+def scalar(name=None, optional=False):
+    return ValueTyped(ir.ScalarExpr, 'not a scalar expr', name=name,
+                      optional=optional)
+
+
+def collection(name=None, optional=False):
+    return ValueTyped((ir.ArrayExpr, ir.TableExpr), 'not a collection',
+                      name=name, optional=optional)
+
+
+def value(name=None, optional=False):
+    return ValueTyped(ir.ValueExpr, 'not a value expr',
+                      name=name, optional=optional)
+
+
+def table(name=None):
+    pass
+
+
+class Number(ValueTyped):
+
+    def __init__(self, allow_boolean=True, **arg_kwds):
+        self.allow_boolean = allow_boolean
+        ValueTyped.__init__(self, ir.NumericValue, 'not numeric', **arg_kwds)
+
+    def _validate(self, args, i):
+        arg = ValueTyped._validate(self, args, i)
+
+        if isinstance(arg, ir.BooleanValue) and not self.allow_boolean:
+            raise IbisTypeError('not implemented for boolean values')
+
+        return arg
+
+
+number = Number
+
+
+def integer(**arg_kwds):
+    return ValueTyped(dt.int_, 'not integer', **arg_kwds)
+
+
+def double(**arg_kwds):
+    return ValueTyped(dt.double, 'not double', **arg_kwds)
+
+
+def decimal(**arg_kwds):
+    return ValueTyped(dt.Decimal, 'not decimal', **arg_kwds)
+
+
+def timestamp(**arg_kwds):
+    return ValueTyped(ir.TimestampValue, 'not decimal', **arg_kwds)
+
+
+def timedelta(**arg_kwds):
+    from ibis.expr.temporal import Timedelta
+    return AnyTyped(Timedelta, 'not a timedelta', **arg_kwds)
+
+
+def string(**arg_kwds):
+    return ValueTyped(dt.string, 'not string', **arg_kwds)
+
+
+def boolean(**arg_kwds):
+    return ValueTyped(dt.boolean, 'not string', **arg_kwds)
+
+
+def one_of(args, **arg_kwds):
+    return OneOf(args, **arg_kwds)
+
+
+class StringOptions(Argument):
+
+    def __init__(self, options, **arg_kwds):
+        self.options = options
+        Argument.__init__(self, **arg_kwds)
+
+    def _validate(self, args, i):
+        arg = args[i]
+        if arg not in self.options:
+            raise IbisTypeError('{0} not among options {1}'
+                                .format(arg, repr(self.options)))
+        return arg
+
+
+string_options = StringOptions
+
+
+class ListOf(Argument):
+
+    def __init__(self, value_type, min_length=0, **arg_kwds):
+        self.value_type = _to_argument(value_type)
+        self.min_length = min_length
+        Argument.__init__(self, **arg_kwds)
+
+    def _validate(self, args, i):
+        arg = args[i]
+        if isinstance(arg, tuple):
+            arg = args[i] = list(arg)
+
+        if not isinstance(arg, list):
+            raise IbisTypeError('not a list')
+
+        if len(arg) < self.min_length:
+            raise IbisTypeError('list must have at least {} elements'
+                                .format(self.min_length))
+
+        checked_args = []
+        for j in range(len(arg)):
+            try:
+                checked_arg = self.value_type.validate(arg, j)
+            except IbisTypeError as e:
+                exc = e.args[0]
+                msg = ('List element {0} had a type error: {1}'
+                       .format(j, exc))
+                raise IbisTypeError(msg)
+            checked_args.append(checked_arg)
+
+        args[i] = checked_args
+
+        return checked_args
+
+
+list_of = ListOf
+
+
+class DataTypeArgument(Argument):
+
+    def _validate(self, args, i):
+        arg = args[i]
+
+        if isinstance(arg, py_string):
+            arg = arg.lower()
+
+        arg = args[i] = dt.validate_type(arg)
+        return arg
+
+
+data_type = DataTypeArgument
```

### Comparing `ibis_framework-9.0.0.dev686/ibis/expr/datatypes/core.py` & `ibis-framework-v0.6.0/ibis/expr/types.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,1103 +1,1146 @@
-from __future__ import annotations
+# Copyright 2014 Cloudera Inc.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+# http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import datetime
+import six
+
+from ibis.common import IbisError, RelationError
+import ibis.common as com
+import ibis.compat as compat
+import ibis.config as config
+import ibis.util as util
 
-import datetime as pydatetime
-import decimal as pydecimal
-import numbers
-import uuid as pyuuid
-from abc import abstractmethod
-from collections.abc import Iterable, Iterator, Mapping, Sequence
-from numbers import Integral, Real
-from typing import Any, Generic, Literal, NamedTuple, Optional, TypeVar, get_type_hints
-
-import toolz
-from public import public
-from typing_extensions import Self, get_args, get_origin
-
-from ibis.common.annotations import attribute
-from ibis.common.collections import FrozenDict, MapSet
-from ibis.common.dispatch import lazy_singledispatch
-from ibis.common.grounds import Concrete, Singleton
-from ibis.common.patterns import Coercible, CoercionError
-from ibis.common.temporal import IntervalUnit, TimestampUnit
-
-
-@lazy_singledispatch
-def dtype(value: Any, nullable: bool = True) -> DataType:
-    """Create a DataType object.
 
-    Parameters
-    ----------
-    value
-        The object to coerce to an Ibis DataType. Supported inputs include
-        strings, python type annotations, numpy dtypes, pandas dtypes, and
-        pyarrow types.
-    nullable
-        Whether the type should be nullable. Defaults to True.
-
-    Examples
-    --------
-    >>> import ibis
-    >>> ibis.dtype("int32")
-    Int32(nullable=True)
-    >>> ibis.dtype("array<float>")
-    Array(value_type=Float64(nullable=True), nullable=True)
-
-    DataType objects may also be created from Python types:
-
-    >>> ibis.dtype(int)
-    Int64(nullable=True)
-    >>> ibis.dtype(list[float])
-    Array(value_type=Float64(nullable=True), nullable=True)
-
-    Or other type systems, like numpy/pandas/pyarrow types:
-
-    >>> import pyarrow as pa
-    >>> ibis.dtype(pa.int32())
-    Int32(nullable=True)
+class Parameter(object):
 
     """
-    if isinstance(value, DataType):
-        return value
-    else:
-        return DataType.from_typehint(value)
+    Placeholder, to be implemented
+    """
 
+    pass
 
-@dtype.register(str)
-def from_string(value):
-    return DataType.from_string(value)
 
+# ---------------------------------------------------------------------
 
-@dtype.register("numpy.dtype")
-def from_numpy_dtype(value, nullable=True):
-    return DataType.from_numpy(value, nullable)
 
+class Expr(object):
 
-@dtype.register("pandas.core.dtypes.base.ExtensionDtype")
-def from_pandas_extension_dtype(value, nullable=True):
-    return DataType.from_pandas(value, nullable)
+    """
 
+    """
 
-@dtype.register("pyarrow.lib.DataType")
-def from_pyarrow(value, nullable=True):
-    return DataType.from_pyarrow(value, nullable)
+    def __init__(self, arg):
+        # TODO: all inputs must inherit from a common table API
+        self._arg = arg
+
+    def __repr__(self):
+        if config.options.interactive:
+            try:
+                result = self.execute()
+                return repr(result)
+            except com.TranslationError as e:
+                output = ('Translation to backend failed\n'
+                          'Error message: {0}\n'
+                          'Expression repr follows:\n{1}'
+                          .format(e.args[0], self._repr()))
+                return output
+        else:
+            return self._repr()
 
+    def _repr(self, memo=None):
+        from ibis.expr.format import ExprFormatter
+        return ExprFormatter(self).get_result()
 
-@dtype.register("polars.datatypes.classes.DataTypeClass")
-def from_polars(value, nullable=True):
-    return DataType.from_polars(value, nullable)
+    def pipe(self, f, *args, **kwargs):
+        """
+        Generic composition function to enable expression pipelining
 
+        >>> (expr
+             .pipe(f, *args, **kwargs)
+             .pipe(g, *args2, **kwargs2))
 
-# lock the dispatcher to prevent new types from being registered
-del dtype.register
+        is equivalent to
 
+        >>> g(f(expr, *args, **kwargs), *args2, **kwargs2)
 
-@public
-class DataType(Concrete, Coercible):
-    """Base class for all data types.
+        Parameters
+        ----------
+        f : function or (function, arg_name) tuple
+          If the expression needs to be passed as anything other than the first
+          argument to the function, pass a tuple with the argument name. For
+          example, (f, 'data') if the function f expects a 'data' keyword
+        args : positional arguments
+        kwargs : keyword arguments
+
+        Examples
+        --------
+        >>> def foo(data, a=None, b=None):
+                pass
+        >>> def bar(a, b, data=None):
+                pass
+        >>> expr.pipe(foo, a=5, b=10)
+        >>> expr.pipe((bar, 'data'), 1, 2)
 
-    Instances are immutable.
-    """
+        Returns
+        -------
+        result : result type of passed function
+        """
+        if isinstance(f, tuple):
+            f, data_keyword = f
+            kwargs = kwargs.copy()
+            kwargs[data_keyword] = self
+            return f(*args, **kwargs)
+        else:
+            return f(self, *args, **kwargs)
 
-    nullable: bool = True
+    __call__ = pipe
 
-    @property
-    @abstractmethod
-    def scalar(self): ...
+    def op(self):
+        return self._arg
 
     @property
-    @abstractmethod
-    def column(self): ...
+    def _factory(self):
+        def factory(arg, name=None):
+            return type(self)(arg, name=name)
+        return factory
 
-    # TODO(kszucs): remove it, prefer to use Annotable.__repr__ instead
-    @property
-    def _pretty_piece(self) -> str:
-        return ""
+    def _can_implicit_cast(self, arg):
+        return False
 
-    # TODO(kszucs): should remove it, only used internally
-    @property
-    def name(self) -> str:
-        """Return the name of the data type."""
-        return self.__class__.__name__
+    def execute(self, limit='default', async=False):
+        """
+        If this expression is based on physical tables in a database backend,
+        execute it against that backend.
 
-    @classmethod
-    def __coerce__(cls, value, **kwargs):
-        if isinstance(value, cls):
-            return value
+        Parameters
+        ----------
+        limit : integer or None, default 'default'
+          Pass an integer to effect a specific row limit. limit=None means "no
+          limit". The default is whatever is in ibis.options.
+
+        Returns
+        -------
+        result : expression-dependent
+          Result of compiling expression and executing in backend
+        """
+        from ibis.client import execute
+        return execute(self, limit=limit, async=async)
+
+    def compile(self, limit=None):
+        """
+        Compile expression to whatever execution target, to verify
+
+        Returns
+        -------
+        compiled : value or list
+           query representation or list thereof
+        """
+        from ibis.client import compile
+        return compile(self, limit=limit)
+
+    def verify(self):
+        """
+        Returns True if expression can be compiled to its attached client
+        """
         try:
-            return dtype(value)
-        except (TypeError, RuntimeError) as e:
-            raise CoercionError("Unable to coerce to a DataType") from e
-
-    def __call__(self, **kwargs):
-        return self.copy(**kwargs)
-
-    def __str__(self) -> str:
-        prefix = "!" * (not self.nullable)
-        return f"{prefix}{self.name.lower()}{self._pretty_piece}"
+            self.compile()
+            return True
+        except:
+            return False
 
     def equals(self, other):
-        if not isinstance(other, DataType):
-            raise TypeError(
-                f"invalid equality comparison between DataType and {type(other)}"
-            )
-        return self == other
-
-    def cast(self, other, **kwargs):
-        # TODO(kszucs): remove it or deprecate it?
-        from ibis.expr.datatypes.cast import cast
-
-        return cast(self, other, **kwargs)
-
-    def castable(self, to, **kwargs) -> bool:
-        """Check whether this type is castable to another."""
-        from ibis.expr.datatypes.cast import castable
+        if type(self) != type(other):
+            return False
+        return self._arg.equals(other._arg)
 
-        return castable(self, to, **kwargs)
+    def _can_compare(self, other):
+        return False
 
-    @classmethod
-    def from_string(cls, value) -> Self:
-        from ibis.expr.datatypes.parse import parse
+    def _root_tables(self):
+        return self.op().root_tables()
 
-        try:
-            return parse(value)
-        except SyntaxError:
-            raise TypeError(f"{value!r} cannot be parsed as a datatype")
+    def _get_unbound_tables(self):
+        # The expression graph may contain one or more tables of a particular
+        # known schema
+        pass
 
-    @classmethod
-    def from_typehint(cls, typ, nullable=True) -> Self:
-        origin_type = get_origin(typ)
 
-        if origin_type is None:
-            if isinstance(typ, type):
-                if issubclass(typ, Parametric):
-                    raise TypeError(
-                        f"Cannot construct a parametric {typ.__name__} datatype based "
-                        "on the type itself"
-                    )
-                elif issubclass(typ, DataType):
-                    return typ(nullable=nullable)
-                elif typ is type(None):
-                    return null
-                elif issubclass(typ, bool):
-                    return Boolean(nullable=nullable)
-                elif issubclass(typ, bytes):
-                    return Binary(nullable=nullable)
-                elif issubclass(typ, str):
-                    return String(nullable=nullable)
-                elif issubclass(typ, Integral):
-                    return Int64(nullable=nullable)
-                elif issubclass(typ, Real):
-                    return Float64(nullable=nullable)
-                elif issubclass(typ, pydecimal.Decimal):
-                    return Decimal(nullable=nullable)
-                elif issubclass(typ, pydatetime.datetime):
-                    return Timestamp(nullable=nullable)
-                elif issubclass(typ, pydatetime.date):
-                    return Date(nullable=nullable)
-                elif issubclass(typ, pydatetime.time):
-                    return Time(nullable=nullable)
-                elif issubclass(typ, pydatetime.timedelta):
-                    return Interval(unit="us", nullable=nullable)
-                elif issubclass(typ, pyuuid.UUID):
-                    return UUID(nullable=nullable)
-                elif annots := get_type_hints(typ):
-                    return Struct(toolz.valmap(dtype, annots), nullable=nullable)
-                else:
-                    raise TypeError(
-                        f"Cannot construct an ibis datatype from python type `{typ!r}`"
-                    )
+def _safe_repr(x, memo=None):
+    return x._repr(memo=memo) if isinstance(x, (Expr, Node)) else repr(x)
+
+
+class Node(object):
+
+    """
+    Node is the base class for all relational algebra and analytical
+    functionality. It transforms the input expressions into an output
+    expression.
+
+    Each node implementation is responsible for validating the inputs,
+    including any type promotion and / or casting issues, and producing a
+    well-typed expression
+
+    Note that Node is deliberately not made an expression subclass: think
+    of Node as merely a typed expression builder.
+    """
+
+    def __init__(self, args):
+        self.args = args
+
+    def __repr__(self):
+        return self._repr()
+
+    def _repr(self, memo=None):
+        # Quick and dirty to get us started
+        opname = type(self).__name__
+        pprint_args = []
+
+        memo = memo or {}
+
+        if id(self) in memo:
+            return memo[id(self)]
+
+        def _pp(x):
+            if isinstance(x, Expr):
+                key = id(x.op())
             else:
-                raise TypeError(
-                    f"Cannot construct an ibis datatype from python value `{typ!r}`"
-                )
-        elif issubclass(origin_type, (Sequence, Array)):
-            (value_type,) = map(dtype, get_args(typ))
-            return Array(value_type)
-        elif issubclass(origin_type, (Mapping, Map)):
-            key_type, value_type = map(dtype, get_args(typ))
-            return Map(key_type, value_type)
-        else:
-            raise TypeError(f"Value {typ!r} is not a valid datatype")
+                key = id(x)
 
-    @classmethod
-    def from_numpy(cls, numpy_type, nullable=True) -> Self:
-        """Return the equivalent ibis datatype."""
-        from ibis.formats.numpy import NumpyType
+            if key in memo:
+                return memo[key]
+            result = _safe_repr(x, memo=memo)
+            memo[key] = result
+            return result
+
+        for x in self.args:
+            if isinstance(x, (tuple, list)):
+                pp = repr([_pp(y) for y in x])
+            else:
+                pp = _pp(x)
+            pprint_args.append(pp)
 
-        return NumpyType.to_ibis(numpy_type, nullable=nullable)
+        return '%s(%s)' % (opname, ', '.join(pprint_args))
 
-    @classmethod
-    def from_pandas(cls, pandas_type, nullable=True) -> Self:
-        """Return the equivalent ibis datatype."""
-        from ibis.formats.pandas import PandasType
+    def flat_args(self):
+        for arg in self.args:
+            if isinstance(arg, (tuple, list)):
+                for x in arg:
+                    yield x
+            else:
+                yield arg
 
-        return PandasType.to_ibis(pandas_type, nullable=nullable)
+    def equals(self, other):
+        if type(self) != type(other):
+            return False
 
-    @classmethod
-    def from_pyarrow(cls, arrow_type, nullable=True) -> Self:
-        """Return the equivalent ibis datatype."""
-        from ibis.formats.pyarrow import PyArrowType
+        if len(self.args) != len(other.args):
+            return False
 
-        return PyArrowType.to_ibis(arrow_type, nullable=nullable)
+        for left, right in zip(self.args, other.args):
+            if not all_equal(left, right):
+                return False
+        return True
 
-    @classmethod
-    def from_polars(cls, polars_type, nullable=True) -> Self:
-        """Return the equivalent ibis datatype."""
-        from ibis.formats.polars import PolarsType
+    def is_ancestor(self, other):
+        if isinstance(other, Expr):
+            other = other.op()
 
-        return PolarsType.to_ibis(polars_type, nullable=nullable)
+        return self.equals(other)
 
-    def to_numpy(self):
-        """Return the equivalent numpy datatype."""
-        from ibis.formats.numpy import NumpyType
+    def to_expr(self):
+        klass = self.output_type()
+        return klass(self)
 
-        return NumpyType.from_ibis(self)
+    def output_type(self):
+        """
+        This function must resolve the output type of the expression and return
+        the node wrapped in the appropriate ValueExpr type.
+        """
+        raise NotImplementedError
 
-    def to_pandas(self):
-        """Return the equivalent pandas datatype."""
-        from ibis.formats.pandas import PandasType
 
-        return PandasType.from_ibis(self)
+def all_equal(left, right):
+    if isinstance(left, list):
+        if not isinstance(right, list):
+            return False
+        for a, b in zip(left, right):
+            if not all_equal(a, b):
+                return False
+        return True
 
-    def to_pyarrow(self):
-        """Return the equivalent pyarrow datatype."""
-        from ibis.formats.pyarrow import PyArrowType
+    if hasattr(left, 'equals'):
+        return left.equals(right)
+    else:
+        return left == right
+    return True
 
-        return PyArrowType.from_ibis(self)
 
-    def to_polars(self):
-        """Return the equivalent polars datatype."""
-        from ibis.formats.polars import PolarsType
+class ValueNode(Node):
 
-        return PolarsType.from_ibis(self)
+    def __init__(self, *args):
+        args = self._validate_args(args)
+        Node.__init__(self, args)
 
-    def is_array(self) -> bool:
-        """Return True if an instance of an Array type."""
-        return isinstance(self, Array)
+    def _validate_args(self, args):
+        if not hasattr(self, 'input_type'):
+            return args
 
-    def is_binary(self) -> bool:
-        """Return True if an instance of a Binary type."""
-        return isinstance(self, Binary)
+        return self.input_type.validate(args)
 
-    def is_boolean(self) -> bool:
-        """Return True if an instance of a Boolean type."""
-        return isinstance(self, Boolean)
+    def root_tables(self):
+        exprs = [arg for arg in self.args if isinstance(arg, Expr)]
+        return distinct_roots(*exprs)
 
-    def is_date(self) -> bool:
-        """Return True if an instance of a Date type."""
-        return isinstance(self, Date)
+    def resolve_name(self):
+        raise com.ExpressionError('Expression is not named: %s' % repr(self))
 
-    def is_decimal(self) -> bool:
-        """Return True if an instance of a Decimal type."""
-        return isinstance(self, Decimal)
 
-    def is_enum(self) -> bool:
-        """Return True if an instance of an Enum type."""
-        return isinstance(self, Enum)
+class TableColumn(ValueNode):
 
-    def is_float16(self) -> bool:
-        """Return True if an instance of a Float16 type."""
-        return isinstance(self, Float16)
+    """
+    Selects a column from a TableExpr
+    """
 
-    def is_float32(self) -> bool:
-        """Return True if an instance of a Float32 type."""
-        return isinstance(self, Float32)
+    def __init__(self, name, table_expr):
+        Node.__init__(self, [name, table_expr])
 
-    def is_float64(self) -> bool:
-        """Return True if an instance of a Float64 type."""
-        return isinstance(self, Float64)
+        if name not in table_expr.schema():
+            raise KeyError("'{0}' is not a field".format(name))
 
-    def is_floating(self) -> bool:
-        """Return True if an instance of any Floating type."""
-        return isinstance(self, Floating)
+        self.name = name
+        self.table = table_expr
 
-    def is_geospatial(self) -> bool:
-        """Return True if an instance of a Geospatial type."""
-        return isinstance(self, GeoSpatial)
+    def parent(self):
+        return self.table
 
-    def is_inet(self) -> bool:
-        """Return True if an instance of an Inet type."""
-        return isinstance(self, INET)
+    def resolve_name(self):
+        return self.name
 
-    def is_int16(self) -> bool:
-        """Return True if an instance of an Int16 type."""
-        return isinstance(self, Int16)
+    def root_tables(self):
+        return self.table._root_tables()
 
-    def is_int32(self) -> bool:
-        """Return True if an instance of an Int32 type."""
-        return isinstance(self, Int32)
+    def to_expr(self):
+        ctype = self.table._get_type(self.name)
+        klass = ctype.array_type()
+        return klass(self, name=self.name)
 
-    def is_int64(self) -> bool:
-        """Return True if an instance of an Int64 type."""
-        return isinstance(self, Int64)
 
-    def is_int8(self) -> bool:
-        """Return True if an instance of an Int8 type."""
-        return isinstance(self, Int8)
+class ExpressionList(Node):
 
-    def is_integer(self) -> bool:
-        """Return True if an instance of any Integer type."""
-        return isinstance(self, Integer)
+    def __init__(self, exprs):
+        exprs = [as_value_expr(x) for x in exprs]
+        Node.__init__(self, exprs)
 
-    def is_interval(self) -> bool:
-        """Return True if an instance of an Interval type."""
-        return isinstance(self, Interval)
+    def root_tables(self):
+        return distinct_roots(*self.args)
 
-    def is_json(self) -> bool:
-        """Return True if an instance of a JSON type."""
-        return isinstance(self, JSON)
+    def output_type(self):
+        return ExprList
 
-    def is_linestring(self) -> bool:
-        """Return True if an instance of a LineString type."""
-        return isinstance(self, LineString)
 
-    def is_macaddr(self) -> bool:
-        """Return True if an instance of a MACADDR type."""
-        return isinstance(self, MACADDR)
+class ExprList(Expr):
 
-    def is_map(self) -> bool:
-        """Return True if an instance of a Map type."""
-        return isinstance(self, Map)
+    def exprs(self):
+        return self.op().args
 
-    def is_multilinestring(self) -> bool:
-        """Return True if an instance of a MultiLineString type."""
-        return isinstance(self, MultiLineString)
+    def names(self):
+        return [x.get_name() for x in self.exprs()]
 
-    def is_multipoint(self) -> bool:
-        """Return True if an instance of a MultiPoint type."""
-        return isinstance(self, MultiPoint)
+    def rename(self, f):
+        new_exprs = [x.name(f(x.get_name())) for x in self.exprs()]
+        return ExpressionList(new_exprs).to_expr()
 
-    def is_multipolygon(self) -> bool:
-        """Return True if an instance of a MultiPolygon type."""
-        return isinstance(self, MultiPolygon)
+    def prefix(self, value):
+        return self.rename(lambda x: value + x)
 
-    def is_nested(self) -> bool:
-        """Return true if an instance of any nested (Array/Map/Struct) type."""
-        return isinstance(self, (Array, Map, Struct))
+    def suffix(self, value):
+        return self.rename(lambda x: x + value)
 
-    def is_null(self) -> bool:
-        """Return true if an instance of a Null type."""
-        return isinstance(self, Null)
+    def concat(self, *others):
+        """
+        Concatenate expression lists
 
-    def is_numeric(self) -> bool:
-        """Return true if an instance of a Numeric type."""
-        return isinstance(self, Numeric)
+        Returns
+        -------
+        combined : ExprList
+        """
+        exprs = list(self.exprs())
+        for o in others:
+            if not isinstance(o, ExprList):
+                raise TypeError(o)
+            exprs.extend(o.exprs())
+        return ExpressionList(exprs).to_expr()
 
-    def is_point(self) -> bool:
-        """Return true if an instance of a Point type."""
-        return isinstance(self, Point)
 
-    def is_polygon(self) -> bool:
-        """Return true if an instance of a Polygon type."""
-        return isinstance(self, Polygon)
+class Literal(ValueNode):
 
-    def is_primitive(self) -> bool:
-        """Return true if an instance of a Primitive type."""
-        return isinstance(self, Primitive)
+    def __init__(self, value):
+        self.value = value
 
-    def is_signed_integer(self) -> bool:
-        """Return true if an instance of a SignedInteger type."""
-        return isinstance(self, SignedInteger)
+    def __repr__(self):
+        return 'Literal(%s)' % repr(self.value)
+
+    @property
+    def args(self):
+        return [self.value]
 
-    def is_string(self) -> bool:
-        """Return true if an instance of a String type."""
-        return isinstance(self, String)
+    def equals(self, other):
+        if not isinstance(other, Literal):
+            return False
+        return (isinstance(other.value, type(self.value)) and
+                self.value == other.value)
+
+    def output_type(self):
+        import ibis.expr.rules as rules
+        if isinstance(self.value, bool):
+            klass = BooleanScalar
+        elif isinstance(self.value, compat.integer_types):
+            int_type = rules.int_literal_class(self.value)
+            klass = int_type.scalar_type()
+        elif isinstance(self.value, float):
+            klass = DoubleScalar
+        elif isinstance(self.value, six.string_types):
+            klass = StringScalar
+        elif isinstance(self.value, datetime.datetime):
+            klass = TimestampScalar
+        else:
+            raise com.InputTypeError(self.value)
 
-    def is_struct(self) -> bool:
-        """Return true if an instance of a Struct type."""
-        return isinstance(self, Struct)
+        return klass
 
-    def is_temporal(self) -> bool:
-        """Return true if an instance of a Temporal type."""
-        return isinstance(self, Temporal)
+    def root_tables(self):
+        return []
 
-    def is_time(self) -> bool:
-        """Return true if an instance of a Time type."""
-        return isinstance(self, Time)
 
-    def is_timestamp(self) -> bool:
-        """Return true if an instance of a Timestamp type."""
-        return isinstance(self, Timestamp)
+class TableNode(Node):
 
-    def is_uint16(self) -> bool:
-        """Return true if an instance of a UInt16 type."""
-        return isinstance(self, UInt16)
+    def get_type(self, name):
+        return self.get_schema().get_type(name)
 
-    def is_uint32(self) -> bool:
-        """Return true if an instance of a UInt32 type."""
-        return isinstance(self, UInt32)
+    def to_expr(self):
+        return TableExpr(self)
 
-    def is_uint64(self) -> bool:
-        """Return true if an instance of a UInt64 type."""
-        return isinstance(self, UInt64)
 
-    def is_uint8(self) -> bool:
-        """Return true if an instance of a UInt8 type."""
-        return isinstance(self, UInt8)
+class BlockingTableNode(TableNode):
+    # Try to represent the fact that whatever lies here is a semantically
+    # distinct table. Like projections, aggregations, and so forth
+    pass
 
-    def is_unknown(self) -> bool:
-        """Return true if an instance of an Unknown type."""
-        return isinstance(self, Unknown)
 
-    def is_unsigned_integer(self) -> bool:
-        """Return true if an instance of an UnsignedInteger type."""
-        return isinstance(self, UnsignedInteger)
+def distinct_roots(*args):
+    all_roots = []
+    for arg in args:
+        all_roots.extend(arg._root_tables())
+    return util.unique_by_key(all_roots, id)
 
-    def is_uuid(self) -> bool:
-        """Return true if an instance of a UUID type."""
-        return isinstance(self, UUID)
 
-    def is_variadic(self) -> bool:
-        """Return true if an instance of a Variadic type."""
-        return isinstance(self, Variadic)
+# ---------------------------------------------------------------------
+# Helper / factory functions
 
 
-@public
-class Unknown(DataType, Singleton):
-    """An unknown type."""
+class ValueExpr(Expr):
 
-    scalar = "UnknownScalar"
-    column = "UnknownColumn"
+    """
+    Base class for a data generating expression having a fixed and known type,
+    either a single value (scalar)
+    """
 
+    _implicit_casts = set()
 
-@public
-class Primitive(DataType, Singleton):
-    """Values with known size."""
+    def __init__(self, arg, name=None):
+        Expr.__init__(self, arg)
+        self._name = name
 
+    def equals(self, other):
+        if not isinstance(other, ValueExpr):
+            return False
 
-# TODO(kszucs): consider to remove since we don't actually use this information
-@public
-class Variadic(DataType):
-    """Values with unknown size."""
+        if self._name != other._name:
+            return False
 
+        return Expr.equals(self, other)
 
-@public
-class Parametric(DataType):
-    """Types that can be parameterized."""
+    def type(self):
+        import ibis.expr.datatypes as dt
+        return dt._primitive_types[self._typename]
 
+    def _base_type(self):
+        # Parametric types like "decimal"
+        return self.type()
 
-@public
-class Null(Primitive):
-    """Null values."""
+    def _can_cast_implicit(self, typename):
+        from ibis.expr.rules import ImplicitCast
+        rule = ImplicitCast(self.type(), self._implicit_casts)
+        return rule.can_cast(typename)
 
-    scalar = "NullScalar"
-    column = "NullColumn"
+    def get_name(self):
+        if self._name is not None:
+            # This value has been explicitly named
+            return self._name
 
+        # In some but not all cases we can get a name from the node that
+        # produces the value
+        return self.op().resolve_name()
 
-@public
-class Boolean(Primitive):
-    """[](`True`) or [](`False`) values."""
+    def name(self, name):
+        return self._factory(self._arg, name=name)
 
-    scalar = "BooleanScalar"
-    column = "BooleanColumn"
 
+class ScalarExpr(ValueExpr):
 
-@public
-class Bounds(NamedTuple):
-    """The lower and upper bound of a fixed-size value."""
+    pass
 
-    lower: int
-    upper: int
 
-    def __contains__(self, value: int) -> bool:
-        return self.lower <= value <= self.upper
+class ArrayExpr(ValueExpr):
 
+    def parent(self):
+        return self._arg
 
-@public
-class Numeric(DataType):
-    """Numeric types."""
+    def to_projection(self):
+        """
+        Promote this column expression to a table projection
+        """
+        roots = self._root_tables()
+        if len(roots) > 1:
+            raise RelationError('Cannot convert array expression involving '
+                                'multiple base table references to a '
+                                'projection')
 
+        table = TableExpr(roots[0])
+        return table.projection([self])
 
-@public
-class Integer(Primitive, Numeric):
-    """Integer values."""
 
-    scalar = "IntegerScalar"
-    column = "IntegerColumn"
+class AnalyticExpr(Expr):
 
     @property
-    @abstractmethod
-    def nbytes(self) -> int:
-        """Return the number of bytes used to store values of this type."""
+    def _factory(self):
+        def factory(arg):
+            return type(self)(arg)
+        return factory
 
+    def type(self):
+        return 'analytic'
 
-@public
-class String(Variadic, Singleton):
-    """A type representing a string.
 
-    Notes
-    -----
-    Because of differences in the way different backends handle strings, we
-    cannot assume that strings are UTF-8 encoded.
+class TableExpr(Expr):
 
-    """
+    @property
+    def _factory(self):
+        def factory(arg):
+            return TableExpr(arg)
+        return factory
+
+    def _assert_valid(self, exprs):
+        from ibis.expr.analysis import ExprValidator
+        ExprValidator([self]).validate_all(exprs)
+
+    def __contains__(self, name):
+        return name in self.schema()
+
+    def __getitem__(self, what):
+        if isinstance(what, six.string_types):
+            return self.get_column(what)
+
+        if isinstance(what, slice):
+            step = what.step
+            if step is not None and step != 1:
+                raise ValueError('Slice step can only be 1')
+            start = what.start or 0
+            stop = what.stop
+
+            if stop is None or stop < 0:
+                raise ValueError('End index must be a positive number')
+
+            if start < 0:
+                raise ValueError('Start index must be a positive number')
+
+            return self.limit(stop - start, offset=start)
+
+        what = bind_expr(self, what)
+
+        if isinstance(what, AnalyticExpr):
+            what = what._table_getitem()
+
+        if isinstance(what, (list, tuple, TableExpr)):
+            # Projection case
+            return self.projection(what)
+        elif isinstance(what, BooleanArray):
+            # Boolean predicate
+            return self.filter([what])
+        elif isinstance(what, ArrayExpr):
+            # Projection convenience
+            return self.projection(what)
+        else:
+            raise NotImplementedError
 
-    scalar = "StringScalar"
-    column = "StringColumn"
+    def __len__(self):
+        raise com.ExpressionError('Use .count() instead')
 
+    def __getattr__(self, key):
+        try:
+            return object.__getattribute__(self, key)
+        except AttributeError:
+            if not self._is_materialized() or key not in self.schema():
+                raise
+
+            return self.get_column(key)
+
+    def __dir__(self):
+        attrs = dir(type(self))
+        if self._is_materialized():
+            attrs = list(sorted(set(attrs + self.schema().names)))
+        return attrs
+
+    def _resolve(self, exprs):
+        exprs = util.promote_list(exprs)
+
+        # Stash this helper method here for now
+        out_exprs = []
+        for expr in exprs:
+            expr = self._ensure_expr(expr)
+            out_exprs.append(expr)
+        return out_exprs
+
+    def _ensure_expr(self, expr):
+        if isinstance(expr, six.string_types):
+            return self[expr]
+        elif not isinstance(expr, Expr):
+            return expr(self)
+        else:
+            return expr
 
-@public
-class Binary(Variadic, Singleton):
-    """A type representing a sequence of bytes.
+    def _get_type(self, name):
+        return self._arg.get_type(name)
 
-    Notes
-    -----
-    Some databases treat strings and blobs of equally, and some do not.
+    def get_columns(self, iterable):
+        """
+        Get multiple columns from the table
 
-    For example, Impala doesn't make a distinction between string and binary
-    types but PostgreSQL has a `TEXT` type and a `BYTEA` type which are
-    distinct types that have different behavior.
+        Examples
+        --------
+        a, b, c = table.get_columns(['a', 'b', 'c'])
 
-    """
+        Returns
+        -------
+        columns : list of column/array expressions
+        """
+        return [self.get_column(x) for x in iterable]
+
+    def get_column(self, name):
+        """
+        Get a reference to a single column from the table
+
+        Returns
+        -------
+        column : array expression
+        """
+        ref = TableColumn(name, self)
+        return ref.to_expr()
 
-    scalar = "BinaryScalar"
-    column = "BinaryColumn"
+    @property
+    def columns(self):
+        return self.schema().names
 
+    def schema(self):
+        """
+        Get the schema for this table (if one is known)
 
-@public
-class Temporal(DataType):
-    """Data types related to time."""
+        Returns
+        -------
+        schema : Schema
+        """
+        if not self._is_materialized():
+            raise IbisError('Table operation is not yet materialized')
+        return self.op().get_schema()
+
+    def _is_materialized(self):
+        # The operation produces a known schema
+        return self.op().has_schema()
 
+    def add_column(self, expr, name=None):
+        """
+        Add indicated column expression to table, producing a new table. Note:
+        this is a shortcut for performing a projection having the same effect.
 
-@public
-class Date(Temporal, Primitive):
-    """Date values."""
+        Returns
+        -------
+        modified_table : TableExpr
+        """
+        expr = self._ensure_expr(expr)
 
-    scalar = "DateScalar"
-    column = "DateColumn"
+        if not isinstance(expr, ArrayExpr):
+            raise com.InputTypeError('Must pass array expression')
 
+        if name is not None:
+            expr = expr.name(name)
 
-@public
-class Time(Temporal, Primitive):
-    """Time values."""
+        return self.projection([self, expr])
 
-    scalar = "TimeScalar"
-    column = "TimeColumn"
+    def group_by(self, by):
+        """
+        Create an intermediate grouped table expression, pending some group
+        operation to be applied with it.
 
+        Examples
+        --------
+        x.group_by([b1, b2]).aggregate(metrics)
 
-@public
-class Timestamp(Temporal, Parametric):
-    """Timestamp values."""
+        Returns
+        -------
+        grouped_expr : GroupedTableExpr
+        """
+        from ibis.expr.groupby import GroupedTableExpr
+        return GroupedTableExpr(self, by)
 
-    timezone: Optional[str] = None
-    """The timezone of values of this type."""
 
-    # Literal[*range(10)] is only supported from 3.11
-    scale: Optional[Literal[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]] = None
-    """The scale of the timestamp if known."""
+# -----------------------------------------------------------------------------
+# Declare all typed ValueExprs. This is what the user will actually interact
+# with: an instance of each is well-typed and includes all valid methods
+# defined for each type.
 
-    scalar = "TimestampScalar"
-    column = "TimestampColumn"
 
-    @classmethod
-    def from_unit(cls, unit, timezone=None, nullable=True):
-        """Return a timestamp type with the given unit and timezone."""
-        unit = TimestampUnit(unit)
-        if unit == TimestampUnit.SECOND:
-            scale = 0
-        elif unit == TimestampUnit.MILLISECOND:
-            scale = 3
-        elif unit == TimestampUnit.MICROSECOND:
-            scale = 6
-        elif unit == TimestampUnit.NANOSECOND:
-            scale = 9
-        else:
-            raise ValueError(f"Invalid unit {unit}")
-        return cls(scale=scale, timezone=timezone, nullable=nullable)
+class AnyValue(ValueExpr):
 
-    @property
-    def unit(self) -> str:
-        """Return the unit of the timestamp."""
-        if self.scale is None or self.scale == 0:
-            return TimestampUnit.SECOND
-        elif 1 <= self.scale <= 3:
-            return TimestampUnit.MILLISECOND
-        elif 4 <= self.scale <= 6:
-            return TimestampUnit.MICROSECOND
-        elif 7 <= self.scale <= 9:
-            return TimestampUnit.NANOSECOND
-        else:
-            raise ValueError(f"Invalid scale {self.scale}")
+    _typename = 'any'
 
-    @property
-    def _pretty_piece(self) -> str:
-        if self.scale is not None and self.timezone is not None:
-            return f"('{self.timezone}', {self.scale:d})"
-        elif self.timezone is not None:
-            return f"('{self.timezone}')"
-        elif self.scale is not None:
-            return f"({self.scale:d})"
-        else:
-            return ""
 
+class NullValue(AnyValue):
 
-@public
-class SignedInteger(Integer):
-    """Signed integer values."""
+    _typename = 'null'
 
-    @property
-    def bounds(self):
-        exp = self.nbytes * 8 - 1
-        upper = (1 << exp) - 1
-        return Bounds(lower=~upper, upper=upper)
+    def _can_cast_implicit(self, typename):
+        return True
 
 
-@public
-class UnsignedInteger(Integer):
-    """Unsigned integer values."""
+class NumericValue(AnyValue):
 
-    @property
-    def bounds(self):
-        exp = self.nbytes * 8
-        upper = (1 << exp) - 1
-        return Bounds(lower=0, upper=upper)
+    def _can_compare(self, other):
+        return isinstance(other, NumericValue)
 
 
-@public
-class Floating(Primitive, Numeric):
-    """Floating point values."""
+class IntegerValue(NumericValue):
+    pass
 
-    scalar = "FloatingScalar"
-    column = "FloatingColumn"
 
-    @property
-    @abstractmethod
-    def nbytes(self) -> int:  # pragma: no cover
-        """Return the number of bytes used to store values of this type."""
+class BooleanValue(NumericValue):
 
+    _typename = 'boolean'
 
-@public
-class Int8(SignedInteger):
-    """Signed 8-bit integers."""
 
-    nbytes = 1
+class Int8Value(IntegerValue):
 
+    _typename = 'int8'
+    _implicit_casts = set(['int16', 'int32', 'int64', 'float', 'double',
+                           'decimal'])
 
-@public
-class Int16(SignedInteger):
-    """Signed 16-bit integers."""
 
-    nbytes = 2
+class Int16Value(IntegerValue):
 
+    _typename = 'int16'
+    _implicit_casts = set(['int32', 'int64', 'float', 'double', 'decimal'])
 
-@public
-class Int32(SignedInteger):
-    """Signed 32-bit integers."""
 
-    nbytes = 4
+class Int32Value(IntegerValue):
 
+    _typename = 'int32'
+    _implicit_casts = set(['int64', 'float', 'double', 'decimal'])
 
-@public
-class Int64(SignedInteger):
-    """Signed 64-bit integers."""
 
-    nbytes = 8
+class Int64Value(IntegerValue):
 
+    _typename = 'int64'
+    _implicit_casts = set(['float', 'double', 'decimal'])
 
-@public
-class UInt8(UnsignedInteger):
-    """Unsigned 8-bit integers."""
 
-    nbytes = 1
+class FloatingValue(NumericValue):
+    pass
 
 
-@public
-class UInt16(UnsignedInteger):
-    """Unsigned 16-bit integers."""
+class FloatValue(FloatingValue):
 
-    nbytes = 2
+    _typename = 'float'
+    _implicit_casts = set(['double', 'decimal'])
 
 
-@public
-class UInt32(UnsignedInteger):
-    """Unsigned 32-bit integers."""
+class DoubleValue(FloatingValue):
 
-    nbytes = 4
+    _typename = 'double'
+    _implicit_casts = set(['decimal'])
 
 
-@public
-class UInt64(UnsignedInteger):
-    """Unsigned 64-bit integers."""
+class StringValue(AnyValue):
 
-    nbytes = 8
+    _typename = 'string'
 
+    def _can_compare(self, other):
+        return isinstance(other, StringValue)
 
-@public
-class Float16(Floating):
-    """16-bit floating point numbers."""
 
-    nbytes = 2
+class DecimalValue(NumericValue):
 
+    _typename = 'decimal'
+    _implicit_casts = set(['float', 'double'])
 
-@public
-class Float32(Floating):
-    """32-bit floating point numbers."""
+    def __init__(self, meta):
+        self.meta = meta
+        self._precision = meta.precision
+        self._scale = meta.scale
 
-    nbytes = 4
+    def type(self):
+        from ibis.expr.datatypes import Decimal
+        return Decimal(self._precision, self._scale)
 
+    def _base_type(self):
+        return 'decimal'
 
-@public
-class Float64(Floating):
-    """64-bit floating point numbers."""
+    @classmethod
+    def _make_constructor(cls, meta):
+        def constructor(arg, name=None):
+            return cls(arg, meta, name=name)
+        return constructor
 
-    nbytes = 8
 
+class TimestampValue(AnyValue):
 
-@public
-class Decimal(Numeric, Parametric):
-    """Fixed-precision decimal values."""
+    _typename = 'timestamp'
 
-    precision: Optional[int] = None
-    """The number of decimal places values of this type can hold."""
+    def _can_implicit_cast(self, arg):
+        op = arg.op()
+        if isinstance(op, Literal):
+            try:
+                import pandas as pd
+                pd.Timestamp(op.value)
+                return True
+            except ValueError:
+                return False
+        return False
 
-    scale: Optional[int] = None
-    """The number of values after the decimal point."""
+    def _can_compare(self, other):
+        return isinstance(other, TimestampValue)
 
-    scalar = "DecimalScalar"
-    column = "DecimalColumn"
+    def _implicit_cast(self, arg):
+        # assume we've checked this is OK at this point...
+        op = arg.op()
+        return TimestampScalar(op)
 
-    def __init__(
-        self,
-        precision: int | None = None,
-        scale: int | None = None,
-        **kwargs: Any,
-    ) -> None:
-        if precision is not None:
-            if not isinstance(precision, numbers.Integral):
-                raise TypeError(
-                    "Decimal type precision must be an integer; "
-                    f"got {type(precision)}"
-                )
-            if precision < 0:
-                raise ValueError("Decimal type precision cannot be negative")
-            if not precision:
-                raise ValueError("Decimal type precision cannot be zero")
-        if scale is not None:
-            if not isinstance(scale, numbers.Integral):
-                raise TypeError("Decimal type scale must be an integer")
-            if scale < 0:
-                raise ValueError("Decimal type scale cannot be negative")
-            if precision is not None and precision < scale:
-                raise ValueError(
-                    "Decimal type precision must be greater than or equal to "
-                    f"scale. Got precision={precision:d} and scale={scale:d}"
-                )
-        super().__init__(precision=precision, scale=scale, **kwargs)
 
-    @property
-    def _pretty_piece(self) -> str:
-        precision = self.precision
-        scale = self.scale
-        if precision is None and scale is None:
-            return ""
+class NumericArray(ArrayExpr, NumericValue):
+    pass
 
-        args = [str(precision) if precision is not None else "_"]
 
-        if scale is not None:
-            args.append(str(scale))
+class NullScalar(NullValue, ScalarExpr):
+    """
+    A scalar value expression representing NULL
+    """
+    pass
 
-        return f"({', '.join(args)})"
 
+class BooleanScalar(ScalarExpr, BooleanValue):
+    pass
 
-@public
-class Interval(Parametric):
-    """Interval values."""
 
-    unit: IntervalUnit
-    """The time unit of the interval."""
+class BooleanArray(NumericArray, BooleanValue):
+    pass
 
-    scalar = "IntervalScalar"
-    column = "IntervalColumn"
 
-    @property
-    def resolution(self):
-        """The interval unit's name."""
-        return self.unit.singular
+class Int8Scalar(ScalarExpr, Int8Value):
+    pass
 
-    @property
-    def _pretty_piece(self) -> str:
-        return f"('{self.unit.value}')"
 
+class Int8Array(NumericArray, Int8Value):
+    pass
 
-@public
-class Struct(Parametric, MapSet):
-    """Structured values."""
 
-    fields: FrozenDict[str, DataType]
+class Int16Scalar(ScalarExpr, Int16Value):
+    pass
 
-    scalar = "StructScalar"
-    column = "StructColumn"
 
-    @classmethod
-    def from_tuples(
-        cls, pairs: Iterable[tuple[str, str | DataType]], nullable: bool = True
-    ) -> Struct:
-        """Construct a `Struct` type from pairs.
+class Int16Array(NumericArray, Int16Value):
+    pass
 
-        Parameters
-        ----------
-        pairs
-            An iterable of pairs of field name and type
-        nullable
-            Whether the type is nullable
 
-        Returns
-        -------
-        Struct
-            Struct data type instance
+class Int32Scalar(ScalarExpr, Int32Value):
+    pass
 
-        """
-        return cls(dict(pairs), nullable=nullable)
 
-    @attribute
-    def names(self) -> tuple[str, ...]:
-        """Return the names of the struct's fields."""
-        return tuple(self.keys())
+class Int32Array(NumericArray, Int32Value):
+    pass
 
-    @attribute
-    def types(self) -> tuple[DataType, ...]:
-        """Return the types of the struct's fields."""
-        return tuple(self.values())
 
-    def __len__(self) -> int:
-        return len(self.fields)
+class Int64Scalar(ScalarExpr, Int64Value):
+    pass
 
-    def __iter__(self) -> Iterator[str]:
-        return iter(self.fields)
 
-    def __getitem__(self, key: str) -> DataType:
-        return self.fields[key]
+class Int64Array(NumericArray, Int64Value):
+    pass
 
-    def __repr__(self) -> str:
-        return f"'{self.name}({list(self.items())}, nullable={self.nullable})"
 
-    @property
-    def _pretty_piece(self) -> str:
-        pairs = ", ".join(map("{}: {}".format, self.names, self.types))
-        return f"<{pairs}>"
+class FloatScalar(ScalarExpr, FloatValue):
+    pass
 
 
-T = TypeVar("T", bound=DataType, covariant=True)
+class FloatArray(NumericArray, FloatValue):
+    pass
 
 
-@public
-class Array(Variadic, Parametric, Generic[T]):
-    """Array values."""
+class DoubleScalar(ScalarExpr, DoubleValue):
+    pass
 
-    value_type: T
 
-    scalar = "ArrayScalar"
-    column = "ArrayColumn"
+class DoubleArray(NumericArray, DoubleValue):
+    pass
+
+
+class StringScalar(ScalarExpr, StringValue):
+    pass
 
-    @property
-    def _pretty_piece(self) -> str:
-        return f"<{self.value_type}>"
 
+class StringArray(ArrayExpr, StringValue):
+    pass
 
-K = TypeVar("K", bound=DataType, covariant=True)
-V = TypeVar("V", bound=DataType, covariant=True)
 
+class TimestampScalar(ScalarExpr, TimestampValue):
+    pass
 
-@public
-class Map(Variadic, Parametric, Generic[K, V]):
-    """Associative array values."""
 
-    key_type: K
-    value_type: V
+class TimestampArray(ArrayExpr, TimestampValue):
+    pass
 
-    scalar = "MapScalar"
-    column = "MapColumn"
+
+class DecimalScalar(DecimalValue, ScalarExpr):
+
+    def __init__(self, arg, meta, name=None):
+        DecimalValue.__init__(self, meta)
+        ScalarExpr.__init__(self, arg, name=name)
 
     @property
-    def _pretty_piece(self) -> str:
-        return f"<{self.key_type}, {self.value_type}>"
+    def _factory(self):
+        def factory(arg, name=None):
+            return DecimalScalar(arg, self.meta, name=name)
+        return factory
 
 
-@public
-class JSON(Variadic):
-    """JSON values."""
+class DecimalArray(DecimalValue, NumericArray):
 
-    scalar = "JSONScalar"
-    column = "JSONColumn"
+    def __init__(self, arg, meta, name=None):
+        DecimalValue.__init__(self, meta)
+        ArrayExpr.__init__(self, arg, name=name)
 
+    @property
+    def _factory(self):
+        def factory(arg, name=None):
+            return DecimalArray(arg, self.meta, name=name)
+        return factory
+
+
+class CategoryValue(AnyValue):
+
+    """
+    Represents some ordered data categorization; tracked as an int32 value
+    until explicitly
+    """
 
-@public
-class GeoSpatial(DataType):
-    """Geospatial values."""
+    _typename = 'category'
+    _implicit_casts = Int16Value._implicit_casts
 
-    geotype: Optional[Literal["geography", "geometry"]] = None
-    """The specific geospatial type."""
+    def __init__(self, meta):
+        self.meta = meta
 
-    srid: Optional[int] = None
-    """The spatial reference identifier."""
+    def type(self):
+        return self.meta
 
-    column = "GeoSpatialColumn"
-    scalar = "GeoSpatialScalar"
+    def _base_type(self):
+        return 'category'
+
+    def _can_compare(self, other):
+        return isinstance(other, IntegerValue)
+
+
+class CategoryScalar(CategoryValue, ScalarExpr):
+
+    def __init__(self, arg, meta, name=None):
+        CategoryValue.__init__(self, meta)
+        ScalarExpr.__init__(self, arg, name=name)
+
+    @property
+    def _factory(self):
+        def factory(arg, name=None):
+            return CategoryScalar(arg, self.meta, name=name)
+        return factory
+
+
+class CategoryArray(CategoryValue, ArrayExpr):
+
+    def __init__(self, arg, meta, name=None):
+        CategoryValue.__init__(self, meta)
+        ArrayExpr.__init__(self, arg, name=name)
 
     @property
-    def _pretty_piece(self) -> str:
-        piece = ""
-        if self.geotype is not None:
-            piece += f":{self.geotype}"
-        if self.srid is not None:
-            piece += f";{self.srid}"
-        return piece
+    def _factory(self):
+        def factory(arg, name=None):
+            return CategoryArray(arg, self.meta, name=name)
+        return factory
+
+
+class UnnamedMarker(object):
+    pass
+
+
+unnamed = UnnamedMarker()
+
+
+def as_value_expr(val):
+    import pandas as pd
+    if not isinstance(val, Expr):
+        if isinstance(val, (tuple, list)):
+            val = sequence(val)
+        elif isinstance(val, pd.Series):
+            val = sequence(list(val))
+        else:
+            val = literal(val)
+
+    return val
+
+
+def literal(value):
+    """
+    Create a scalar expression from a Python value
+
+    Parameters
+    ----------
+    value : some Python basic type
+
+    Returns
+    -------
+    lit_value : value expression, type depending on input value
+    """
+    if value is None or value is null:
+        return null()
+    else:
+        return Literal(value).to_expr()
 
 
-@public
-class Point(GeoSpatial):
-    """A point described by two coordinates."""
+_NULL = None
 
-    scalar = "PointScalar"
-    column = "PointColumn"
 
+def null():
+    """
+    Create a NULL/NA scalar
+    """
+    global _NULL
+    if _NULL is None:
+        _NULL = NullScalar(NullLiteral())
 
-@public
-class LineString(GeoSpatial):
-    """A sequence of 2 or more points."""
+    return _NULL
 
-    scalar = "LineStringScalar"
-    column = "LineStringColumn"
 
+def sequence(values):
+    """
+    Wrap a list of Python values as an Ibis sequence type
 
-@public
-class Polygon(GeoSpatial):
-    """A set of one or more closed line strings.
+    Parameters
+    ----------
+    values : list
+      Should all be None or the same type
 
-    The first line string represents the shape (external ring) and the
-    rest represent holes in that shape (internal rings).
+    Returns
+    -------
+    seq : Sequence
     """
+    return ValueList(values).to_expr()
 
-    scalar = "PolygonScalar"
-    column = "PolygonColumn"
 
+class NullLiteral(ValueNode):
 
-@public
-class MultiLineString(GeoSpatial):
-    """A set of one or more line strings."""
+    """
+    Typeless NULL literal
+    """
 
-    scalar = "MultiLineStringScalar"
-    column = "MultiLineStringColumn"
+    def __init__(self):
+        pass
 
+    @property
+    def args(self):
+        return [None]
 
-@public
-class MultiPoint(GeoSpatial):
-    """A set of one or more points."""
+    def equals(self, other):
+        return isinstance(other, NullLiteral)
 
-    scalar = "MultiPointScalar"
-    column = "MultiPointColumn"
+    def output_type(self):
+        return NullScalar
 
+    def root_tables(self):
+        return []
 
-@public
-class MultiPolygon(GeoSpatial):
-    """A set of one or more polygons."""
 
-    scalar = "MultiPolygonScalar"
-    column = "MultiPolygonColumn"
+class ListExpr(ArrayExpr, AnyValue):
+    pass
 
 
-@public
-class UUID(DataType):
-    """A 128-bit number used to identify information in computer systems."""
+class SortExpr(Expr):
+    pass
 
-    scalar = "UUIDScalar"
-    column = "UUIDColumn"
 
+class ValueList(ValueNode):
 
-@public
-class MACADDR(DataType):
-    """Media Access Control (MAC) address of a network interface."""
+    """
+    Data structure for a list of value expressions
+    """
 
-    scalar = "MACADDRScalar"
-    column = "MACADDRColumn"
+    def __init__(self, args):
+        self.values = [as_value_expr(x) for x in args]
+        ValueNode.__init__(self, self.values)
 
+    def root_tables(self):
+        return distinct_roots(*self.values)
 
-@public
-class INET(DataType):
-    """IP addresses."""
+    def to_expr(self):
+        return ListExpr(self)
 
-    scalar = "INETScalar"
-    column = "INETColumn"
 
+def bind_expr(table, expr):
+    if isinstance(expr, (list, tuple)):
+        return [bind_expr(table, x) for x in expr]
 
-# ---------------------------------------------------------------------
+    return table._ensure_expr(expr)
+
+
+def find_base_table(expr):
+    if isinstance(expr, TableExpr):
+        return expr
+
+    for arg in expr.op().flat_args():
+        if isinstance(arg, Expr):
+            r = find_base_table(arg)
+            if isinstance(r, TableExpr):
+                return r
+
+
+def find_all_base_tables(expr, memo=None):
+    if memo is None:
+        memo = {}
+
+    node = expr.op()
+
+    if (isinstance(expr, TableExpr) and
+            isinstance(node, BlockingTableNode)):
+        if id(expr) not in memo:
+            memo[id(expr)] = expr
+        return memo
+
+    for arg in expr.op().flat_args():
+        if isinstance(arg, Expr):
+            find_all_base_tables(arg, memo)
 
-null = Null()
-boolean = Boolean()
-int8 = Int8()
-int16 = Int16()
-int32 = Int32()
-int64 = Int64()
-uint8 = UInt8()
-uint16 = UInt16()
-uint32 = UInt32()
-uint64 = UInt64()
-float16 = Float16()
-float32 = Float32()
-float64 = Float64()
-string = String()
-binary = Binary()
-date = Date()
-time = Time()
-timestamp = Timestamp()
-# geo spatial data type
-geometry = GeoSpatial(geotype="geometry")
-geography = GeoSpatial(geotype="geography")
-point = Point()
-linestring = LineString()
-polygon = Polygon()
-multilinestring = MultiLineString()
-multipoint = MultiPoint()
-multipolygon = MultiPolygon()
-# json
-json = JSON()
-# special string based data type
-uuid = UUID()
-macaddr = MACADDR()
-inet = INET()
-decimal = Decimal()
-unknown = Unknown()
-
-Enum = String
-
-
-public(
-    Any=DataType,
-    null=null,
-    boolean=boolean,
-    int8=int8,
-    int16=int16,
-    int32=int32,
-    int64=int64,
-    uint8=uint8,
-    uint16=uint16,
-    uint32=uint32,
-    uint64=uint64,
-    float16=float16,
-    float32=float32,
-    float64=float64,
-    string=string,
-    binary=binary,
-    date=date,
-    time=time,
-    timestamp=timestamp,
-    dtype=dtype,
-    geometry=geometry,
-    geography=geography,
-    point=point,
-    linestring=linestring,
-    polygon=polygon,
-    multilinestring=multilinestring,
-    multipoint=multipoint,
-    multipolygon=multipolygon,
-    json=json,
-    uuid=uuid,
-    macaddr=macaddr,
-    inet=inet,
-    decimal=decimal,
-    unknown=unknown,
-    Enum=Enum,
-    Geography=GeoSpatial,
-    Geometry=GeoSpatial,
-    Set=Array,
-)
+    return memo
```

### Comparing `ibis_framework-9.0.0.dev686/ibis/tests/__init__.py` & `ibis-framework-v0.6.0/ibis/tests/__init__.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,9 +1,7 @@
-from __future__ import annotations
-
 # Copyright 2015 Cloudera Inc.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 # http://www.apache.org/licenses/LICENSE-2.0
```

### Comparing `ibis_framework-9.0.0.dev686/ibis/tests/expr/test_analysis.py` & `ibis-framework-v0.6.0/ibis/expr/tests/test_analysis.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,279 +1,269 @@
-from __future__ import annotations
-
-import pytest
+# Copyright 2014 Cloudera Inc.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+# http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 import ibis
-import ibis.common.exceptions as com
+
+from ibis.compat import unittest
+from ibis.expr.tests.mocks import BasicTestCase
+import ibis.expr.analysis as L
 import ibis.expr.operations as ops
-from ibis.expr.rewrites import simplify
-from ibis.expr.tests.test_newrels import join_tables
+import ibis.common as com
+
+from ibis.tests.util import assert_equal
+
 
 # Place to collect esoteric expression analysis bugs and tests
 
 
-def test_rewrite_join_projection_without_other_ops(con):
-    # Star schema with fact table
-    table = con.table("star1")
-    table2 = con.table("star2")
-    table3 = con.table("star3")
-
-    filtered = table[table["f"] > 0]
-
-    pred1 = table["foo_id"] == table2["foo_id"]
-    pred2 = filtered["bar_id"] == table3["bar_id"]
-
-    j1 = filtered.left_join(table2, [pred1])
-    j2 = j1.inner_join(table3, [pred2])
-    # Project out the desired fields
-    view = j2[[filtered, table2["value1"], table3["value2"]]]
+class TestTableExprBasics(BasicTestCase, unittest.TestCase):
 
-    with join_tables(j2) as (r1, r2, r3):
-        # Construct the thing we expect to obtain
-        expected = ops.JoinChain(
-            first=r1,
-            rest=[
-                ops.JoinLink(
-                    how="left",
-                    table=r2,
-                    predicates=[r1["foo_id"] == r2["foo_id"]],
-                ),
-                ops.JoinLink(
-                    how="inner",
-                    table=r3,
-                    predicates=[r1["bar_id"] == r3["bar_id"]],
-                ),
-            ],
-            values={
-                "c": r1.c,
-                "f": r1.f,
-                "foo_id": r1.foo_id,
-                "bar_id": r1.bar_id,
-                "value1": r2.value1,
-                "value2": r3.value2,
-            },
-        )
-        assert view.op() == expected
-
-
-def test_multiple_join_deeper_reference():
-    # Join predicates down the chain might reference one or more root
-    # tables in the hierarchy.
-    table1 = ibis.table({"key1": "string", "key2": "string", "value1": "double"})
-    table2 = ibis.table({"key3": "string", "value2": "double"})
-    table3 = ibis.table({"key4": "string", "value3": "double"})
-
-    joined = table1.inner_join(table2, [table1["key1"] == table2["key3"]])
-    joined2 = joined.inner_join(table3, [table1["key2"] == table3["key4"]])
-
-    # it works, what more should we test here?
-    repr(joined2)
-
-
-def test_filter_on_projected_field(con):
-    # See #173. Impala and other SQL engines do not allow filtering on a
-    # just-created alias in a projection
-    region = con.table("tpch_region")
-    nation = con.table("tpch_nation")
-    customer = con.table("tpch_customer")
-    orders = con.table("tpch_orders")
-
-    fields_of_interest = [
-        customer,
-        region.r_name.name("region"),
-        orders.o_totalprice.name("amount"),
-        orders.o_orderdate.cast("timestamp").name("odate"),
-    ]
-
-    all_join = (
-        region.join(nation, region.r_regionkey == nation.n_regionkey)
-        .join(customer, customer.c_nationkey == nation.n_nationkey)
-        .join(orders, orders.o_custkey == customer.c_custkey)
-    )
-
-    tpch = all_join[fields_of_interest]
-
-    # Correlated subquery, yikes!
-    t2 = tpch.view()
-    conditional_avg = t2[(t2.region == tpch.region)].amount.mean()
-
-    # `amount` is part of the projection above as an aliased field
-    amount_filter = tpch.amount > conditional_avg
-
-    result = tpch.filter([amount_filter])
-
-    # Now then! Predicate pushdown here is inappropriate, so we check that
-    # it didn't occur.
-    assert isinstance(result.op(), ops.Filter)
-    assert result.op().parent == tpch.op()
-
-
-def test_join_predicate_from_derived_raises():
-    # Join predicate references a derived table, but we can salvage and
-    # rewrite it to get the join semantics out
-    # see ibis #74
-    table = ibis.table([("c", "int32"), ("f", "double"), ("g", "string")], "foo_table")
-
-    table2 = ibis.table([("key", "string"), ("value", "double")], "bar_table")
-
-    filter_pred = table["f"] > 0
-    table3 = table[filter_pred]
-
-    with pytest.raises(com.IntegrityError, match="they belong to another relation"):
-        # TODO(kszucs): could be smarter actually and rewrite the predicate
-        # to contain the conditions from the filter
-        table.inner_join(table2, [table3["g"] == table2["key"]])
-
-
-def test_bad_join_predicate_raises():
-    table = ibis.table([("c", "int32"), ("f", "double"), ("g", "string")], "foo_table")
-    table2 = ibis.table([("key", "string"), ("value", "double")], "bar_table")
-    table3 = ibis.table([("key", "string"), ("value", "double")], "baz_table")
-
-    with pytest.raises(com.IntegrityError):
-        table.inner_join(table2, [table["g"] == table3["key"]])
-
-
-def test_filter_self_join():
-    # GH #667
-    purchases = ibis.table(
-        [
-            ("region", "string"),
-            ("kind", "string"),
-            ("user", "int64"),
-            ("amount", "double"),
-        ],
-        "purchases",
-    )
-
-    metric = purchases.amount.sum().name("total")
-    agged = purchases.group_by(["region", "kind"]).aggregate(metric)
-    assert agged.op() == ops.Aggregate(
-        parent=purchases,
-        groups={"region": purchases.region, "kind": purchases.kind},
-        metrics={"total": purchases.amount.sum()},
-    )
-
-    left = agged[agged.kind == "foo"]
-    right = agged[agged.kind == "bar"]
-    assert left.op() == ops.Filter(
-        parent=agged,
-        predicates=[agged.kind == "foo"],
-    )
-    assert right.op() == ops.Filter(
-        parent=agged,
-        predicates=[agged.kind == "bar"],
-    )
-
-    cond = left.region == right.region
-    joined = left.join(right, cond)
-    metric = (left.total - right.total).name("diff")
-    what = [left.region, metric]
-    projected = joined.select(what)
-
-    with join_tables(joined) as (r1, r2):
-        join = ops.JoinChain(
-            first=r1,
-            rest=[
-                ops.JoinLink("inner", r2, [r1.region == r2.region]),
-            ],
-            values={
-                "region": r1.region,
-                "diff": r1.total - r2.total,
-            },
-        )
-        assert projected.op() == join
-
-
-def test_is_ancestor_analytic():
-    x = ibis.table(ibis.schema([("col", "int32")]), "x")
-    with_filter_col = x[x.columns + [ibis.null().name("filter")]]
-    filtered = with_filter_col[with_filter_col["filter"].isnull()]
-    subquery = filtered[filtered.columns]
-
-    with_analytic = subquery[subquery.columns + [subquery.count().name("analytic")]]
-
-    assert not subquery.op().equals(with_analytic.op())
-
-
-# Pr 2635
-def test_mutation_fusion_no_overwrite():
-    """Test fusion with chained mutation that doesn't overwrite existing
-    columns."""
-    t = ibis.table(ibis.schema([("col", "int32")]), "t")
-
-    result = t
-    result = result.mutate(col1=t["col"] + 1)
-    result = result.mutate(col2=t["col"] + 2)
-    result = result.mutate(col3=t["col"] + 3)
-
-    simplified = simplify(result.op())
-    assert simplified == ops.Project(
-        parent=t,
-        values={
-            "col": t["col"],
-            "col1": t["col"] + 1,
-            "col2": t["col"] + 2,
-            "col3": t["col"] + 3,
-        },
-    )
-
-
-# Pr 2635
-def test_mutation_fusion_overwrite():
-    """Test fusion with chained mutation that overwrites existing columns."""
-    t = ibis.table(ibis.schema([("col", "int32")]), "t")
-
-    result = t
-
-    result = result.mutate(col1=t["col"] + 1)
-    result = result.mutate(col2=t["col"] + 2)
-    result = result.mutate(col3=t["col"] + 3)
-    result = result.mutate(col=t["col"] - 1)
-
-    with pytest.raises(com.IntegrityError):
-        # unable to dereference the column since result doesn't contain it anymore
-        result.mutate(col4=t["col"] + 4)
-
-    simplified = simplify(result.op())
-    assert simplified == ops.Project(
-        parent=t,
-        values={
-            "col": t["col"] - 1,
-            "col1": t["col"] + 1,
-            "col2": t["col"] + 2,
-            "col3": t["col"] + 3,
-        },
-    )
-
-
-# Pr 2635
-def test_select_filter_mutate_fusion():
-    """Test fusion with filter followed by mutation on the same input."""
-
-    t = ibis.table(ibis.schema([("col", "float32")]), "t")
-
-    t1 = t[["col"]]
-    assert t1.op() == ops.Project(parent=t, values={"col": t.col})
-
-    t2 = t1[t1["col"].isnan()]
-    assert t2.op() == ops.Filter(parent=t1, predicates=[t1.col.isnan()])
-
-    t3 = t2.mutate(col=t2["col"].cast("int32"))
-    assert t3.op() == ops.Project(parent=t2, values={"col": t2.col.cast("int32")})
-
-    # create the expected expression
-    filt = ops.Filter(parent=t, predicates=[t.col.isnan()]).to_expr()
-    proj = ops.Project(parent=filt, values={"col": filt.col.cast("int32")}).to_expr()
-
-    t3_opt = simplify(t3.op()).to_expr()
-    assert t3_opt.equals(proj)
-
-
-def test_agg_selection_does_not_share_roots():
-    t = ibis.table(dict(a="string"), name="t")
-    s = ibis.table(dict(b="float64"), name="s")
-    gb = t.group_by("a")
-    n = s.count()
+    def test_rewrite_substitute_distinct_tables(self):
+        t = self.con.table('test1')
+        tt = self.con.table('test1')
+
+        expr = t[t.c > 0]
+        expr2 = tt[tt.c > 0]
+
+        metric = t.f.sum().name('metric')
+        expr3 = expr.aggregate(metric)
+
+        result = L.sub_for(expr3, [(expr2, t)])
+        expected = t.aggregate(metric)
 
-    with pytest.raises(com.IntegrityError, match=" they belong to another relation"):
-        gb.aggregate(n=n)
+        assert_equal(result, expected)
+
+    def test_rewrite_join_projection_without_other_ops(self):
+        # Drop out filters and other commutative table operations. Join
+        # predicates are "lifted" to reference the base, unmodified join roots
+
+        # Star schema with fact table
+        table = self.con.table('star1')
+        table2 = self.con.table('star2')
+        table3 = self.con.table('star3')
+
+        filtered = table[table['f'] > 0]
+
+        pred1 = table['foo_id'] == table2['foo_id']
+        pred2 = filtered['bar_id'] == table3['bar_id']
+
+        j1 = filtered.left_join(table2, [pred1])
+        j2 = j1.inner_join(table3, [pred2])
+
+        # Project out the desired fields
+        view = j2[[filtered, table2['value1'], table3['value2']]]
+
+        # Construct the thing we expect to obtain
+        ex_pred2 = table['bar_id'] == table3['bar_id']
+        ex_expr = (table.left_join(table2, [pred1])
+                   .inner_join(table3, [ex_pred2]))
+
+        rewritten_proj = L.substitute_parents(view)
+        op = rewritten_proj.op()
+        assert_equal(op.table, ex_expr)
+
+        # Ensure that filtered table has been substituted with the base table
+        assert op.selections[0] is table
+
+    def test_rewrite_past_projection(self):
+        table = self.con.table('test1')
+
+        # Rewrite past a projection
+        table3 = table[['c', 'f']]
+        expr = table3['c'] == 2
+
+        result = L.substitute_parents(expr)
+        expected = table['c'] == 2
+        assert_equal(result, expected)
+
+        # Unsafe to rewrite past projection
+        table5 = table[(table.f * 2).name('c'), table.f]
+        expr = table5['c'] == 2
+        result = L.substitute_parents(expr)
+        assert result is expr
+
+    def test_rewrite_expr_with_parent(self):
+        table = self.con.table('test1')
+
+        table2 = table[table['f'] > 0]
+
+        expr = table2['c'] == 2
+
+        result = L.substitute_parents(expr)
+        expected = table['c'] == 2
+        assert_equal(result, expected)
+
+        # Substitution not fully possible if we depend on a new expr in a
+        # projection
+
+        table4 = table[['c', (table['c'] * 2).name('foo')]]
+        expr = table4['c'] == table4['foo']
+        result = L.substitute_parents(expr)
+        expected = table['c'] == table4['foo']
+        assert_equal(result, expected)
+
+    def test_rewrite_distinct_but_equal_objects(self):
+        t = self.con.table('test1')
+        t_copy = self.con.table('test1')
+
+        table2 = t[t_copy['f'] > 0]
+
+        expr = table2['c'] == 2
+
+        result = L.substitute_parents(expr)
+        expected = t['c'] == 2
+        assert_equal(result, expected)
+
+    def test_projection_with_join_pushdown_rewrite_refs(self):
+        # Observed this expression IR issue in a TopK-rewrite context
+        table1 = ibis.table([
+            ('a_key1', 'string'),
+            ('a_key2', 'string'),
+            ('a_value', 'double')
+        ], 'foo')
+
+        table2 = ibis.table([
+            ('b_key1', 'string'),
+            ('b_name', 'string'),
+            ('b_value', 'double')
+        ], 'bar')
+
+        table3 = ibis.table([
+            ('c_key2', 'string'),
+            ('c_name', 'string')
+        ], 'baz')
+
+        proj = (table1.inner_join(table2, [('a_key1', 'b_key1')])
+                .inner_join(table3, [(table1.a_key2, table3.c_key2)])
+                [table1, table2.b_name.name('b'), table3.c_name.name('c'),
+                 table2.b_value])
+
+        cases = [
+            (proj.a_value > 0, table1.a_value > 0),
+            (proj.b_value > 0, table2.b_value > 0)
+        ]
+
+        for higher_pred, lower_pred in cases:
+            result = proj.filter([higher_pred])
+            op = result.op()
+            assert isinstance(op, ops.Projection)
+            filter_op = op.table.op()
+            assert isinstance(filter_op, ops.Filter)
+            new_pred = filter_op.predicates[0]
+            assert_equal(new_pred, lower_pred)
+
+    def test_multiple_join_deeper_reference(self):
+        # Join predicates down the chain might reference one or more root
+        # tables in the hierarchy.
+        table1 = ibis.table({'key1': 'string', 'key2': 'string',
+                            'value1': 'double'})
+        table2 = ibis.table({'key3': 'string', 'value2': 'double'})
+        table3 = ibis.table({'key4': 'string', 'value3': 'double'})
+
+        joined = table1.inner_join(table2, [table1['key1'] == table2['key3']])
+        joined2 = joined.inner_join(table3, [table1['key2'] == table3['key4']])
+
+        # it works, what more should we test here?
+        materialized = joined2.materialize()
+        repr(materialized)
+
+    def test_filter_on_projected_field(self):
+        # See #173. Impala and other SQL engines do not allow filtering on a
+        # just-created alias in a projection
+        region = self.con.table('tpch_region')
+        nation = self.con.table('tpch_nation')
+        customer = self.con.table('tpch_customer')
+        orders = self.con.table('tpch_orders')
+
+        fields_of_interest = [customer,
+                              region.r_name.name('region'),
+                              orders.o_totalprice.name('amount'),
+                              orders.o_orderdate
+                              .cast('timestamp').name('odate')]
+
+        all_join = (
+            region.join(nation, region.r_regionkey == nation.n_regionkey)
+            .join(customer, customer.c_nationkey == nation.n_nationkey)
+            .join(orders, orders.o_custkey == customer.c_custkey))
+
+        tpch = all_join[fields_of_interest]
+
+        # Correlated subquery, yikes!
+        t2 = tpch.view()
+        conditional_avg = t2[(t2.region == tpch.region)].amount.mean()
+
+        # `amount` is part of the projection above as an aliased field
+        amount_filter = tpch.amount > conditional_avg
+
+        result = tpch.filter([amount_filter])
+
+        # Now then! Predicate pushdown here is inappropriate, so we check that
+        # it didn't occur.
+
+        # If filter were pushed below projection, the top-level operator type
+        # would be Projection instead.
+        assert type(result.op()) == ops.Filter
+
+    def test_bad_join_predicate_raises(self):
+        # Join predicate references a derived table, but we can salvage and
+        # rewrite it to get the join semantics out
+        # see ibis #74
+        table = ibis.table([
+            ('c', 'int32'),
+            ('f', 'double'),
+            ('g', 'string')
+        ], 'foo_table')
+
+        table2 = ibis.table([
+            ('key', 'string'),
+            ('value', 'double')
+        ], 'bar_table')
+
+        filter_pred = table['f'] > 0
+        table3 = table[filter_pred]
+
+        with self.assertRaises(com.ExpressionError):
+            table.inner_join(table2, [table3['g'] == table2['key']])
+
+        # expected = table.inner_join(table2, [table['g'] == table2['key']])
+        # assert_equal(result, expected)
+
+    def test_filter_self_join(self):
+        # GH #667
+        purchases = ibis.table([('region', 'string'),
+                                ('kind', 'string'),
+                                ('user', 'int64'),
+                                ('amount', 'double')], 'purchases')
+
+        metric = purchases.amount.sum().name('total')
+        agged = (purchases.group_by(['region', 'kind'])
+                 .aggregate(metric))
+
+        left = agged[agged.kind == 'foo']
+        right = agged[agged.kind == 'bar']
+
+        cond = left.region == right.region
+        joined = left.join(right, cond)
+
+        # unmodified by analysis
+        assert_equal(joined.op().predicates[0], cond)
+
+        metric = (left.total - right.total).name('diff')
+        what = [left.region, metric]
+        projected = joined.projection(what)
+
+        proj_exprs = projected.op().selections
+
+        # proj exprs unaffected by analysis
+        assert_equal(proj_exprs[0], left.region)
+        assert_equal(proj_exprs[1], metric)
```

### Comparing `ibis_framework-9.0.0.dev686/ibis/tests/expr/test_table.py` & `ibis-framework-v0.6.0/ibis/sql/tests/test_compiler.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,2079 +1,2091 @@
-from __future__ import annotations
-
-import datetime
-import pickle
-import re
-
-import numpy as np
-import pandas as pd
-import pytest
-from pytest import param
+# Copyright 2014 Cloudera Inc.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+# http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 import ibis
-import ibis.common.exceptions as com
-import ibis.expr.datatypes as dt
-import ibis.expr.operations as ops
-import ibis.expr.schema as sch
-import ibis.expr.types as ir
-import ibis.selectors as s
-from ibis import _
-from ibis.common.annotations import ValidationError
-from ibis.common.deferred import Deferred
-from ibis.common.exceptions import ExpressionError, IntegrityError, RelationError
-from ibis.expr import api
-from ibis.expr.rewrites import simplify
-from ibis.expr.tests.test_newrels import join_tables
-from ibis.expr.types import Column, Table
-from ibis.tests.util import assert_equal, assert_pickle_roundtrip
-
-
-@pytest.fixture
-def set_ops_schema_top():
-    return [("key", "string"), ("value", "double")]
-
-
-@pytest.fixture
-def set_ops_schema_bottom():
-    return [("key", "string"), ("key2", "string"), ("value", "double")]
-
-
-@pytest.fixture
-def setops_table_foo(set_ops_schema_top):
-    return ibis.table(set_ops_schema_top, "foo")
-
-
-@pytest.fixture
-def setops_table_bar(set_ops_schema_top):
-    return ibis.table(set_ops_schema_top, "bar")
-
-
-@pytest.fixture
-def setops_table_baz(set_ops_schema_bottom):
-    return ibis.table(set_ops_schema_bottom, "baz")
-
-
-@pytest.fixture
-def setops_relation_error_message():
-    return "Table schemas must be equal for set operations"
-
-
-def test_empty_schema():
-    table = api.table([], "foo")
-    assert not table.schema()
-
-
-def test_columns(con):
-    t = con.table("alltypes")
-    result = t.columns
-    expected = ["a", "b", "c", "d", "e", "f", "g", "h", "i", "j", "k"]
-    assert result == expected
-
-
-def test_view_new_relation(table):
-    # For assisting with self-joins and other self-referential operations
-    # where we need to be able to treat instances of the same Table as
-    # semantically distinct
-    #
-    # This thing is not exactly a projection, since it has no semantic
-    # meaning when it comes to execution
-    tview1 = table.view()
-    tview2 = table.view()
-    tview2_ = tview2.view()
-
-    node1 = tview1.op()
-    node2 = tview2.op()
-    node2_ = tview2_.op()
-
-    assert isinstance(node1, ops.SelfReference)
-    assert isinstance(node2, ops.SelfReference)
-    assert node1.parent is node2.parent
-    assert node1 != node2
-    assert node2_ is node2
-
-
-def test_getitem_column_select(table):
-    for k in table.columns:
-        col = table[k]
-
-        # Make sure it's the right type
-        assert isinstance(col, Column)
-
-
-def test_select_using_selector(table):
-    expr = table[s.numeric()]
-    expected = table.select(
-        table.a,
-        table.b,
-        table.c,
-        table.d,
-        table.e,
-        table.f,
-    )
-    assert expr.equals(expected)
-
-
-def test_table_tab_completion():
-    table = ibis.table({"a": "int", "b": "int", "for": "int", "with spaces": "int"})
-    # Only valid python identifiers in getattr tab completion
-    attrs = set(dir(table))
-    assert {"a", "b"}.issubset(attrs)
-    assert {"for", "with spaces"}.isdisjoint(attrs)
-    # All columns in getitem tab completion
-    items = set(table._ipython_key_completions_())
-    assert items.issuperset(table.columns)
-
-
-def test_getitem_attribute(table):
-    result = table.a
-    assert_equal(result, table["a"])
-
-    # Project and add a name that conflicts with a Table built-in
-    # attribute
-    view = table[[table, table["a"].name("schema")]]
-    assert not isinstance(view.schema, Column)
-
-
-def test_getitem_missing_column(table):
-    with pytest.raises(com.IbisTypeError, match="oops"):
-        table["oops"]
-
-
-def test_getattr_missing_column(table):
-    with pytest.raises(AttributeError, match="oops"):
-        table.oops  # noqa: B018
-
-
-def test_typo_method_name_recommendation(table):
-    with pytest.raises(AttributeError, match="order_by"):
-        table.sort("a")
-
-    # Existing columns take precedence over raising an error
-    # for a common method typo
-    table2 = table.rename(sort="a")
-    assert isinstance(table2.sort, Column)
-
-
-def test_projection(table):
-    cols = ["f", "a", "h"]
-
-    proj = table[cols]
-    assert isinstance(proj, Table)
-    assert isinstance(proj.op(), ops.Project)
-
-    assert proj.schema().names == tuple(cols)
-    for c in cols:
-        expr = proj[c]
-        assert isinstance(expr, type(table[c]))
 
+from ibis.impala.compiler import build_ast, to_sql
 
-def test_projection_no_list(table):
-    expr = (table.f * 2).name("bar")
-    result = table.select(expr)
-    expected = table.select([expr])
-    assert_equal(result, expected)
+from ibis import impala
 
+from ibis.expr.tests.mocks import MockConnection
+from ibis.compat import unittest
+import ibis.common as com
 
-def test_projection_with_exprs(table):
-    # unnamed expr to test
-    mean_diff = (table["a"] - table["c"]).mean()
-
-    col_exprs = [table["b"].log().name("log_b"), mean_diff.name("mean_diff")]
-
-    proj = table[col_exprs + ["g"]]
-    schema = proj.schema()
-    assert schema.names == ("log_b", "mean_diff", "g")
-    assert schema.types == (dt.double, dt.double, dt.string)
-
-    # Test with unnamed expr
-    proj = table.select(["g", table["a"] - table["c"]])
-    schema = proj.schema()
-    assert schema.names == ("g", "Subtract(a, c)")
-    assert schema.types == (dt.string, dt.int64)
-
-
-def test_projection_duplicate_names(table):
-    with pytest.raises(com.IntegrityError):
-        table.select([table.c, table.c])
-
-
-def test_projection_sort_keys_errors(table):
-    """Forbid using `asc`/`desc` in selections"""
-    with pytest.raises(ValidationError):
-        table.select([table.c.desc()])
-
-    with pytest.raises(ValidationError):
-        table.mutate(new=table.c.asc())
-
-
-def test_projection_invalid_root(table):
-    schema1 = {"foo": "double", "bar": "int32"}
-
-    left = api.table(schema1, name="foo")
-    right = api.table(schema1, name="bar")
-
-    exprs = [right["foo"], right["bar"]]
-    with pytest.raises(IntegrityError):
-        left.select(exprs)
-
-
-def test_projection_with_star_expr(table):
-    new_expr = (table["a"] * 5).name("bigger_a")
-
-    t = table
-
-    # it lives!
-    proj = t[t, new_expr]
-    repr(proj)
-
-    ex_names = table.schema().names + ("bigger_a",)
-    assert proj.schema().names == ex_names
-
-    # cannot pass an invalid table expression
-    t2 = t.aggregate([t["a"].sum().name("sum(a)")], by=["g"])
-    with pytest.raises(IntegrityError):
-        t[[t2]]
-    # TODO: there may be some ways this can be invalid
-
-
-def test_projection_convenient_syntax(table):
-    proj = table[table, table["a"].name("foo")]
-    proj2 = table[[table, table["a"].name("foo")]]
-    assert_equal(proj, proj2)
-
-
-def test_projection_mutate_analysis_bug(con):
-    # GH #549
-
-    t = con.table("airlines")
-
-    filtered = t[t.depdelay.notnull()]
-    leg = ibis.literal("-").join([t.origin, t.dest])
-    mutated = filtered.mutate(leg=leg)
-
-    # it works!
-    mutated["year", "month", "day", "depdelay", "leg"]
-
-
-def test_projection_self(table):
-    result = table[table]
-    expected = table.select(table)
-
-    assert_equal(result, expected)
-
-
-def test_projection_array_expr(table):
-    result = table[table.a]
-    expected = table[[table.a]]
-    assert_equal(result, expected)
-
-
-@pytest.mark.parametrize("empty", [list(), dict()])
-def test_projection_no_expr(table, empty):
-    with pytest.raises(com.IbisTypeError, match="must select at least one"):
-        table.select(empty)
+import ibis.expr.api as api
+import ibis.expr.operations as ops
 
 
-# FIXME(kszucs): currently bind() flattens the list of expressions, so arbitrary
-# nesting is allowed, need to revisit
-# def test_projection_invalid_nested_list(table):
-#     errmsg = "must be coerceable to expressions"
-#     with pytest.raises(com.IbisTypeError, match=errmsg):
-#         table.select(["a", ["b"]])
-#     with pytest.raises(com.IbisTypeError, match=errmsg):
-#         table[["a", ["b"]]]
-#     with pytest.raises(com.IbisTypeError, match=errmsg):
-#         table["a", ["b"]]
+class TestASTBuilder(unittest.TestCase):
 
+    def setUp(self):
+        self.con = MockConnection()
 
-def test_mutate(table):
-    expr = table.mutate(
-        [
-            (table.a + 1).name("x1"),
-            table.b.sum().name("x2"),
-            (_.a + 2).name("x3"),
-            lambda _: (_.a + 3).name("x4"),
-            ibis.literal(4),
-            ibis.literal("five"),
+    def test_ast_with_projection_join_filter(self):
+        table = self.con.table('test1')
+        table2 = self.con.table('test2')
+
+        filter_pred = table['f'] > 0
+
+        table3 = table[filter_pred]
+
+        join_pred = table3['g'] == table2['key']
+
+        joined = table2.inner_join(table3, [join_pred])
+        result = joined[[table3, table2['value']]]
+
+        ast = build_ast(result)
+        stmt = ast.queries[0]
+
+        def foo():
+            table3 = table[filter_pred]
+            joined = table2.inner_join(table3, [join_pred])
+            result = joined[[table3, table2['value']]]
+            return result
+
+        assert len(stmt.select_set) == 2
+        assert len(stmt.where) == 1
+        assert stmt.where[0] is filter_pred
+
+        # Check that the join has been rebuilt to only include the root tables
+        tbl = stmt.table_set
+        tbl_node = tbl.op()
+        assert isinstance(tbl_node, ops.InnerJoin)
+        assert tbl_node.left is table2
+        assert tbl_node.right is table
+
+        # table expression substitution has been made in the predicate
+        assert tbl_node.predicates[0].equals(table['g'] == table2['key'])
+
+    def test_ast_with_aggregation_join_filter(self):
+        table = self.con.table('test1')
+        table2 = self.con.table('test2')
+
+        filter_pred = table['f'] > 0
+        table3 = table[filter_pred]
+        join_pred = table3['g'] == table2['key']
+
+        joined = table2.inner_join(table3, [join_pred])
+
+        met1 = (table3['f'] - table2['value']).mean().name('foo')
+        result = joined.aggregate([met1, table3['f'].sum().name('bar')],
+                                  by=[table3['g'], table2['key']])
+
+        ast = build_ast(result)
+        stmt = ast.queries[0]
+
+        # hoisted metrics
+        ex_metrics = [(table['f'] - table2['value']).mean().name('foo'),
+                      table['f'].sum().name('bar')]
+        ex_by = [table['g'], table2['key']]
+
+        # hoisted join and aggregate
+        expected_table_set = \
+            table2.inner_join(table, [table['g'] == table2['key']])
+        assert stmt.table_set.equals(expected_table_set)
+
+        # Check various exprs
+        for res, ex in zip(stmt.select_set, ex_by + ex_metrics):
+            assert res.equals(ex)
+
+        for res, ex in zip(stmt.group_by, ex_by):
+            assert stmt.select_set[res].equals(ex)
+
+        # Check we got the filter
+        assert len(stmt.where) == 1
+        assert stmt.where[0].equals(filter_pred)
+
+
+class TestNonTabularResults(unittest.TestCase):
+
+    """
+
+    """
+
+    def setUp(self):
+        self.con = MockConnection()
+        self.table = self.con.table('alltypes')
+
+    def test_simple_scalar_aggregates(self):
+        from pandas import DataFrame
+
+        # Things like table.column.{sum, mean, ...}()
+        table = self.con.table('alltypes')
+
+        expr = table[table.c > 0].f.sum()
+
+        ast = build_ast(expr)
+        query = ast.queries[0]
+
+        sql_query = query.compile()
+        expected = """SELECT sum(`f`) AS `sum`
+FROM alltypes
+WHERE `c` > 0"""
+
+        assert sql_query == expected
+
+        # Maybe the result handler should act on the cursor. Not sure.
+        handler = query.result_handler
+        output = DataFrame({'sum': [5]})
+        assert handler(output) == 5
+
+    def test_table_column_unbox(self):
+        from pandas import DataFrame
+
+        table = self.table
+        m = table.f.sum().name('total')
+        agged = table[table.c > 0].group_by('g').aggregate([m])
+        expr = agged.g
+
+        ast = build_ast(expr)
+        query = ast.queries[0]
+
+        sql_query = query.compile()
+        expected = """\
+SELECT `g`
+FROM (
+  SELECT `g`, sum(`f`) AS `total`
+  FROM alltypes
+  WHERE `c` > 0
+  GROUP BY 1
+) t0"""
+
+        assert sql_query == expected
+
+        # Maybe the result handler should act on the cursor. Not sure.
+        handler = query.result_handler
+        output = DataFrame({'g': ['foo', 'bar', 'baz']})
+        assert (handler(output) == output['g']).all()
+
+    def test_complex_array_expr_projection(self):
+        # May require finding the base table and forming a projection.
+        expr = (self.table.group_by('g')
+                .aggregate([self.table.count().name('count')]))
+        expr2 = expr.g.cast('double')
+
+        query = impala.compile(expr2)
+        expected = """SELECT CAST(`g` AS double) AS `tmp`
+FROM (
+  SELECT `g`, count(*) AS `count`
+  FROM alltypes
+  GROUP BY 1
+) t0"""
+        assert query == expected
+
+    def test_scalar_exprs_no_table_refs(self):
+        expr1 = ibis.now()
+        expected1 = """\
+SELECT now() AS `tmp`"""
+
+        expr2 = ibis.literal(1) + ibis.literal(2)
+        expected2 = """\
+SELECT 1 + 2 AS `tmp`"""
+
+        cases = [
+            (expr1, expected1),
+            (expr2, expected2)
+        ]
+
+        for expr, expected in cases:
+            result = impala.compile(expr)
+            assert result == expected
+
+    def test_expr_list_no_table_refs(self):
+        exlist = ibis.api.expr_list([ibis.literal(1).name('a'),
+                                     ibis.now().name('b'),
+                                     ibis.literal(2).log().name('c')])
+        result = impala.compile(exlist)
+        expected = """\
+SELECT 1 AS `a`, now() AS `b`, ln(2) AS `c`"""
+        assert result == expected
+
+    def test_isnull_case_expr_rewrite_failure(self):
+        # #172, case expression that was not being properly converted into an
+        # aggregation
+        reduction = self.table.g.isnull().ifelse(1, 0).sum()
+
+        result = impala.compile(reduction)
+        expected = """\
+SELECT sum(CASE WHEN `g` IS NULL THEN 1 ELSE 0 END) AS `sum`
+FROM alltypes"""
+        assert result == expected
+
+
+def _get_query(expr):
+    ast = build_ast(expr)
+    return ast.queries[0]
+
+nation = api.table([
+    ('n_regionkey', 'int32'),
+    ('n_nationkey', 'int32'),
+    ('n_name', 'string')
+], 'nation')
+
+region = api.table([
+    ('r_regionkey', 'int32'),
+    ('r_name', 'string')
+], 'region')
+
+customer = api.table([
+    ('c_nationkey', 'int32'),
+    ('c_name', 'string'),
+    ('c_acctbal', 'double')
+], 'customer')
+
+
+def _table_wrapper(name, tname=None):
+    @property
+    def f(self):
+        return self._table_from_schema(name, tname)
+    return f
+
+
+class ExprTestCases(object):
+
+    _schemas = {
+        'foo': [
+            ('job', 'string'),
+            ('dept_id', 'string'),
+            ('year', 'int32'),
+            ('y', 'double')
         ],
-        kw1=(table.a + 6),
-        kw2=table.b.sum(),
-        kw3=(_.a + 7),
-        kw4=lambda _: (_.a + 8),
-        kw5=ibis.literal(9),
-        kw6=ibis.literal("ten"),
-    )
-    expected = table[
-        table,
-        (table.a + 1).name("x1"),
-        table.b.sum().name("x2"),
-        (table.a + 2).name("x3"),
-        (table.a + 3).name("x4"),
-        ibis.literal(4).name("4"),
-        ibis.literal("five").name("'five'"),
-        (table.a + 6).name("kw1"),
-        table.b.sum().name("kw2"),
-        (table.a + 7).name("kw3"),
-        (table.a + 8).name("kw4"),
-        ibis.literal(9).name("kw5"),
-        ibis.literal("ten").name("kw6"),
-    ]
-    assert_equal(expr, expected)
-
-
-def test_mutate_alter_existing_columns(table):
-    new_f = table.f * 2
-    foo = table.d * 2
-    expr = table.mutate(f=new_f, foo=foo)
-
-    expected = table[
-        "a",
-        "b",
-        "c",
-        "d",
-        "e",
-        new_f.name("f"),
-        "g",
-        "h",
-        "i",
-        "j",
-        "k",
-        foo.name("foo"),
-    ]
-
-    assert_equal(expr, expected)
-
-
-def test_replace_column():
-    tb = api.table([("a", "int32"), ("b", "double"), ("c", "string")])
-
-    expr = tb.b.cast("int32")
-    tb2 = tb.mutate(b=expr)
-    expected = tb[tb.a, expr.name("b"), tb.c]
-
-    assert_equal(tb2, expected)
-
-
-def test_filter_no_list(table):
-    pred = table.a > 5
-
-    result = table.filter(pred)
-    expected = table[pred]
-    assert_equal(result, expected)
-
-
-def test_add_predicate(table):
-    pred = table["a"] > 5
-    result = table[pred]
-    assert isinstance(result.op(), ops.Filter)
-
-
-def test_invalid_predicate(table, schema):
-    # a lookalike
-    table2 = api.table(schema, name="bar")
-    predicate = table2.a > 5
-    with pytest.raises(IntegrityError):
-        table.filter(predicate)
-
-
-def test_add_predicate_coalesce(table):
-    # Successive predicates get combined into one rather than nesting. This
-    # is mainly to enhance readability since we could handle this during
-    # expression evaluation anyway.
-    pred1 = table["a"] > 5
-    pred2 = table["b"] > 0
-
-    result = simplify(table[pred1][pred2].op()).to_expr()
-    expected = table.filter([pred1, pred2])
-    assert_equal(result, expected)
-
-    # 59, if we are not careful, we can obtain broken refs
-    subset = table[pred1]
-    result = simplify(subset.filter([subset["b"] > 0]).op()).to_expr()
-    assert_equal(result, expected)
-
-
-def test_repr_same_but_distinct_objects(con):
-    t = con.table("test1")
-    t_copy = con.table("test1")
-    table2 = t[t_copy["f"] > 0]
-
-    result = repr(table2)
-    assert result.count("DatabaseTable") == 1
-
-
-def test_filter_fusion_distinct_table_objects(con):
-    t = con.table("test1")
-    tt = con.table("test1")
-
-    expr = t[t.f > 0][t.c > 0]
-    expr2 = t[t.f > 0][tt.c > 0]
-    expr3 = t[tt.f > 0][tt.c > 0]
-    expr4 = t[tt.f > 0][t.c > 0]
-
-    assert_equal(expr, expr2)
-    assert repr(expr) == repr(expr2)
-    assert_equal(expr, expr3)
-    assert_equal(expr, expr4)
-
-
-def test_relabel():
-    table = api.table({"x": "int32", "y": "string", "z": "double"})
-
-    # Using a mapping
-    with pytest.warns(FutureWarning, match="Table.rename"):
-        res = table.relabel({"x": "x_1", "y": "y_1"}).schema()
-    sol = sch.schema({"x_1": "int32", "y_1": "string", "z": "double"})
-    assert_equal(res, sol)
-
-    # Using a function
-    with pytest.warns(FutureWarning, match="Table.rename"):
-        res = table.relabel(lambda x: None if x == "z" else f"{x}_1").schema()
-    assert_equal(res, sol)
-
-    # Using a format string
-    with pytest.warns(FutureWarning, match="Table.rename"):
-        res = table.relabel("_{name}_")
-        sol = table.relabel({"x": "_x_", "y": "_y_", "z": "_z_"})
-    assert_equal(res, sol)
-
-    # Mapping with unknown columns errors
-    with pytest.raises(com.IbisTypeError, match="'missing' is not found in table"):
-        with pytest.warns(FutureWarning, match="Table.rename"):
-            table.relabel({"missing": "oops"})
-
-
-def test_rename():
-    table = api.table({"x": "int32", "y": "string", "z": "double"})
-    sol = sch.schema({"x_1": "int32", "y_1": "string", "z": "double"})
-
-    # Using kwargs
-    res = table.rename(x_1="x", y_1="y").schema()
-    assert_equal(res, sol)
-
-    # Using a mapping
-    res = table.rename({"x_1": "x", "y_1": "y"}).schema()
-    assert_equal(res, sol)
-
-    # Using a mix
-    res = table.rename({"x_1": "x"}, y_1="y").schema()
-    assert_equal(res, sol)
-
-
-def test_rename_function():
-    table = api.table({"x": "int32", "y": "string", "z": "double"})
-
-    res = table.rename(lambda x: None if x == "z" else f"{x}_1").schema()
-    sol = sch.schema({"x_1": "int32", "y_1": "string", "z": "double"})
-    assert_equal(res, sol)
-
-    # Explicit rename takes precedence
-    res = table.rename(lambda x: f"{x}_1", z_2="z").schema()
-    sol = sch.schema({"x_1": "int32", "y_1": "string", "z_2": "double"})
-    assert_equal(res, sol)
-
-
-def test_rename_format_string():
-    t = ibis.table({"x": "int", "y": "int", "z": "int"})
-
-    res = t.rename("_{name}_")
-    sol = t.rename({"_x_": "x", "_y_": "y", "_z_": "z"})
-    assert_equal(res, sol)
-
-    with pytest.raises(ValueError, match="Format strings must"):
-        t.rename("no format string parameter")
-
-    with pytest.raises(ValueError, match="Format strings must"):
-        t.rename("{unknown} format string parameter")
-
-
-def test_rename_snake_case():
-    cases = [
-        ("cola", "cola"),
-        ("col_b", "ColB"),
-        ("col_c", "colC"),
-        ("col_d", "col-d"),
-        ("col_e", "col_e"),
-        ("column_f", " Column F "),
-        ("column_g_with_hyphens", "Column G-with-hyphens"),
-        ("col_h_notcamelcase", "Col H notCamelCase"),
-    ]
-    t = ibis.table({c: "int" for _, c in cases})
-    res = t.rename("snake_case")
-    sol = t.rename(dict(cases))
-    assert_equal(res, sol)
-
-
-def test_rename_all_caps():
-    cases = [
-        ("COLA", "cola"),
-        ("COL_B", "ColB"),
-        ("COL_C", "colC"),
-        ("COL_D", "col-d"),
-        ("COL_E", "col_e"),
-        ("COLUMN_F", " Column F "),
-        ("COLUMN_G_WITH_HYPHENS", "Column G-with-hyphens"),
-        ("COL_H_NOTCAMELCASE", "Col H notCamelCase"),
-    ]
-    t = ibis.table({c: "int" for _, c in cases})
-    res = t.rename("ALL_CAPS")
-    sol = t.rename(dict(cases))
-    assert_equal(res, sol)
-
-
-def test_limit(table):
-    limited = table.limit(10, offset=5)
-    assert limited.op().n == 10
-    assert limited.op().offset == 5
-
-
-def test_order_by(table):
-    result = table.order_by(["f"]).op()
-
-    sort_key = result.keys[0]
-
-    assert_equal(sort_key.expr, table.f.op())
-    assert sort_key.ascending
-
-    # non-list input. per #150
-    result2 = table.order_by("f").op()
-    assert_equal(result, result2)
-
-    key2 = result2.keys[0]
-    assert key2.descending is False
-
-
-def test_order_by_desc_deferred_sort_key(table):
-    result = table.group_by("g").size().order_by(ibis._[1].desc())
-
-    tmp = table.group_by("g").size()
-    expected = tmp.order_by(ibis.desc(tmp[1]))
-
-    assert_equal(result, expected)
-
-
-def test_order_by_asc_deferred_sort_key(table):
-    result = table.group_by("g").size().order_by(ibis._[1])
-
-    tmp = table.group_by("g").size()
-    expected = tmp.order_by(tmp[1])
-    expected2 = tmp.order_by(ibis.asc(tmp[1]))
-
-    assert_equal(result, expected)
-    assert_equal(result, expected2)
-
-
-# different instantiations create unique objects
-rand = ibis.random()
-
-
-@pytest.mark.parametrize(
-    ("key", "expected"),
-    [
-        param(ibis.NA, ibis.NA.op(), id="na"),
-        param(rand, rand.op(), id="random"),
-        param(1.0, ibis.literal(1.0).op(), id="float"),
-        param(ibis.literal("a"), ibis.literal("a").op(), id="string"),
-        param(ibis.literal([1, 2, 3]), ibis.literal([1, 2, 3]).op(), id="array"),
-    ],
-)
-def test_order_by_scalar(table, key, expected):
-    result = table.order_by(key)
-    assert result.op().keys == (ops.SortKey(expected),)
-
-
-@pytest.mark.parametrize(
-    ("key", "exc_type"),
-    [
-        ("bogus", com.IbisTypeError),
-        (("bogus", False), com.IbisTypeError),
-        (ibis.desc("bogus"), com.IbisTypeError),
-        (_.bogus, AttributeError),
-        (_.bogus.desc(), AttributeError),
-    ],
-)
-@pytest.mark.parametrize(
-    "expr_func",
-    [
-        param(lambda t: t, id="table"),
-        param(lambda t: t.select("a", "b"), id="selection"),
-        param(lambda t: t.group_by("a").agg(new=_.b.sum()), id="aggregation"),
-    ],
-)
-def test_order_by_nonexistent_column_errors(table, expr_func, key, exc_type):
-    # `order_by` is implemented on a few different operations, we check them
-    # all in turn here.
-    expr = expr_func(table)
-    with pytest.raises(exc_type):
-        expr.order_by(key)
-
-
-def test_slice(table):
-    expr1 = table[:5]
-    expr2 = table[:5:1]
-    expr3 = table[5:]
-    assert_equal(expr1, table.limit(5))
-    assert_equal(expr1, expr2)
-    assert_equal(expr3, table.limit(None, offset=5))
-
-    expr1 = table[2:7]
-    expr2 = table[2:7:1]
-    expr3 = table[2::1]
-    assert_equal(expr1, table.limit(5, offset=2))
-    assert_equal(expr1, expr2)
-    assert_equal(expr3, table.limit(None, offset=2))
-
-
-@pytest.mark.parametrize("step", [-1, 0, 2])
-def test_invalid_slice(table, step):
-    with pytest.raises(ValueError):
-        table[:5:step]
-
-
-def test_table_count(table):
-    result = table.count()
-    assert isinstance(result, ir.IntegerScalar)
-    assert isinstance(result.op(), ops.CountStar)
-
-
-def test_len_raises_expression_error(table):
-    with pytest.raises(com.ExpressionError):
-        len(table)
-
-
-def test_sum_expr_basics(table, int_col):
-    # Impala gives bigint for all integer types
-    result = table[int_col].sum()
-    assert isinstance(result, ir.IntegerScalar)
-    assert isinstance(result.op(), ops.Sum)
-
-
-def test_sum_expr_basics_floats(table, float_col):
-    # Impala gives double for all floating point types
-    result = table[float_col].sum()
-    assert isinstance(result, ir.FloatingScalar)
-    assert isinstance(result.op(), ops.Sum)
-
-
-def test_mean_expr_basics(table, numeric_col):
-    result = table[numeric_col].mean()
-    assert isinstance(result, ir.FloatingScalar)
-    assert isinstance(result.op(), ops.Mean)
-
-
-def test_aggregate_no_keys(table):
-    metrics = [
-        table["a"].sum().name("sum(a)"),
-        table["c"].mean().name("mean(c)"),
-    ]
-
-    # A Table, which in SQL at least will yield a table with a single
-    # row
-    result = table.aggregate(metrics)
-    assert isinstance(result, Table)
-
-
-def test_aggregate_keys_basic(table):
-    metrics = [
-        table["a"].sum().name("sum(a)"),
-        table["c"].mean().name("mean(c)"),
-    ]
-
-    # A Table, which in SQL at least will yield a table with a single
-    # row
-    result = table.aggregate(metrics, by=["g"])
-    assert isinstance(result, Table)
-
-    # it works!
-    repr(result)
-
-
-def test_aggregate_having_implicit_metric(table):
-    metric = table.f.sum().name("total")
-    by = "g"
-    having = table.c.sum() > 10
-
-    implicit_having_metric = table.aggregate(metric, by=by, having=having)
-    expected_aggregate = ops.Aggregate(
-        parent=table,
-        groups={"g": table.g},
-        metrics={"total": table.f.sum(), table.c.sum().get_name(): table.c.sum()},
-    )
-    expected_filter = ops.Filter(
-        parent=expected_aggregate,
-        predicates=[
-            ops.Greater(ops.Field(expected_aggregate, table.c.sum().get_name()), 10)
+        'bar': [
+            ('x', 'double'),
+            ('job', 'string')
         ],
-    )
-    expected_project = ops.Project(
-        parent=expected_filter,
-        values={
-            "g": ops.Field(expected_filter, "g"),
-            "total": ops.Field(expected_filter, "total"),
-        },
-    )
-    assert implicit_having_metric.op() == expected_project
-
-
-def test_agg_having_explicit_metric(table):
-    metric = table.f.sum().name("total")
-    by = "g"
-    having = table.c.sum() > 10
-
-    explicit_having_metric = table.aggregate(
-        [metric, table.c.sum().name("sum")], by=by, having=having
-    )
-    expected_aggregate = ops.Aggregate(
-        parent=table,
-        groups={"g": table.g},
-        metrics={"total": table.f.sum(), "sum": table.c.sum()},
-    )
-    expected_filter = ops.Filter(
-        parent=expected_aggregate,
-        predicates=[ops.Greater(ops.Field(expected_aggregate, "sum"), 10)],
-    )
-    assert explicit_having_metric.op() == expected_filter
-
-
-def test_aggregate_keywords(table):
-    t = table
-
-    expr = t.aggregate(foo=t.f.sum(), bar=lambda x: x.f.mean(), by="g")
-    expr2 = t.group_by("g").aggregate(foo=t.f.sum(), bar=lambda x: x.f.mean())
-    expected = t.aggregate([t.f.sum().name("foo"), t.f.mean().name("bar")], by="g")
-
-    assert_equal(expr, expected)
-    assert_equal(expr2, expected)
-
-
-def test_select_on_literals(table):
-    # literal ints and strings are column indices, everything else is a value
-    expr1 = table.select(col1=True, col2=1, col3="a")
-    expr2 = table.select(col1=ibis.literal(True), col2=ibis.literal(1), col3=table.a)
-    assert expr1.equals(expr2)
-
-
-def test_filter_on_literal_boolean(table):
-    expr1 = table.filter(True)
-    expr2 = table.filter(ibis.literal(True))
-    assert expr1.equals(expr2)
-
-
-def test_filter_on_literal_string_is_column(table):
-    expr1 = table.filter("h")
-    expr2 = table.filter(table.h)
-    assert expr1.equals(expr2)
-
-
-def test_filter_on_literal_then_aggregate(table):
-    # Mostly just a smoketest, this used to error on construction
-    expr = table.filter(ibis.literal(True)).agg(lambda t: t.a.sum().name("total"))
-    assert expr.columns == ["total"]
-
-
-def test_group_by_having_api(table):
-    # #154, add a HAVING post-predicate in a composable way
-    metric = table.f.sum().name("foo")
-    postp = table.d.mean() > 1
-    expr = table.group_by("g").having(postp).aggregate(metric)
-
-    agg = ops.Aggregate(
-        parent=table,
-        groups={"g": table.g},
-        metrics={"foo": table.f.sum(), "Mean(d)": table.d.mean()},
-    ).to_expr()
-    filt = ops.Filter(
-        parent=agg,
-        predicates=[agg["Mean(d)"] > 1],
-    ).to_expr()
-    proj = ops.Project(
-        parent=filt,
-        values={"g": filt.g, "foo": filt.foo},
-    )
-    assert expr.op() == proj
-
-
-def test_group_by_kwargs(table):
-    t = table
-    expr = t.group_by(["f", t.h], z="g", z2=t.d).aggregate(t.d.mean().name("foo"))
-    expected = t.group_by(["f", t.h, t.g.name("z"), t.d.name("z2")]).aggregate(
-        t.d.mean().name("foo")
-    )
-    assert_equal(expr, expected)
-
-
-def test_group_by_nargs(table):
-    t = table
-    by = ["f", t.h]
-    e1 = t.group_by(by, z="g").aggregate(t.d.mean().name("foo"))
-    e2 = t.group_by(*by, z="g").aggregate(t.d.mean().name("foo"))
-    assert_equal(e1, e2)
-
-
-def test_compound_aggregate_expr(table):
-    # See ibis #24
-    compound_expr = (table["a"].sum() / table["a"].mean()).name("foo")
-
-    # Validates internally
-    table.aggregate([compound_expr])
-
-
-def test_groupby_convenience(table):
-    metrics = [table.f.sum().name("total")]
-
-    expr = table.group_by("g").aggregate(metrics)
-    expected = table.aggregate(metrics, by=["g"])
-    assert_equal(expr, expected)
-
-    group_expr = table.g.cast("double").name("g")
-    expr = table.group_by(group_expr).aggregate(metrics)
-    expected = table.aggregate(metrics, by=[group_expr])
-    assert_equal(expr, expected)
-
-
-@pytest.mark.parametrize("group", [[], (), None])
-def test_group_by_nothing(table, group):
-    with pytest.raises(com.IbisInputError):
-        table.group_by(group)
-
-
-def test_group_by_count_size(table):
-    # #148, convenience for interactive use, and so forth
-    result1 = table.group_by("g").size()
-    result2 = table.group_by("g").count()
-
-    expected = table.group_by("g").aggregate(table.count())
-
-    assert_equal(result1, expected)
-    assert_equal(result2, expected)
-
-
-def test_group_by_column_select_api(table):
-    grouped = table.group_by("g")
-
-    result = grouped.f.sum()
-    expected = grouped.aggregate(table.f.sum().name("sum(f)"))
-    assert_equal(result, expected)
-
-    supported_functions = ["sum", "mean", "count", "size", "max", "min"]
-
-    # make sure they all work
-    for fn in supported_functions:
-        getattr(grouped.f, fn)()
-
-
-def test_value_counts_convenience(table):
-    # #152
-    result = table.g.value_counts()
-    expected = table.select("g").group_by("g").aggregate(g_count=lambda t: t.count())
-
-    assert_equal(result, expected)
-
-
-def test_isin_value_counts(table):
-    # #157, this code path was untested before
-    bool_clause = table.g.notin(["1", "4", "7"])
-    # it works!
-    bool_clause.name("notin").value_counts()
-
-
-def test_value_counts_unnamed_expr(con):
-    nation = con.table("tpch_nation")
-
-    expr = nation.n_name.lower().value_counts()
-    expected = nation.n_name.lower().name("Lowercase(n_name)").value_counts()
-    assert_equal(expr, expected)
-
-
-def test_aggregate_unnamed_expr(con):
-    nation = con.table("tpch_nation")
-    expr = nation.n_name.lower().left(1)
-
-    agg = nation.group_by(expr).aggregate(nation.count().name("metric"))
-    schema = agg.schema()
-    assert schema.names == ("Substring(Lowercase(n_name), 0, 1)", "metric")
-    assert schema.types == (dt.string, dt.int64)
-
-
-def test_join_no_predicate_list(con):
-    region = con.table("tpch_region")
-    nation = con.table("tpch_nation")
-
-    pred = region.r_regionkey == nation.n_regionkey
-    joined = region.inner_join(nation, pred)
-
-    with join_tables(joined) as (r1, r2):
-        expected = ops.JoinChain(
-            first=r1,
-            rest=[ops.JoinLink("inner", r2, [r1.r_regionkey == r2.n_regionkey])],
-            values={
-                "r_regionkey": r1.r_regionkey,
-                "r_name": r1.r_name,
-                "r_comment": r1.r_comment,
-                "n_nationkey": r2.n_nationkey,
-                "n_name": r2.n_name,
-                "n_regionkey": r2.n_regionkey,
-                "n_comment": r2.n_comment,
-            },
-        )
-        assert joined.op() == expected
-
-
-def test_join_deferred(con):
-    region = con.table("tpch_region")
-    nation = con.table("tpch_nation")
-
-    res = region.join(nation, _.r_regionkey == nation.n_regionkey)
-
-    with join_tables(res) as (r1, r2):
-        expected = ops.JoinChain(
-            first=r1,
-            rest=[ops.JoinLink("inner", r2, [r1.r_regionkey == r2.n_regionkey])],
-            values={
-                "r_regionkey": r1.r_regionkey,
-                "r_name": r1.r_name,
-                "r_comment": r1.r_comment,
-                "n_nationkey": r2.n_nationkey,
-                "n_name": r2.n_name,
-                "n_regionkey": r2.n_regionkey,
-                "n_comment": r2.n_comment,
-            },
-        )
-        assert res.op() == expected
+        't1': [
+            ('key1', 'string'),
+            ('key2', 'string'),
+            ('value1', 'double')
+        ],
+        't2': [
+            ('key1', 'string'),
+            ('key2', 'string')
+        ]
+    }
+
+    def _table_from_schema(self, name, tname=None):
+        tname = tname or name
+        return api.table(self._schemas[name], tname)
+
+    def _case_multiple_joins(self):
+        t1 = self.con.table('star1')
+        t2 = self.con.table('star2')
+        t3 = self.con.table('star3')
+
+        predA = t1['foo_id'] == t2['foo_id']
+        predB = t1['bar_id'] == t3['bar_id']
+
+        what = (t1.left_join(t2, [predA])
+                .inner_join(t3, [predB])
+                .projection([t1, t2['value1'], t3['value2']]))
+        return what
+
+    def _case_join_between_joins(self):
+        t1 = api.table([
+            ('key1', 'string'),
+            ('key2', 'string'),
+            ('value1', 'double'),
+        ], 'first')
+
+        t2 = api.table([
+            ('key1', 'string'),
+            ('value2', 'double'),
+        ], 'second')
+
+        t3 = api.table([
+            ('key2', 'string'),
+            ('key3', 'string'),
+            ('value3', 'double'),
+        ], 'third')
+
+        t4 = api.table([
+            ('key3', 'string'),
+            ('value4', 'double')
+        ], 'fourth')
+
+        left = t1.inner_join(t2, [('key1', 'key1')])[t1, t2.value2]
+        right = t3.inner_join(t4, [('key3', 'key3')])[t3, t4.value4]
+
+        joined = left.inner_join(right, [('key2', 'key2')])
+
+        # At one point, the expression simplification was resulting in bad refs
+        # here (right.value3 referencing the table inside the right join)
+        exprs = [left, right.value3, right.value4]
+        projected = joined.projection(exprs)
+
+        return projected
+
+    def _case_join_just_materialized(self):
+        t1 = self.con.table('tpch_nation')
+        t2 = self.con.table('tpch_region')
+        t3 = self.con.table('tpch_customer')
+
+        # GH #491
+        return (t1.inner_join(t2, t1.n_regionkey == t2.r_regionkey)
+                .inner_join(t3, t1.n_nationkey == t3.c_nationkey))
+
+    def _case_semi_anti_joins(self):
+        t1 = self.con.table('star1')
+        t2 = self.con.table('star2')
+
+        sj = t1.semi_join(t2, [t1.foo_id == t2.foo_id])[[t1]]
+        aj = t1.anti_join(t2, [t1.foo_id == t2.foo_id])[[t1]]
+
+        return sj, aj
+
+    def _case_self_reference_simple(self):
+        t1 = self.con.table('star1')
+        return t1.view()
+
+    def _case_self_reference_join(self):
+        t1 = self.con.table('star1')
+        t2 = t1.view()
+        return t1.inner_join(t2, [t1.foo_id == t2.bar_id])[[t1]]
+
+    def _case_join_projection_subquery_bug(self):
+        # From an observed bug, derived from tpch tables
+        geo = (nation.inner_join(region, [('n_regionkey', 'r_regionkey')])
+               [nation.n_nationkey,
+                nation.n_name.name('nation'),
+                region.r_name.name('region')])
+
+        expr = (geo.inner_join(customer, [('n_nationkey', 'c_nationkey')])
+                [customer, geo])
+
+        return expr
+
+    def _case_where_simple_comparisons(self):
+        t1 = self.con.table('star1')
+
+        what = t1.filter([t1.f > 0, t1.c < t1.f * 2])
+
+        return what
+
+    def _case_where_with_join(self):
+        t1 = self.con.table('star1')
+        t2 = self.con.table('star2')
+
+        # This also tests some cases of predicate pushdown
+        e1 = (t1.inner_join(t2, [t1.foo_id == t2.foo_id])
+              .projection([t1, t2.value1, t2.value3])
+              .filter([t1.f > 0, t2.value3 < 1000]))
+
+        e2 = (t1.inner_join(t2, [t1.foo_id == t2.foo_id])
+              .filter([t1.f > 0, t2.value3 < 1000])
+              .projection([t1, t2.value1, t2.value3]))
+
+        return e1, e2
+
+    def _case_subquery_used_for_self_join(self):
+        # There could be cases that should look in SQL like
+        # WITH t0 as (some subquery)
+        # select ...
+        # from t0 t1
+        #   join t0 t2
+        #     on t1.kind = t2.subkind
+        # ...
+        # However, the Ibis code will simply have an expression (projection or
+        # aggregation, say) built on top of the subquery expression, so we need
+        # to extract the subquery unit (we see that it appears multiple times
+        # in the tree).
+        t = self.con.table('alltypes')
+
+        agged = t.aggregate([t.f.sum().name('total')], by=['g', 'a', 'b'])
+        view = agged.view()
+        metrics = [(agged.total - view.total).max().name('metric')]
+        expr = (agged.inner_join(view, [agged.a == view.b])
+                .aggregate(metrics, by=[agged.g]))
+
+        return expr
+
+    def _case_subquery_factor_correlated_subquery(self):
+        region = self.con.table('tpch_region')
+        nation = self.con.table('tpch_nation')
+        customer = self.con.table('tpch_customer')
+        orders = self.con.table('tpch_orders')
+
+        fields_of_interest = [customer,
+                              region.r_name.name('region'),
+                              orders.o_totalprice.name('amount'),
+                              orders.o_orderdate
+                              .cast('timestamp').name('odate')]
+
+        tpch = (region.join(nation, region.r_regionkey == nation.n_regionkey)
+                .join(customer, customer.c_nationkey == nation.n_nationkey)
+                .join(orders, orders.o_custkey == customer.c_custkey)
+                [fields_of_interest])
+
+        # Self-reference + correlated subquery complicates things
+        t2 = tpch.view()
+        conditional_avg = t2[t2.region == tpch.region].amount.mean()
+        amount_filter = tpch.amount > conditional_avg
+
+        return tpch[amount_filter].limit(10)
+
+    def _case_self_join_subquery_distinct_equal(self):
+        region = self.con.table('tpch_region')
+        nation = self.con.table('tpch_nation')
+
+        j1 = (region.join(nation, region.r_regionkey == nation.n_regionkey)
+              [region, nation])
+
+        j2 = (region.join(nation, region.r_regionkey == nation.n_regionkey)
+              [region, nation].view())
+
+        expr = (j1.join(j2, j1.r_regionkey == j2.r_regionkey)
+                [j1.r_name, j2.n_name])
+
+        return expr
+
+    def _case_cte_factor_distinct_but_equal(self):
+        t = self.con.table('alltypes')
+        tt = self.con.table('alltypes')
+
+        expr1 = t.group_by('g').aggregate(t.f.sum().name('metric'))
+        expr2 = tt.group_by('g').aggregate(tt.f.sum().name('metric')).view()
+
+        expr = expr1.join(expr2, expr1.g == expr2.g)[[expr1]]
+
+        return expr
+
+    def _case_tpch_self_join_failure(self):
+        # duplicating the integration test here
+
+        region = self.con.table('tpch_region')
+        nation = self.con.table('tpch_nation')
+        customer = self.con.table('tpch_customer')
+        orders = self.con.table('tpch_orders')
+
+        fields_of_interest = [
+            region.r_name.name('region'),
+            nation.n_name.name('nation'),
+            orders.o_totalprice.name('amount'),
+            orders.o_orderdate.cast('timestamp').name('odate')]
+
+        joined_all = (
+            region.join(nation, region.r_regionkey == nation.n_regionkey)
+            .join(customer, customer.c_nationkey == nation.n_nationkey)
+            .join(orders, orders.o_custkey == customer.c_custkey)
+            [fields_of_interest])
+
+        year = joined_all.odate.year().name('year')
+        total = joined_all.amount.sum().cast('double').name('total')
+        annual_amounts = (joined_all
+                          .group_by(['region', year])
+                          .aggregate(total))
+
+        current = annual_amounts
+        prior = annual_amounts.view()
+
+        yoy_change = (current.total - prior.total).name('yoy_change')
+        yoy = (current.join(prior, current.year == (prior.year - 1))
+               [current.region, current.year, yoy_change])
+        return yoy
+
+    def _case_subquery_in_filter_predicate(self):
+        # E.g. comparing against some scalar aggregate value. See Ibis #43
+        t1 = self.con.table('star1')
+
+        pred = t1.f > t1.f.mean()
+        expr = t1[pred]
+
+        # This brought out another expression rewriting bug, since the filtered
+        # table isn't found elsewhere in the expression.
+        pred2 = t1.f > t1[t1.foo_id == 'foo'].f.mean()
+        expr2 = t1[pred2]
+
+        return expr, expr2
+
+    def _case_filter_subquery_derived_reduction(self):
+        t1 = self.con.table('star1')
+
+        # Reduction can be nested inside some scalar expression
+        pred3 = t1.f > t1[t1.foo_id == 'foo'].f.mean().log()
+        pred4 = t1.f > (t1[t1.foo_id == 'foo'].f.mean().log() + 1)
+
+        expr3 = t1[pred3]
+        expr4 = t1[pred4]
+
+        return expr3, expr4
+
+    def _case_topk_operation(self):
+        # TODO: top K with filter in place
+
+        table = api.table([
+            ('foo', 'string'),
+            ('bar', 'string'),
+            ('city', 'string'),
+            ('v1', 'double'),
+            ('v2', 'double'),
+        ], 'tbl')
+
+        what = table.city.topk(10, by=table.v2.mean())
+        e1 = table[what]
+
+        # Test the default metric (count)
+        what = table.city.topk(10)
+        e2 = table[what]
+
+        return e1, e2
+
+    def _case_simple_aggregate_query(self):
+        t1 = self.con.table('star1')
+        cases = [
+            t1.aggregate([t1['f'].sum().name('total')],
+                         [t1['foo_id']]),
+            t1.aggregate([t1['f'].sum().name('total')],
+                         ['foo_id', 'bar_id'])
+        ]
+
+        return cases
+
+    def _case_aggregate_having(self):
+        # Filtering post-aggregation predicate
+        t1 = self.con.table('star1')
+
+        total = t1.f.sum().name('total')
+        metrics = [total]
+
+        e1 = t1.aggregate(metrics, by=['foo_id'], having=[total > 10])
+        e2 = t1.aggregate(metrics, by=['foo_id'], having=[t1.count() > 100])
+
+        return e1, e2
+
+    def _case_aggregate_count_joined(self):
+        # count on more complicated table
+        region = self.con.table('tpch_region')
+        nation = self.con.table('tpch_nation')
+        join_expr = region.r_regionkey == nation.n_regionkey
+        joined = region.inner_join(nation, join_expr)
+        table_ref = joined[nation, region.r_name.name('region')]
+
+        return table_ref.count()
+
+    def _case_sort_by(self):
+        table = self.con.table('star1')
+
+        return [
+            table.sort_by('f'),
+            table.sort_by(('f', 0)),
+            table.sort_by(['c', ('f', 0)])
+        ]
+
+    def _case_limit(self):
+        star1 = self.con.table('star1')
+
+        cases = [
+            star1.limit(10),
+            star1.limit(10, offset=5),
+            star1[star1.f > 0].limit(10),
+
+            # Semantically, this should produce a subquery
+            star1.limit(10)[lambda x: x.f > 0]
+        ]
+
+        return cases
+
+    foo = _table_wrapper('foo')
+    bar = _table_wrapper('bar')
+    t1 = _table_wrapper('t1', 'foo')
+    t2 = _table_wrapper('t2', 'bar')
+
+    def _case_where_uncorrelated_subquery(self):
+        return self.foo[self.foo.job.isin(self.bar.job)]
+
+    def _case_where_correlated_subquery(self):
+        t1 = self.foo
+        t2 = t1.view()
+
+        stat = t2[t1.dept_id == t2.dept_id].y.mean()
+        return t1[t1.y > stat]
+
+    def _case_exists(self):
+        t1, t2 = self.t1, self.t2
+
+        cond = (t1.key1 == t2.key1).any()
+        expr = t1[cond]
+
+        cond2 = ((t1.key1 == t2.key1) & (t2.key2 == 'foo')).any()
+        expr2 = t1[cond2]
+
+        return expr, expr2
+
+    def _case_not_exists(self):
+        t1, t2 = self.t1, self.t2
+
+        cond = (t1.key1 == t2.key1).any()
+        return t1[-cond]
+
+    def _case_join_with_limited_table(self):
+        t1 = self.con.table('star1')
+        t2 = self.con.table('star2')
+
+        limited = t1.limit(100)
+        joined = (limited.inner_join(t2, [limited.foo_id == t2.foo_id])
+                  [[limited]])
+        return joined
+
+    def _case_union(self, distinct=False):
+        table = self.con.table('functional_alltypes')
+
+        t1 = (table[table.int_col > 0]
+              [table.string_col.name('key'),
+               table.float_col.cast('double').name('value')])
+        t2 = (table[table.int_col <= 0]
+                   [table.string_col.name('key'),
+                    table.double_col.name('value')])
+
+        expr = t1.union(t2, distinct=distinct)
+
+        return expr
+
+    def _case_simple_case(self):
+        t = self.con.table('alltypes')
+        return (t.g.case()
+                .when('foo', 'bar')
+                .when('baz', 'qux')
+                .else_('default')
+                .end())
+
+    def _case_search_case(self):
+        t = self.con.table('alltypes')
+        return (ibis.case()
+                .when(t.f > 0, t.d * 2)
+                .when(t.c < 0, t.a * 2)
+                .end())
+
+    def _case_self_reference_in_exists(self):
+        t = self.con.table('functional_alltypes')
+        t2 = t.view()
+
+        cond = (t.string_col == t2.string_col).any()
+        semi = t[cond]
+        anti = t[-cond]
+
+        return semi, anti
+
+    def _case_self_reference_limit_exists(self):
+        alltypes = self.con.table('functional_alltypes')
+        t = alltypes.limit(100)
+        t2 = t.view()
+        return t[-(t.string_col == t2.string_col).any()]
+
+    def _case_limit_cte_extract(self):
+        alltypes = self.con.table('functional_alltypes')
+        t = alltypes.limit(100)
+        t2 = t.view()
+        return t.join(t2).projection(t)
+
+    def _case_subquery_aliased(self):
+        t1 = self.con.table('star1')
+        t2 = self.con.table('star2')
+
+        agged = t1.aggregate([t1.f.sum().name('total')], by=['foo_id'])
+        what = (agged.inner_join(t2, [agged.foo_id == t2.foo_id])
+                [agged, t2.value1])
+
+        return what
+
+    def _case_filter_self_join_analysis_bug(self):
+        purchases = ibis.table([('region', 'string'),
+                                ('kind', 'string'),
+                                ('user', 'int64'),
+                                ('amount', 'double')], 'purchases')
+
+        metric = purchases.amount.sum().name('total')
+        agged = (purchases.group_by(['region', 'kind'])
+                 .aggregate(metric))
+
+        left = agged[agged.kind == 'foo']
+        right = agged[agged.kind == 'bar']
+
+        joined = left.join(right, left.region == right.region)
+        result = joined[left.region,
+                        (left.total - right.total).name('diff')]
+
+        return result, purchases
+
+    def _case_projection_fuse_filter(self):
+        # Probably test this during the evaluation phase. In SQL, "fusable"
+        # table operations will be combined together into a single select
+        # statement
+        #
+        # see ibis #71 for more on this
+
+        t = ibis.table([
+            ('a', 'int8'),
+            ('b', 'int16'),
+            ('c', 'int32'),
+            ('d', 'int64'),
+            ('e', 'float'),
+            ('f', 'double'),
+            ('g', 'string'),
+            ('h', 'boolean')
+        ], 'foo')
+
+        proj = t['a', 'b', 'c']
+
+        # Rewrite a little more aggressively here
+        expr1 = proj[t.a > 0]
+
+        # at one point these yielded different results
+        filtered = t[t.a > 0]
+
+        expr2 = filtered[t.a, t.b, t.c]
+        expr3 = filtered.projection(['a', 'b', 'c'])
+
+        return expr1, expr2, expr3
+
+
+class TestSelectSQL(unittest.TestCase, ExprTestCases):
+
+    @classmethod
+    def setUpClass(cls):
+        cls.con = MockConnection()
+
+    def _compare_sql(self, expr, expected):
+        result = to_sql(expr)
+        assert result == expected
+
+    def test_nameless_table(self):
+        # Ensure that user gets some kind of sensible error
+        nameless = api.table([('key', 'string')])
+        self.assertRaises(com.RelationError, to_sql, nameless)
+
+        with_name = api.table([('key', 'string')], name='baz')
+        result = to_sql(with_name)
+        assert result == 'SELECT *\nFROM baz'
+
+    def test_physical_table_reference_translate(self):
+        # If an expression's table leaves all reference database tables, verify
+        # we translate correctly
+        table = self.con.table('alltypes')
+
+        query = _get_query(table)
+        sql_string = query.compile()
+        expected = "SELECT *\nFROM alltypes"
+        assert sql_string == expected
+
+    def test_simple_joins(self):
+        t1 = self.con.table('star1')
+        t2 = self.con.table('star2')
+
+        pred = t1['foo_id'] == t2['foo_id']
+        pred2 = t1['bar_id'] == t2['foo_id']
+        cases = [
+            (t1.inner_join(t2, [pred])[[t1]],
+             """SELECT t0.*
+FROM star1 t0
+  INNER JOIN star2 t1
+    ON t0.`foo_id` = t1.`foo_id`"""),
+            (t1.left_join(t2, [pred])[[t1]],
+             """SELECT t0.*
+FROM star1 t0
+  LEFT OUTER JOIN star2 t1
+    ON t0.`foo_id` = t1.`foo_id`"""),
+            (t1.outer_join(t2, [pred])[[t1]],
+             """SELECT t0.*
+FROM star1 t0
+  FULL OUTER JOIN star2 t1
+    ON t0.`foo_id` = t1.`foo_id`"""),
+            # multiple predicates
+            (t1.inner_join(t2, [pred, pred2])[[t1]],
+             """SELECT t0.*
+FROM star1 t0
+  INNER JOIN star2 t1
+    ON t0.`foo_id` = t1.`foo_id` AND
+       t0.`bar_id` = t1.`foo_id`"""),
+        ]
+
+        for expr, expected_sql in cases:
+            result_sql = to_sql(expr)
+            assert result_sql == expected_sql
+
+    def test_multiple_joins(self):
+        what = self._case_multiple_joins()
+
+        result_sql = to_sql(what)
+        expected_sql = """SELECT t0.*, t1.`value1`, t2.`value2`
+FROM star1 t0
+  LEFT OUTER JOIN star2 t1
+    ON t0.`foo_id` = t1.`foo_id`
+  INNER JOIN star3 t2
+    ON t0.`bar_id` = t2.`bar_id`"""
+        assert result_sql == expected_sql
+
+    def test_join_between_joins(self):
+        projected = self._case_join_between_joins()
+
+        result = to_sql(projected)
+        expected = """SELECT t0.*, t1.`value3`, t1.`value4`
+FROM (
+  SELECT t2.*, t3.`value2`
+  FROM `first` t2
+    INNER JOIN second t3
+      ON t2.`key1` = t3.`key1`
+) t0
+  INNER JOIN (
+    SELECT t2.*, t3.`value4`
+    FROM third t2
+      INNER JOIN fourth t3
+        ON t2.`key3` = t3.`key3`
+  ) t1
+    ON t0.`key2` = t1.`key2`"""
+        assert result == expected
+
+    def test_join_just_materialized(self):
+        joined = self._case_join_just_materialized()
+        result = to_sql(joined)
+        expected = """SELECT *
+FROM tpch_nation t0
+  INNER JOIN tpch_region t1
+    ON t0.`n_regionkey` = t1.`r_regionkey`
+  INNER JOIN tpch_customer t2
+    ON t0.`n_nationkey` = t2.`c_nationkey`"""
+        assert result == expected
+
+        result = to_sql(joined.materialize())
+        assert result == expected
+
+    def test_join_no_predicates_for_impala(self):
+        # Impala requires that joins without predicates be written explicitly
+        # as CROSS JOIN, since result sets can accidentally get too large if a
+        # query is executed before predicates are written
+        t1 = self.con.table('star1')
+        t2 = self.con.table('star2')
+
+        joined2 = t1.cross_join(t2)[[t1]]
+
+        expected = """SELECT t0.*
+FROM star1 t0
+  CROSS JOIN star2 t1"""
+        result2 = to_sql(joined2)
+        assert result2 == expected
+
+        for jtype in ['inner_join', 'left_join', 'outer_join']:
+            joined = getattr(t1, jtype)(t2)[[t1]]
+
+            result = to_sql(joined)
+            assert result == expected
+
+    def test_semi_anti_joins(self):
+        sj, aj = self._case_semi_anti_joins()
+
+        result = to_sql(sj)
+        expected = """SELECT t0.*
+FROM star1 t0
+  LEFT SEMI JOIN star2 t1
+    ON t0.`foo_id` = t1.`foo_id`"""
+        assert result == expected
+
+        result = to_sql(aj)
+        expected = """SELECT t0.*
+FROM star1 t0
+  LEFT ANTI JOIN star2 t1
+    ON t0.`foo_id` = t1.`foo_id`"""
+        assert result == expected
+
+    def test_self_reference_simple(self):
+        expr = self._case_self_reference_simple()
+
+        result_sql = to_sql(expr)
+        expected_sql = "SELECT *\nFROM star1"
+        assert result_sql == expected_sql
+
+    def test_join_self_reference(self):
+        result = self._case_self_reference_join()
+
+        result_sql = to_sql(result)
+        expected_sql = """SELECT t0.*
+FROM star1 t0
+  INNER JOIN star1 t1
+    ON t0.`foo_id` = t1.`bar_id`"""
+        assert result_sql == expected_sql
+
+    def test_join_projection_subquery_broken_alias(self):
+        expr = self._case_join_projection_subquery_bug()
+
+        result = to_sql(expr)
+        expected = """SELECT t1.*, t0.*
+FROM (
+  SELECT t2.`n_nationkey`, t2.`n_name` AS `nation`, t3.`r_name` AS `region`
+  FROM nation t2
+    INNER JOIN region t3
+      ON t2.`n_regionkey` = t3.`r_regionkey`
+) t0
+  INNER JOIN customer t1
+    ON t0.`n_nationkey` = t1.`c_nationkey`"""
+        assert result == expected
+
+    def test_where_simple_comparisons(self):
+        what = self._case_where_simple_comparisons()
+        result = to_sql(what)
+        expected = """SELECT *
+FROM star1
+WHERE `f` > 0 AND
+      `c` < (`f` * 2)"""
+        assert result == expected
+
+    def test_where_in_array_literal(self):
+        # e.g.
+        # where string_col in (v1, v2, v3)
+        raise unittest.SkipTest
+
+    def test_where_with_join(self):
+        e1, e2 = self._case_where_with_join()
+
+        expected_sql = """SELECT t0.*, t1.`value1`, t1.`value3`
+FROM star1 t0
+  INNER JOIN star2 t1
+    ON t0.`foo_id` = t1.`foo_id`
+WHERE t0.`f` > 0 AND
+      t1.`value3` < 1000"""
+
+        result_sql = to_sql(e1)
+        assert result_sql == expected_sql
+
+        result2_sql = to_sql(e2)
+        assert result2_sql == expected_sql
+
+    def test_where_no_pushdown_possible(self):
+        t1 = self.con.table('star1')
+        t2 = self.con.table('star2')
+
+        joined = (t1.inner_join(t2, [t1.foo_id == t2.foo_id])
+                  [t1, (t1.f - t2.value1).name('diff')])
+
+        filtered = joined[joined.diff > 1]
+
+        # TODO: I'm not sure if this is exactly what we want
+        expected_sql = """SELECT *
+FROM (
+  SELECT t0.*, t0.`f` - t1.`value1` AS `diff`
+  FROM star1 t0
+    INNER JOIN star2 t1
+      ON t0.`foo_id` = t1.`foo_id`
+  WHERE t0.`f` > 0 AND
+        t1.`value3` < 1000
+)
+WHERE `diff` > 1"""
 
+        raise unittest.SkipTest
 
-def test_join_invalid_predicate(con):
-    region = con.table("tpch_region")
-    nation = con.table("tpch_nation")
-
-    with pytest.raises(com.InputTypeError):
-        region.inner_join(nation, object())
-
-
-def test_asof_join():
-    left = ibis.table([("time", "int32"), ("value", "double")])
-    right = ibis.table([("time", "int32"), ("value2", "double")])
-    joined = api.asof_join(left, right, "time")
-
-    assert joined.columns == [
-        "time",
-        "value",
-        "time_right",
-        "value2",
-    ]
-    pred = joined.op().rest[0].predicates[0]
-    assert pred.left.name == pred.right.name == "time"
-
-
-# TODO(kszucs): ensure the correctness of the pd.merge_asof(by=...) argument emulation
-def test_asof_join_with_by():
-    left = ibis.table([("time", "int32"), ("key", "int32"), ("value", "double")])
-    right = ibis.table([("time", "int32"), ("key", "int32"), ("value2", "double")])
-
-    join_without_by = api.asof_join(left, right, "time")
-    with join_tables(join_without_by) as (r1, r2):
-        r2 = join_without_by.op().rest[0].table.to_expr()
-        expected = ops.JoinChain(
-            first=r1,
-            rest=[ops.JoinLink("asof", r2, [r1.time >= r2.time])],
-            values={
-                "time": r1.time,
-                "key": r1.key,
-                "value": r1.value,
-                "time_right": r2.time,
-                "key_right": r2.key,
-                "value2": r2.value2,
-            },
-        )
-        assert join_without_by.op() == expected
+        result_sql = to_sql(filtered)
+        assert result_sql == expected_sql
 
-    join_with_predicates = api.asof_join(left, right, "time", predicates="key")
-    with join_tables(join_with_predicates) as (r1, r2):
-        expected = ops.JoinChain(
-            first=r1,
-            rest=[
-                ops.JoinLink("asof", r2, [r1.time >= r2.time, r1.key == r2.key]),
-            ],
-            values={
-                "time": r1.time,
-                "key": r1.key,
-                "value": r1.value,
-                "time_right": r2.time,
-                "key_right": r2.key,
-                "value2": r2.value2,
-            },
+    def test_where_with_between(self):
+        t = self.con.table('alltypes')
+
+        what = t.filter([t.a > 0, t.f.between(0, 1)])
+        result = to_sql(what)
+        expected = """SELECT *
+FROM alltypes
+WHERE `a` > 0 AND
+      `f` BETWEEN 0 AND 1"""
+        assert result == expected
+
+    def test_where_analyze_scalar_op(self):
+        # root cause of #310
+
+        table = self.con.table('functional_alltypes')
+
+        expr = (table.filter([table.timestamp_col <
+                             (ibis.timestamp('2010-01-01') + ibis.month(3)),
+                             table.timestamp_col < (ibis.now() +
+                                                    ibis.day(10))])
+                .count())
+
+        result = to_sql(expr)
+        expected = """\
+SELECT count(*) AS `count`
+FROM functional_alltypes
+WHERE `timestamp_col` < months_add('2010-01-01 00:00:00', 3) AND
+      `timestamp_col` < days_add(now(), 10)"""
+        assert result == expected
+
+    def test_bug_duplicated_where(self):
+        # GH #539
+        table = self.con.table('airlines')
+
+        t = table['arrdelay', 'dest']
+        expr = (t.group_by('dest')
+                .mutate(dest_avg=t.arrdelay.mean(),
+                        dev=t.arrdelay - t.arrdelay.mean()))
+
+        worst = expr[expr.dev.notnull()].sort_by(ibis.desc('dev')).limit(10)
+        result = to_sql(worst)
+        expected = """\
+SELECT *
+FROM (
+  SELECT `arrdelay`, `dest`,
+         avg(`arrdelay`) OVER (PARTITION BY `dest`) AS `dest_avg`,
+         `arrdelay` - avg(`arrdelay`) OVER (PARTITION BY `dest`) AS `dev`
+  FROM airlines
+) t0
+WHERE `dev` IS NOT NULL
+ORDER BY `dev` DESC
+LIMIT 10"""
+        assert result == expected
+
+    def test_simple_aggregate_query(self):
+        expected = [
+            """SELECT `foo_id`, sum(`f`) AS `total`
+FROM star1
+GROUP BY 1""",
+            """SELECT `foo_id`, `bar_id`, sum(`f`) AS `total`
+FROM star1
+GROUP BY 1, 2"""
+        ]
+
+        cases = self._case_simple_aggregate_query()
+        for expr, expected_sql in zip(cases, expected):
+            result_sql = to_sql(expr)
+            assert result_sql == expected_sql
+
+    def test_aggregate_having(self):
+        e1, e2 = self._case_aggregate_having()
+
+        result = to_sql(e1)
+        expected = """SELECT `foo_id`, sum(`f`) AS `total`
+FROM star1
+GROUP BY 1
+HAVING sum(`f`) > 10"""
+        assert result == expected
+
+        result = to_sql(e2)
+        expected = """SELECT `foo_id`, sum(`f`) AS `total`
+FROM star1
+GROUP BY 1
+HAVING count(*) > 100"""
+        assert result == expected
+
+    def test_aggregate_table_count_metric(self):
+        expr = self.con.table('star1').count()
+
+        result = to_sql(expr)
+        expected = """SELECT count(*) AS `count`
+FROM star1"""
+        assert result == expected
+
+    def test_aggregate_count_joined(self):
+        expr = self._case_aggregate_count_joined()
+
+        result = to_sql(expr)
+        expected = """SELECT count(*) AS `count`
+FROM (
+  SELECT t2.*, t1.`r_name` AS `region`
+  FROM tpch_region t1
+    INNER JOIN tpch_nation t2
+      ON t1.`r_regionkey` = t2.`n_regionkey`
+) t0"""
+        assert result == expected
+
+    def test_expr_template_field_name_binding(self):
+        # Given an expression with no concrete links to actual database tables,
+        # indicate a mapping between the distinct unbound table leaves of the
+        # expression and some database tables with compatible schemas but
+        # potentially different column names
+        pass
+
+    def test_no_aliases_needed(self):
+        table = api.table([
+            ('key1', 'string'),
+            ('key2', 'string'),
+            ('value', 'double')
+        ])
+
+        expr = table.aggregate([table['value'].sum().name('total')],
+                               by=['key1', 'key2'])
+
+        query = _get_query(expr)
+        context = query.context
+        assert not context.need_aliases()
+
+    def test_table_names_overlap_default_aliases(self):
+        # see discussion in #104; this actually is not needed for query
+        # correctness, and only makes the generated SQL nicer
+        raise unittest.SkipTest
+
+        t0 = api.table([
+            ('key', 'string'),
+            ('v1', 'double')
+        ], 't1')
+
+        t1 = api.table([
+            ('key', 'string'),
+            ('v2', 'double')
+        ], 't0')
+
+        expr = t0.join(t1, t0.key == t1.key)[t0.key, t0.v1, t1.v2]
+
+        result = to_sql(expr)
+        expected = """\
+SELECT t2.`key`, t2.`v1`, t3.`v2`
+FROM t0 t2
+  INNER JOIN t1 t3
+    ON t2.`key` = t3.`key`"""
+
+        assert result == expected
+
+    def test_context_aliases_multiple_join(self):
+        t1 = self.con.table('star1')
+        t2 = self.con.table('star2')
+        t3 = self.con.table('star3')
+
+        expr = (t1.left_join(t2, [t1['foo_id'] == t2['foo_id']])
+                .inner_join(t3, [t1['bar_id'] == t3['bar_id']])
+                [[t1, t2['value1'], t3['value2']]])
+
+        query = _get_query(expr)
+        context = query.context
+
+        assert context.get_ref(t1) == 't0'
+        assert context.get_ref(t2) == 't1'
+        assert context.get_ref(t3) == 't2'
+
+    def test_fuse_projections(self):
+        table = api.table([
+            ('foo', 'int32'),
+            ('bar', 'int64'),
+            ('value', 'double')
+        ], name='tbl')
+
+        # Cases where we project in both cases using the base table reference
+        f1 = (table['foo'] + table['bar']).name('baz')
+        pred = table['value'] > 0
+
+        table2 = table[table, f1]
+        table2_filtered = table2[pred]
+
+        f2 = (table2['foo'] * 2).name('qux')
+        f3 = (table['foo'] * 2).name('qux')
+
+        table3 = table2.projection([table2, f2])
+
+        # fusion works even if there's a filter
+        table3_filtered = table2_filtered.projection([table2, f2])
+
+        expected = table[table, f1, f3]
+        expected2 = table[pred][table, f1, f3]
+
+        assert table3.equals(expected)
+        assert table3_filtered.equals(expected2)
+
+        ex_sql = """SELECT *, `foo` + `bar` AS `baz`, `foo` * 2 AS `qux`
+FROM tbl"""
+
+        ex_sql2 = """SELECT *, `foo` + `bar` AS `baz`, `foo` * 2 AS `qux`
+FROM tbl
+WHERE `value` > 0"""
+
+        table3_sql = to_sql(table3)
+        table3_filt_sql = to_sql(table3_filtered)
+
+        assert table3_sql == ex_sql
+        assert table3_filt_sql == ex_sql2
+
+        # Use the intermediate table refs
+        table3 = table2.projection([table2, f2])
+
+        # fusion works even if there's a filter
+        table3_filtered = table2_filtered.projection([table2, f2])
+
+        expected = table[table, f1, f3]
+        expected2 = table[pred][table, f1, f3]
+
+        assert table3.equals(expected)
+        assert table3_filtered.equals(expected2)
+
+    def test_projection_filter_fuse(self):
+        expr1, expr2, expr3 = self._case_projection_fuse_filter()
+
+        sql1 = to_sql(expr1)
+        sql2 = to_sql(expr2)
+        sql3 = to_sql(expr3)
+
+        assert sql1 == sql2
+        assert sql1 == sql3
+
+    def test_bug_project_multiple_times(self):
+        # 108
+        customer = self.con.table('tpch_customer')
+        nation = self.con.table('tpch_nation')
+        region = self.con.table('tpch_region')
+
+        joined = (
+            customer.inner_join(nation,
+                                [customer.c_nationkey == nation.n_nationkey])
+            .inner_join(region,
+                        [nation.n_regionkey == region.r_regionkey])
         )
-        assert join_with_predicates.op() == expected
+        proj1 = [customer, nation.n_name, region.r_name]
+        step1 = joined[proj1]
 
+        topk_by = step1.c_acctbal.cast('double').sum()
+        pred = step1.n_name.topk(10, by=topk_by)
 
-@pytest.mark.parametrize(
-    ("ibis_interval", "timedelta_interval"),
-    [
-        [ibis.interval(days=2), pd.Timedelta("2 days")],
-        [ibis.interval(days=2), datetime.timedelta(days=2)],
-        [ibis.interval(hours=5), pd.Timedelta("5 hours")],
-        [ibis.interval(hours=5), datetime.timedelta(hours=5)],
-        [ibis.interval(minutes=7), pd.Timedelta("7 minutes")],
-        [ibis.interval(minutes=7), datetime.timedelta(minutes=7)],
-        [ibis.interval(seconds=9), pd.Timedelta("9 seconds")],
-        [ibis.interval(seconds=9), datetime.timedelta(seconds=9)],
-        [ibis.interval(milliseconds=11), pd.Timedelta("11 milliseconds")],
-        [ibis.interval(milliseconds=11), datetime.timedelta(milliseconds=11)],
-        [ibis.interval(microseconds=15), pd.Timedelta("15 microseconds")],
-        [ibis.interval(microseconds=15), datetime.timedelta(microseconds=15)],
-        [ibis.interval(nanoseconds=17), pd.Timedelta("17 nanoseconds")],
-    ],
+        proj_exprs = [step1.c_name, step1.r_name, step1.n_name]
+        step2 = step1[pred]
+        expr = step2.projection(proj_exprs)
+
+        # it works!
+        result = to_sql(expr)
+        expected = """\
+SELECT `c_name`, `r_name`, `n_name`
+FROM (
+  SELECT t1.*, t2.`n_name`, t3.`r_name`
+  FROM tpch_customer t1
+    INNER JOIN tpch_nation t2
+      ON t1.`c_nationkey` = t2.`n_nationkey`
+    INNER JOIN tpch_region t3
+      ON t2.`n_regionkey` = t3.`r_regionkey`
+    LEFT SEMI JOIN (
+      SELECT t2.`n_name`, sum(CAST(t1.`c_acctbal` AS double)) AS `sum`
+      FROM tpch_customer t1
+        INNER JOIN tpch_nation t2
+          ON t1.`c_nationkey` = t2.`n_nationkey`
+        INNER JOIN tpch_region t3
+          ON t2.`n_regionkey` = t3.`r_regionkey`
+      GROUP BY 1
+      ORDER BY `sum` DESC
+      LIMIT 10
+    ) t4
+      ON t2.`n_name` = t4.`n_name`
+) t0"""
+        assert result == expected
+
+    def test_aggregate_projection_subquery(self):
+        t = self.con.table('alltypes')
+
+        proj = t[t.f > 0][t, (t.a + t.b).name('foo')]
+
+        result = to_sql(proj)
+        expected = """SELECT *, `a` + `b` AS `foo`
+FROM alltypes
+WHERE `f` > 0"""
+        assert result == expected
+
+        def agg(x):
+            return x.aggregate([x.foo.sum().name('foo total')], by=['g'])
+
+        # predicate gets pushed down
+        filtered = proj[proj.g == 'bar']
+
+        result = to_sql(filtered)
+        expected = """SELECT *, `a` + `b` AS `foo`
+FROM alltypes
+WHERE `f` > 0 AND
+      `g` = 'bar'"""
+        assert result == expected
+
+        agged = agg(filtered)
+        result = to_sql(agged)
+        expected = """SELECT `g`, sum(`foo`) AS `foo total`
+FROM (
+  SELECT *, `a` + `b` AS `foo`
+  FROM alltypes
+  WHERE `f` > 0 AND
+        `g` = 'bar'
+) t0
+GROUP BY 1"""
+        assert result == expected
+
+        # Pushdown is not possible (in Impala, Postgres, others)
+        agged2 = agg(proj[proj.foo < 10])
+
+        result = to_sql(agged2)
+        expected = """SELECT t0.`g`, sum(t0.`foo`) AS `foo total`
+FROM (
+  SELECT *, `a` + `b` AS `foo`
+  FROM alltypes
+  WHERE `f` > 0
+) t0
+WHERE t0.`foo` < 10
+GROUP BY 1"""
+        assert result == expected
+
+    def test_subquery_aliased(self):
+        case = self._case_subquery_aliased()
+
+        expected = """SELECT t0.*, t1.`value1`
+FROM (
+  SELECT `foo_id`, sum(`f`) AS `total`
+  FROM star1
+  GROUP BY 1
+) t0
+  INNER JOIN star2 t1
+    ON t0.`foo_id` = t1.`foo_id`"""
+        self._compare_sql(case, expected)
+
+    def test_double_nested_subquery_no_aliases(self):
+        # We don't require any table aliasing anywhere
+        t = api.table([
+            ('key1', 'string'),
+            ('key2', 'string'),
+            ('key3', 'string'),
+            ('value', 'double')
+        ], 'foo_table')
+
+        agg1 = t.aggregate([t.value.sum().name('total')],
+                           by=['key1', 'key2', 'key3'])
+        agg2 = agg1.aggregate([agg1.total.sum().name('total')],
+                              by=['key1', 'key2'])
+        agg3 = agg2.aggregate([agg2.total.sum().name('total')],
+                              by=['key1'])
+
+        result = to_sql(agg3)
+        expected = """SELECT `key1`, sum(`total`) AS `total`
+FROM (
+  SELECT `key1`, `key2`, sum(`total`) AS `total`
+  FROM (
+    SELECT `key1`, `key2`, `key3`, sum(`value`) AS `total`
+    FROM foo_table
+    GROUP BY 1, 2, 3
+  ) t1
+  GROUP BY 1, 2
+) t0
+GROUP BY 1"""
+        assert result == expected
+
+    def test_aggregate_projection_alias_bug(self):
+        # Observed in use
+        t1 = self.con.table('star1')
+        t2 = self.con.table('star2')
+
+        what = (t1.inner_join(t2, [t1.foo_id == t2.foo_id])
+                [[t1, t2.value1]])
+
+        what = what.aggregate([what.value1.sum().name('total')],
+                              by=[what.foo_id])
+
+        # TODO: Not fusing the aggregation with the projection yet
+        result = to_sql(what)
+        expected = """SELECT `foo_id`, sum(`value1`) AS `total`
+FROM (
+  SELECT t1.*, t2.`value1`
+  FROM star1 t1
+    INNER JOIN star2 t2
+      ON t1.`foo_id` = t2.`foo_id`
+) t0
+GROUP BY 1"""
+        assert result == expected
+
+    def test_aggregate_fuse_with_projection(self):
+        # see above test case
+        pass
+
+    def test_subquery_used_for_self_join(self):
+        expr = self._case_subquery_used_for_self_join()
+
+        result = to_sql(expr)
+        expected = """WITH t0 AS (
+  SELECT `g`, `a`, `b`, sum(`f`) AS `total`
+  FROM alltypes
+  GROUP BY 1, 2, 3
 )
-def test_asof_join_with_tolerance(ibis_interval, timedelta_interval):
-    left = ibis.table([("time", "timestamp"), ("key", "int32"), ("value", "double")])
-    right = ibis.table([("time", "timestamp"), ("key", "int32"), ("value2", "double")])
-
-    for interval in [ibis_interval, timedelta_interval]:
-        joined = api.asof_join(left, right, "time", tolerance=interval)
-
-        asof = left.asof_join(right, "time")
-        filt = asof.filter(
-            [
-                asof.time <= asof.time_right + interval,
-                asof.time >= asof.time_right - interval,
-            ]
-        )
-        join = left.left_join(filt, [left.time == filt.time])
-        expected = join.select(
-            left,
-            time_right=filt.time_right,
-            key_right=filt.key_right,
-            value2=filt.value2,
-        )
-
-        assert joined.equals(expected)
-
-
-def test_equijoin_schema_merge():
-    table1 = ibis.table([("key1", "string"), ("value1", "double")])
-    table2 = ibis.table([("key2", "string"), ("stuff", "int32")])
-
-    pred = table1["key1"] == table2["key2"]
-    join_types = ["inner_join", "left_join", "outer_join"]
-
-    ex_schema = ibis.schema(
-        names=["key1", "value1", "key2", "stuff"],
-        types=["string", "double", "string", "int32"],
-    )
-
-    for fname in join_types:
-        f = getattr(table1, fname)
-        joined = f(table2, [pred])
-        assert_equal(joined.schema(), ex_schema)
-
-
-def test_join_combo_with_projection(table):
-    # Test a case where there is column name overlap, but the projection
-    # passed makes it a non-issue. Highly relevant with self-joins
-    #
-    # For example, where left/right have some field names in common:
-    # SELECT left.*, right.a, right.b
-    # FROM left join right on left.key = right.key
-    t = table
-    t2 = t.mutate(foo=t.f * 2, bar=t.f * 4)
-
-    # this works
-    joined = t.left_join(t2, [t["g"] == t2["g"]])
-    proj = joined.select([t, t2["foo"], t2["bar"]])
-    repr(proj)
-
-
-def test_join_getitem_projection(con):
-    region = con.table("tpch_region")
-    nation = con.table("tpch_nation")
-
-    pred = region.r_regionkey == nation.n_regionkey
-    joined = region.inner_join(nation, pred)
-
-    result = joined[nation]
-    expected = joined.select(nation)
-    assert_equal(result, expected)
-
-
-def test_self_join(table):
-    # Self-joins are problematic with this design because column
-    # expressions may reference either the left or right  For example:
-    #
-    # SELECT left.key, sum(left.value - right.value) as total_deltas
-    # FROM table left
-    #  INNER JOIN table right
-    #    ON left.current_period = right.previous_period + 1
-    # GROUP BY 1
-    #
-    # One way around the self-join issue is to force the user to add
-    # prefixes to the joined fields, then project using those. Not that
-    # satisfying, though.
-    left = table
-    right = table.view()
-    metric = (left["a"] - right["b"]).mean().name("metric")
-
-    joined = left.inner_join(right, [right["g"] == left["g"]])
-
-    # Project out left table schema
-    proj = joined[[left]]
-    assert_equal(proj.schema(), left.schema())
-
-    # Try aggregating on top of joined
-    aggregated = joined.aggregate([metric], by=[left["g"]])
-    ex_schema = api.Schema({"g": "string", "metric": "double"})
-    assert_equal(aggregated.schema(), ex_schema)
-
-
-def test_self_join_no_view_convenience(table):
-    # #165, self joins ought to be possible when the user specifies the
-    # column names to join on rather than referentially-valid expressions
-
-    result = table.join(table, [("g", "g")])
-    expected_cols = list(table.columns)
-    # TODO(kszucs): the inner join convenience to don't duplicate the
-    # equivalent columns from the right table is not implemented yet
-    expected_cols.extend(f"{c}_right" for c in table.columns if c != "g")
-    assert result.columns == expected_cols
-
-
-def test_join_reference_bug(con):
-    # GH#403
-    orders = con.table("tpch_orders")
-    customer = con.table("tpch_customer")
-    lineitem = con.table("tpch_lineitem")
-
-    items = orders.join(lineitem, orders.o_orderkey == lineitem.l_orderkey)[
-        lineitem, orders.o_custkey, orders.o_orderpriority
-    ].join(customer, [("o_custkey", "c_custkey")])
-    items["o_orderpriority"].value_counts()
-
-
-def test_join_project_after(table):
-    # e.g.
-    #
-    # SELECT L.foo, L.bar, R.baz, R.qux
-    # FROM table1 L
-    #   INNER JOIN table2 R
-    #     ON L.key = R.key
-    #
-    # or
-    #
-    # SELECT L.*, R.baz
-    # ...
-    #
-    # The default for a join is selecting all fields if possible
-    table1 = ibis.table([("key1", "string"), ("value1", "double")])
-    table2 = ibis.table([("key2", "string"), ("stuff", "int32")])
-
-    pred = table1["key1"] == table2["key2"]
-
-    joined = table1.left_join(table2, [pred])
-    projected = joined.select([table1, table2["stuff"]])
-    assert projected.schema().names == ("key1", "value1", "stuff")
-
-    projected = joined.select([table2, table1["key1"]])
-    assert projected.schema().names == ("key2", "stuff", "key1")
-
-
-def test_semi_join_schema(table):
-    # A left semi join discards the schema of the right table
-    table1 = ibis.table([("key1", "string"), ("value1", "double")])
-    table2 = ibis.table([("key2", "string"), ("stuff", "double")])
-
-    pred = table1["key1"] == table2["key2"]
-    semi_joined = table1.semi_join(table2, [pred])
-
-    result_schema = semi_joined.schema()
-    assert_equal(result_schema, table1.schema())
-
-
-def test_cross_join(table):
-    metrics = [
-        table["a"].sum().name("sum_a"),
-        table["b"].mean().name("mean_b"),
-    ]
-    scalar_aggs = table.aggregate(metrics)
-
-    joined = table.cross_join(scalar_aggs)
-    agg_schema = api.Schema({"sum_a": "int64", "mean_b": "double"})
-    ex_schema = table.schema() | agg_schema
-    assert_equal(joined.schema(), ex_schema)
-
-
-def test_cross_join_multiple(table):
-    a = table["a", "b", "c"]
-    b = table["d", "e"]
-    c = table["f", "h"]
-
-    joined = ibis.cross_join(a, b, c)
-    with join_tables(joined) as (r1, r2, r3):
-        expected = ops.JoinChain(
-            first=r1,
-            rest=[
-                ops.JoinLink("cross", r2, []),
-                ops.JoinLink("cross", r3, []),
-            ],
-            values={
-                "a": r1.a,
-                "b": r1.b,
-                "c": r1.c,
-                "d": r2.d,
-                "e": r2.e,
-                "f": r3.f,
-                "h": r3.h,
-            },
-        )
-        assert joined.op() == expected
-        # TODO(kszucs): it must be simplified first using an appropriate rewrite rule
-        assert not joined.equals(a.cross_join(b.cross_join(c)))
-
-
-def test_filter_join():
-    table1 = ibis.table({"key1": "string", "key2": "string", "value1": "double"})
-    table2 = ibis.table({"key3": "string", "value2": "double"})
-
-    # It works!
-    joined = table1.inner_join(table2, [table1["key1"] == table2["key3"]])
-    filtered = joined.filter([table1.value1 > 0])
-    repr(filtered)
-
-
-def test_inner_join_overlapping_column_names():
-    t1 = ibis.table([("foo", "string"), ("bar", "string"), ("value1", "double")])
-    t2 = ibis.table([("foo", "string"), ("bar", "string"), ("value2", "double")])
-
-    joined = t1.join(t2, "foo")
-    expected = t1.join(t2, t1.foo == t2.foo)
-    assert_equal(joined, expected)
-    assert joined.columns == ["foo", "bar", "value1", "bar_right", "value2"]
-
-    joined = t1.join(t2, ["foo", "bar"])
-    expected = t1.join(t2, [t1.foo == t2.foo, t1.bar == t2.bar])
-    assert_equal(joined, expected)
-    assert joined.columns == ["foo", "bar", "value1", "value2"]
-
-    # Equality predicates don't have same name, need to rename
-    joined = t1.join(t2, t1.foo == t2.bar)
-    assert joined.columns == [
-        "foo",
-        "bar",
-        "value1",
-        "foo_right",
-        "bar_right",
-        "value2",
-    ]
-
-    # Not all predicates are equality, still need to rename
-    joined = t1.join(t2, ["foo", t1.value1 < t2.value2])
-    assert joined.columns == [
-        "foo",
-        "bar",
-        "value1",
-        "foo_right",
-        "bar_right",
-        "value2",
-    ]
-
-
-@pytest.mark.parametrize(
-    "key_maker",
-    [
-        lambda t1, t2: t1.foo_id == t2.foo_id,
-        lambda t1, t2: [("foo_id", "foo_id")],
-        lambda t1, t2: [(t1.foo_id, t2.foo_id)],
-        lambda t1, t2: [(_.foo_id, _.foo_id)],
-        lambda t1, t2: [(t1.foo_id, _.foo_id)],
-        lambda t1, t2: [(t1[2], t2[0])],  # foo_id is 2nd in t1, 0th in t2
-        lambda t1, t2: [(lambda t: t.foo_id, lambda t: t.foo_id)],
-    ],
+SELECT t0.`g`, max(t0.`total` - t1.`total`) AS `metric`
+FROM t0
+  INNER JOIN t0 t1
+    ON t0.`a` = t1.`b`
+GROUP BY 1"""
+        assert result == expected
+
+    def test_subquery_factor_correlated_subquery(self):
+        # #173, #183 and other issues
+
+        expr = self._case_subquery_factor_correlated_subquery()
+
+        result = to_sql(expr)
+        expected = """\
+WITH t0 AS (
+  SELECT t6.*, t1.`r_name` AS `region`, t3.`o_totalprice` AS `amount`,
+         CAST(t3.`o_orderdate` AS timestamp) AS `odate`
+  FROM tpch_region t1
+    INNER JOIN tpch_nation t2
+      ON t1.`r_regionkey` = t2.`n_regionkey`
+    INNER JOIN tpch_customer t6
+      ON t6.`c_nationkey` = t2.`n_nationkey`
+    INNER JOIN tpch_orders t3
+      ON t3.`o_custkey` = t6.`c_custkey`
 )
-def test_join_key_alternatives(con, key_maker):
-    t1 = con.table("star1")
-    t2 = con.table("star2")
-    key = key_maker(t1, t2)
-
-    joined = t1.inner_join(t2, key)
-    with join_tables(joined) as (r1, r2):
-        expected = ops.JoinChain(
-            first=r1,
-            rest=[
-                ops.JoinLink("inner", r2, [r1.foo_id == r2.foo_id]),
-            ],
-            values={
-                "c": r1.c,
-                "f": r1.f,
-                "foo_id": r1.foo_id,
-                "bar_id": r1.bar_id,
-                "value1": r2.value1,
-                "value3": r2.value3,
-            },
-        )
-        assert joined.op() == expected
-
-
-def test_join_key_invalid(con):
-    t1 = con.table("star1")
-    t2 = con.table("star2")
-
-    with pytest.raises(ExpressionError):
-        t1.inner_join(t2, [("foo_id", "foo_id", "foo_id")])
-
-    # it is working now
-    t1.inner_join(t2, [(s.c("foo_id"), s.c("foo_id"))])
-
-
-def test_join_invalid_refs(con):
-    t1 = con.table("star1")
-    t2 = con.table("star2")
-    t3 = con.table("star3")
-
-    predicate = t1.bar_id == t3.bar_id
-    with pytest.raises(com.IntegrityError):
-        t1.inner_join(t2, [predicate])
-
-
-def test_join_invalid_expr_type(con):
-    left = con.table("star1")
-    invalid_right = left.foo_id
-    join_key = ["bar_id"]
-
-    with pytest.raises(TypeError):
-        left.inner_join(invalid_right, join_key)
-
-
-def test_join_non_boolean_expr(con):
-    t1 = con.table("star1")
-    t2 = con.table("star2")
-
-    # oops
-    predicate = t1.f * t2.value1
-    with pytest.raises(ValidationError):
-        t1.inner_join(t2, [predicate])
-
-
-def test_unravel_compound_equijoin(table):
-    t1 = ibis.table(
-        [
-            ("key1", "string"),
-            ("key2", "string"),
-            ("key3", "string"),
-            ("value1", "double"),
-        ],
-        "foo_table",
-    )
-
-    t2 = ibis.table(
-        [
-            ("key1", "string"),
-            ("key2", "string"),
-            ("key3", "string"),
-            ("value2", "double"),
-        ],
-        "bar_table",
-    )
-
-    p1 = t1.key1 == t2.key1
-    p2 = t1.key2 == t2.key2
-    p3 = t1.key3 == t2.key3
-
-    joined = t1.inner_join(t2, [p1 & p2 & p3])
-    with join_tables(joined) as (r1, r2):
-        expected = ops.JoinChain(
-            first=r1,
-            rest=[
-                ops.JoinLink(
-                    "inner",
-                    r2,
-                    [r1.key1 == r2.key1, r1.key2 == r2.key2, r1.key3 == r2.key3],
-                )
-            ],
-            values={
-                "key1": r1.key1,
-                "key2": r1.key2,
-                "key3": r1.key3,
-                "value1": r1.value1,
-                "value2": r2.value2,
-            },
-        )
-        assert joined.op() == expected
-
-
-def test_union(
-    setops_table_foo,
-    setops_table_bar,
-    setops_table_baz,
-    setops_relation_error_message,
-):
-    result = setops_table_foo.union(setops_table_bar)
-    assert isinstance(result.op().parent, ops.Union)
-    assert not result.op().parent.distinct
-
-    result = setops_table_foo.union(setops_table_bar, distinct=True)
-    assert result.op().parent.distinct
-
-    with pytest.raises(RelationError, match=setops_relation_error_message):
-        setops_table_foo.union(setops_table_baz)
-
-
-def test_intersection(
-    setops_table_foo,
-    setops_table_bar,
-    setops_table_baz,
-    setops_relation_error_message,
-):
-    result = setops_table_foo.intersect(setops_table_bar)
-    assert isinstance(result.op().parent, ops.Intersection)
-
-    with pytest.raises(RelationError, match=setops_relation_error_message):
-        setops_table_foo.intersect(setops_table_baz)
-
-
-def test_difference(
-    setops_table_foo,
-    setops_table_bar,
-    setops_table_baz,
-    setops_relation_error_message,
-):
-    result = setops_table_foo.difference(setops_table_bar)
-    assert isinstance(result.op().parent, ops.Difference)
-
-    with pytest.raises(RelationError, match=setops_relation_error_message):
-        setops_table_foo.difference(setops_table_baz)
-
-
-def test_column_ref_on_projection_rename(con):
-    region = con.table("tpch_region")
-    nation = con.table("tpch_nation")
-    customer = con.table("tpch_customer")
-
-    joined = region.inner_join(
-        nation, [region.r_regionkey == nation.n_regionkey]
-    ).inner_join(customer, [customer.c_nationkey == nation.n_nationkey])
-
-    proj_exprs = [
-        customer,
-        nation.n_name.name("nation"),
-        region.r_name.name("region"),
-    ]
-    joined = joined.select(proj_exprs)
-
-    metrics = [joined.c_acctbal.sum().name("metric")]
-
-    # it works!
-    joined.aggregate(metrics, by=["region"])
-
-
-@pytest.fixture
-def t1():
-    return ibis.table(
-        [("key1", "string"), ("key2", "string"), ("value1", "double")], "foo"
-    )
-
-
-@pytest.fixture
-def t2():
-    return ibis.table([("key1", "string"), ("key2", "string")], "bar")
-
-
-def test_unresolved_existence_predicate(t1, t2):
-    expr = (t1.key1 == t2.key1).any()
-    assert isinstance(expr, Deferred)
-
-    filtered = t2.filter(t1.key1 == t2.key1)
-    subquery = ops.ExistsSubquery(filtered)
-    expected = ops.Filter(parent=t1, predicates=[subquery])
-    assert t1[expr].op() == expected
-
-    filtered = t1.filter(t1.key1 == t2.key1)
-    subquery = ops.ExistsSubquery(filtered)
-    expected = ops.Filter(parent=t2, predicates=[subquery])
-    assert t2[expr].op() == expected
-
-
-def test_resolve_existence_predicate(t1, t2):
-    expr = t1[(t1.key1 == t2.key1).any()]
-    op = expr.op()
-    assert isinstance(op, ops.Filter)
-
-    pred = op.predicates[0].to_expr()
-    assert isinstance(pred.op(), ops.ExistsSubquery)
-
-
-def test_aggregate_metrics(table):
-    functions = [
-        lambda x: x.e.sum().name("esum"),
-        lambda x: x.f.sum().name("fsum"),
-    ]
-    exprs = [table.e.sum().name("esum"), table.f.sum().name("fsum")]
-
-    result = table.aggregate(functions[0])
-    expected = table.aggregate(exprs[0])
-    assert_equal(result, expected)
-
-    result = table.aggregate(functions)
-    expected = table.aggregate(exprs)
-    assert_equal(result, expected)
-
-
-def test_group_by_keys(table):
-    m = table.mutate(foo=table.f * 2, bar=table.e / 2)
-
-    expr = m.group_by(lambda x: x.foo).size()
-    expected = m.group_by("foo").size()
-    assert_equal(expr, expected)
-
-    expr = m.group_by([lambda x: x.foo, lambda x: x.bar]).size()
-    expected = m.group_by(["foo", "bar"]).size()
-    assert_equal(expr, expected)
-
-
-def test_having(table):
-    m = table.mutate(foo=table.f * 2, bar=table.e / 2)
-    expr = m.group_by("foo").having(lambda x: x.foo.sum() > 10).size()
-
-    agg = ops.Aggregate(
-        parent=m,
-        groups={"foo": m.foo},
-        metrics={"CountStar()": ops.CountStar(m), "Sum(foo)": ops.Sum(m.foo)},
-    ).to_expr()
-    filt = ops.Filter(
-        parent=agg,
-        predicates=[agg["Sum(foo)"] > 10],
-    ).to_expr()
-    proj = ops.Project(
-        parent=filt,
-        values={"foo": filt.foo, "CountStar()": filt["CountStar()"]},
-    ).to_expr()
-
-    assert expr.equals(proj)
-
-
-def test_filter(table):
-    m = table.mutate(foo=table.f * 2, bar=table.e / 2)
-
-    result = m.filter(lambda x: x.foo > 10)
-    result2 = m[lambda x: x.foo > 10]
-    expected = m[m.foo > 10]
-
-    assert_equal(result, expected)
-    assert_equal(result2, expected)
-
-    result = m.filter([lambda x: x.foo > 10, lambda x: x.bar < 0])
-    expected = m.filter([m.foo > 10, m.bar < 0])
-    assert_equal(result, expected)
-
-
-def test_order_by2(table):
-    m = table.mutate(foo=table.e + table.f)
-
-    result = m.order_by(lambda x: -x.foo)
-    expected = m.order_by(-m.foo)
-    assert_equal(result, expected)
-
-    result = m.order_by(lambda x: ibis.desc(x.foo))
-    expected = m.order_by(ibis.desc("foo"))
-    assert_equal(result, expected)
-
-    result = m.order_by(ibis.desc(lambda x: x.foo))
-    expected = m.order_by(ibis.desc("foo"))
-    assert_equal(result, expected)
-
-    result = m.order_by(ibis.asc(lambda x: x.foo))
-    expected = m.order_by("foo")
-    assert_equal(result, expected)
-
-
-def test_projection2(table):
-    m = table.mutate(foo=table.f * 2)
-
-    def f(x):
-        return (x.foo * 2).name("bar")
-
-    result = m.select([f, "f"])
-    result2 = m[f, "f"]
-    expected = m.select([f(m), "f"])
-    assert_equal(result, expected)
-    assert_equal(result2, expected)
-
-
-def test_mutate2(table):
-    m = table.mutate(foo=table.f * 2)
-
-    def g(x):
-        return x.foo * 2
-
-    def h(x):
-        return x.bar * 2
-
-    result = m.mutate(bar=g).mutate(baz=h)
-
-    m2 = m.mutate(bar=g(m))
-    expected = m2.mutate(baz=h(m2))
-
-    assert_equal(result, expected)
-
-
-def test_groupby_mutate(table):
-    t = table
-
-    g = t.group_by("g").order_by("f")
-    expr = g.mutate(foo=lambda x: x.f.lag(), bar=lambda x: x.f.rank())
-    expected = g.mutate(foo=t.f.lag(), bar=t.f.rank())
-
-    assert_equal(expr, expected)
-
-
-def test_groupby_projection(table):
-    t = table
-
-    g = t.group_by("g").order_by("f")
-    expr = g.select([lambda x: x.f.lag().name("foo"), lambda x: x.f.rank().name("bar")])
-    expected = g.select([t.f.lag().name("foo"), t.f.rank().name("bar")])
-
-    assert_equal(expr, expected)
-
-
-def test_pickle_table_expr():
-    schema = [("time", "timestamp"), ("key", "string"), ("value", "double")]
-    t0 = ibis.table(schema, name="t0")
-    raw = pickle.dumps(t0, protocol=2)
-    t1 = pickle.loads(raw)
-    assert t1.equals(t0)
-
-
-def test_pickle_table_node(table):
-    n0 = table.op()
-    assert_pickle_roundtrip(n0)
-
-
-def test_pickle_projection_node(table):
-    m = table.mutate(foo=table.f * 2)
-
-    def f(x):
-        return (x.foo * 2).name("bar")
-
-    node = m.select([f, "f"]).op()
-
-    assert_pickle_roundtrip(node)
-
-
-def test_pickle_group_by(table):
-    m = table.mutate(foo=table.f * 2, bar=table.e / 2)
-    expr = m.group_by(lambda x: x.foo).size()
-    node = expr.op()
-
-    assert_pickle_roundtrip(node)
-
-
-def test_pickle_asof_join():
-    left = ibis.table([("time", "int32"), ("value", "double")])
-    right = ibis.table([("time", "int32"), ("value2", "double")])
-    joined = api.asof_join(left, right, "time")
-    node = joined.op()
-
-    assert_pickle_roundtrip(node)
-
-
-def test_group_by_key_function():
-    t = ibis.table([("a", "timestamp"), ("b", "string"), ("c", "double")])
-    expr = t.group_by(new_key=lambda t: t.b.length()).aggregate(foo=t.c.mean())
-    assert expr.columns == ["new_key", "foo"]
-
-
-def test_group_by_no_keys():
-    t = ibis.table([("a", "timestamp"), ("b", "string"), ("c", "double")])
-
-    with pytest.raises(com.IbisInputError):
-        t.group_by(s.startswith("x")).aggregate(foo=t.c.mean())
-
-
-def test_unbound_table_name():
-    t = ibis.table([("a", "timestamp")])
-    name = t.op().name
-    match = re.match(r"^unbound_table_\d+$", name)
-    assert match is not None
-
-
-class MyTable:
-    a: int
-    b: str
-    c: list[float]
-
-
-def test_unbound_table_using_class_definition():
-    expected_schema = ibis.schema({"a": "int64", "b": "string", "c": "array<double>"})
-
-    t1 = ibis.table(MyTable)
-    t2 = ibis.table(MyTable, name="MyNamedTable")
-
-    cases = {t1: "MyTable", t2: "MyNamedTable"}
-    for t, name in cases.items():
-        assert isinstance(t, ir.Table)
-        assert isinstance(t.op(), ops.UnboundTable)
-        assert t.schema() == expected_schema
-        assert t.get_name() == name
-
-
-def test_mutate_chain():
-    one = ibis.table([("a", "string"), ("b", "string")], name="t")
-    two = one.mutate(b=lambda t: t.b.fillna("Short Term"))
-    three = two.mutate(a=lambda t: t.a.fillna("Short Term"))
-
-    values = three.op().values
-    assert isinstance(values["a"], ops.Coalesce)
-    assert isinstance(values["b"], ops.Field)
-    assert values["b"].rel == two.op()
-
-    three_opt = simplify(three.op())
-    assert three_opt == ops.Project(
-        parent=one,
-        values={
-            "a": one.a.fillna("Short Term"),
-            "b": one.b.fillna("Short Term"),
-        },
-    )
-
-
-# TODO(kszucs): move this test case to ibis/tests/sql since it requires the
-# sql backend to be executed
-# def test_multiple_dbcon():
-#     """Expr from multiple connections to same DB should be compatible."""
-#     con1 = MockBackend()
-#     con2 = MockBackend()
-
-#     con1.table("alltypes").union(con2.table("alltypes")).execute()
-
-
-# TODO(kszucs): move this test to ibis/tests/sql since it requires the
-# sql backend to be executed
-# def test_multiple_db_different_backends():
-#     con1 = MockBackend()
-#     con2 = MockAlchemyBackend()
-
-#     backend1_table = con1.table("alltypes")
-#     backend2_table = con2.table("alltypes")
-
-#     expr = backend1_table.union(backend2_table)
-#     with pytest.raises(com.IbisError, match="Multiple backends"):
-#         expr.compile()
-
-
-def test_merge_as_of_allows_overlapping_columns():
-    # GH3295
-    table = ibis.table(
-        [
-            ("field", "string"),
-            ("value", "float64"),
-            ("timestamp_received", "timestamp"),
-        ],
-        name="t",
-    )
-
-    signal_one = table[
-        table["field"].contains("signal_one") & table["field"].contains("current")
-    ]
-    signal_one = signal_one[
-        "value", "timestamp_received", "field"
-    ]  # select columns we care about
-    signal_one = signal_one.rename(current="value", signal_one="field")
-
-    signal_two = table[
-        table["field"].contains("signal_two") & table["field"].contains("voltage")
-    ]
-    signal_two = signal_two[
-        "value", "timestamp_received", "field"
-    ]  # select columns we care about
-    signal_two = signal_two.rename(voltage="value", signal_two="field")
-
-    merged = signal_one.asof_join(signal_two, "timestamp_received")
-    assert merged.columns == [
-        "current",
-        "timestamp_received",
-        "signal_one",
-        "voltage",
-        "timestamp_received_right",
-        "signal_two",
-    ]
-
-
-def test_select_from_unambiguous_join_with_strings():
-    # GH1387
-    t = ibis.table([("a", "int64"), ("b", "string")])
-    s = ibis.table([("b", "int64"), ("c", "string")])
-    joined = t.left_join(s, [t.b == s.c])
-    expr = joined[t, "c"]
-    assert expr.columns == ["a", "b", "c"]
-
-
-def test_filter_applied_to_join():
-    # GH2437
-    countries = ibis.table([("iso_alpha3", "string")])
-    gdp = ibis.table([("country_code", "string"), ("year", "int64")])
-
-    expr = countries.inner_join(
-        gdp,
-        predicates=[countries["iso_alpha3"] == gdp["country_code"]],
-    ).filter(gdp["year"] == 2017)
-    assert expr.columns == ["iso_alpha3", "country_code", "year"]
-
-
-@pytest.mark.parametrize("how", ["inner", "left", "outer", "right"])
-def test_join_lname_rname(how):
-    left = ibis.table([("id", "int64"), ("first_name", "string")])
-    right = ibis.table([("id", "int64"), ("last_name", "string")])
-    method = getattr(left, f"{how}_join")
-
-    expr = method(right)
-    assert expr.columns == ["id", "first_name", "id_right", "last_name"]
-
-    expr = method(right, rname="right_{name}")
-    assert expr.columns == ["id", "first_name", "right_id", "last_name"]
-
-    expr = method(right, lname="left_{name}", rname="")
-    assert expr.columns == ["left_id", "first_name", "id", "last_name"]
-
-    expr = method(right, rname="right_{name}", lname="left_{name}")
-    assert expr.columns == ["left_id", "first_name", "right_id", "last_name"]
-
-
-def test_join_lname_rname_still_collide():
-    t1 = ibis.table({"id": "int64", "col1": "int64", "col2": "int64"})
-    t2 = ibis.table({"id": "int64", "col1": "int64", "col2": "int64"})
-    t3 = ibis.table({"id": "int64", "col1": "int64", "col2": "int64"})
-
-    with pytest.raises(com.IntegrityError):
-        t1.left_join(t2, "id").left_join(t3, "id")._finish()
-
-    # assert "`['col1_right', 'col2_right', 'id_right']`" in str(rec.value)
-    # assert "`lname='', rname='{name}_right'`" in str(rec.value)
-
-
-def test_drop():
-    t = ibis.table(dict.fromkeys("abcd", "int"))
-
-    assert t.drop() is t
-
-    res = t.drop("a")
-    assert res.equals(t.select("b", "c", "d"))
-
-    res = t.drop("a", "b")
-    assert res.equals(t.select("c", "d"))
-
-    assert res.equals(t.select("c", "d"))
-
-    assert res.equals(t.drop(s.matches("a|b")))
-
-    res = t.drop(_.a)
-    assert res.equals(t.select("b", "c", "d"))
-
-    res = t.drop(_.a, _.b)
-    assert res.equals(t.select("c", "d"))
-
-    res = t.drop(_.a, "b")
-    assert res.equals(t.select("c", "d"))
-
-    with pytest.raises(KeyError):
-        t.drop("e")
-
-
-def test_python_table_ambiguous():
-    with pytest.raises(NotImplementedError):
-        ibis.memtable(
-            [(1,)],
-            schema=ibis.schema(dict(a="int8")),
-            columns=["a"],
-        )
-
-
-def test_memtable_filter():
-    # Mostly just a smoketest, this used to error on construction
-    t = ibis.memtable([(1, 2), (3, 4), (5, 6)], columns=["x", "y"])
-    expr = t.filter(t.x > 1)
-    assert expr.columns == ["x", "y"]
-
-
-def test_default_backend_with_unbound_table():
-    t = ibis.table(dict(a="int"), name="t")
-    expr = t.a.sum()
-
-    with pytest.raises(
-        com.IbisError,
-        match="Expression contains unbound tables",
-    ):
-        assert expr.execute()
-
-
-def test_numpy_ufuncs_dont_cast_tables():
-    t = ibis.table(dict.fromkeys("abcd", "int"))
-    for arg in [np.int64(1), np.array([1, 2, 3])]:
-        for left, right in [(t, arg), (arg, t)]:
-            with pytest.raises(TypeError):
-                left + right
-
-
-def test_array_string_compare():
-    t = ibis.table(schema=dict(by="string", words="array<string>"), name="t")
-    expr = t[t.by == "foo"].mutate(words=_.words.unnest()).filter(_.words == "the")
-    assert expr is not None
-
-
-@pytest.mark.parametrize("value", [True, False])
-def test_filter_with_literal(value):
-    t = ibis.table(dict(a="string"))
-    filt = t.filter(ibis.literal(value))
-    assert filt.op() == ops.Filter(parent=t, predicates=[ibis.literal(value)])
-
-    # ints are invalid predicates
-    int_val = ibis.literal(int(value))
-    with pytest.raises(ValidationError):
-        t.filter(int_val)
-
-
-def test_cast():
-    t = ibis.table(dict(a="int", b="string", c="float"), name="t")
-
-    assert t.cast({"a": "string"}).equals(t.mutate(a=t.a.cast("string")))
-
-    with pytest.raises(
-        com.IbisError, match="fields that are not in the table: .+'d'.+"
-    ):
-        t.cast({"d": "array<int>"}).equals(t.select())
-
-    assert t.cast(ibis.schema({"a": "string", "b": "int"})).equals(
-        t.mutate(a=t.a.cast("string"), b=t.b.cast("int"))
-    )
-    assert t.cast([("a", "string"), ("b", "float")]).equals(
-        t.mutate(a=t.a.cast("string"), b=t.b.cast("float"))
-    )
-
-
-def test_pivot_longer():
-    diamonds = ibis.table(
-        {
-            "carat": "float64",
-            "cut": "string",
-            "color": "string",
-            "clarity": "string",
-            "depth": "float64",
-            "table": "float64",
-            "price": "int64",
-            "x": "float64",
-            "y": "float64",
-            "z": "float64",
-        },
-        name="diamonds",
-    )
-    res = diamonds.pivot_longer(s.c("x", "y", "z"), names_to="pos", values_to="xyz")
-    assert res.schema().names == (
-        "carat",
-        "cut",
-        "color",
-        "clarity",
-        "depth",
-        "table",
-        "price",
-        "pos",
-        "xyz",
-    )
-
-
-def test_pivot_longer_strip_prefix():
-    t = ibis.table(
-        dict(artist="string", track="string", wk1="int", wk2="int", wk3="int")
-    )
-    expr = t.pivot_longer(
-        s.startswith("wk"),
-        names_to="week",
-        names_pattern=r"wk(.+)",
-        names_transform=int,
-        values_to="rank",
-        values_transform=_.cast("int"),
-    )
-    schema = ibis.schema(dict(artist="string", track="string", week="int8", rank="int"))
-    assert expr.schema() == schema
-
-
-def test_pivot_longer_pluck_regex():
-    t = ibis.table(
-        dict(artist="string", track="string", x_wk1="int", x_wk2="int", x_wk3="int")
-    )
-    expr = t.pivot_longer(
-        s.matches("^.+wk.$"),
-        names_to=["other_var", "week"],
-        names_pattern=r"(.)_wk(\d)",
-        names_transform=dict(other_var=str.upper, week=int),
-        values_to="rank",
-        values_transform=_.cast("int"),
-    )
-    schema = ibis.schema(
-        dict(
-            artist="string", track="string", other_var="string", week="int8", rank="int"
-        )
-    )
-    assert expr.schema() == schema
-
-
-def test_pivot_longer_no_match():
-    t = ibis.table(
-        dict(artist="string", track="string", x_wk1="int", x_wk2="int", x_wk3="int")
-    )
-    with pytest.raises(
-        com.IbisInputError, match="Selector returned no columns to pivot on"
-    ):
-        t.pivot_longer(
-            s.matches("foo"),
-            names_to=["other_var", "week"],
-            names_pattern=r"(.)_wk(\d)",
-            names_transform=dict(other_var=str.upper, week=int),
-            values_to="rank",
-            values_transform=_.cast("int"),
-        )
-
-
-def test_pivot_wider():
-    fish = ibis.table({"fish": "int", "station": "string", "seen": "int"}, name="fish")
-    res = fish.pivot_wider(
-        names=["Release", "Lisbon"], names_from="station", values_from="seen"
-    )
-    assert res.schema().names == ("fish", "Release", "Lisbon")
-    with pytest.raises(com.IbisInputError, match="Columns .+ are not present in"):
-        fish.pivot_wider(names=["Release", "Lisbon"], values_from="seen")
-
-
-def test_invalid_deferred():
-    t = ibis.table(dict(value="int", lagged_value="int"), name="t")
-
-    with pytest.raises(ValidationError):
-        ops.Greatest((t.value, ibis._.lagged_value))
-
-
-@pytest.mark.parametrize("keep", ["last", None])
-def test_invalid_distinct(keep):
-    t = ibis.table(dict(a="int"), name="t")
-    with pytest.raises(com.IbisError, match="Only keep='first'"):
-        t.distinct(keep=keep)
-
-
-def test_invalid_keep_distinct():
-    t = ibis.table(dict(a="int", b="string"), name="t")
-    with pytest.raises(com.IbisError, match="Invalid value for `keep`:"):
-        t.distinct(on="a", keep="invalid")
-
-
-def test_invalid_distinct_empty_key():
-    t = ibis.table(dict(a="int", b="string"), name="t")
-    with pytest.raises(com.IbisInputError):
-        t.distinct(on="c", keep="first")
-
-
-def test_unbind_with_namespace():
-    schema = ibis.schema({"a": "int"})
-    ns = ops.Namespace(catalog="catalog", database="database")
-
-    t_op = ops.DatabaseTable(name="t", schema=schema, source=None, namespace=ns)
-    t = t_op.to_expr()
-    s = t.unbind()
+SELECT t0.*
+FROM t0
+WHERE t0.`amount` > (
+  SELECT avg(t4.`amount`) AS `mean`
+  FROM t0 t4
+  WHERE t4.`region` = t0.`region`
+)
+LIMIT 10"""
+        assert result == expected
 
-    expected = ops.UnboundTable(name="t", schema=schema, namespace=ns).to_expr()
+    def test_self_join_subquery_distinct_equal(self):
+        expr = self._case_self_join_subquery_distinct_equal()
 
-    assert s.op() == expected.op()
-    assert s.equals(expected)
+        result = to_sql(expr)
+        expected = """\
+WITH t0 AS (
+  SELECT t2.*, t3.*
+  FROM tpch_region t2
+    INNER JOIN tpch_nation t3
+      ON t2.`r_regionkey` = t3.`n_regionkey`
+)
+SELECT t0.`r_name`, t1.`n_name`
+FROM t0
+  INNER JOIN t0 t1
+    ON t0.`r_regionkey` = t1.`r_regionkey`"""
+
+        assert result == expected
+
+    def test_limit_with_self_join(self):
+        t = self.con.table('functional_alltypes')
+        t2 = t.view()
+
+        expr = t.join(t2, t.tinyint_col < t2.timestamp_col.minute()).count()
+
+        # it works
+        result = to_sql(expr)
+        expected = """\
+SELECT count(*) AS `count`
+FROM functional_alltypes t0
+  INNER JOIN functional_alltypes t1
+    ON t0.`tinyint_col` < extract(t1.`timestamp_col`, 'minute')"""
+        assert result == expected
+
+    def test_cte_factor_distinct_but_equal(self):
+        expr = self._case_cte_factor_distinct_but_equal()
+
+        result = to_sql(expr)
+        expected = """\
+WITH t0 AS (
+  SELECT `g`, sum(`f`) AS `metric`
+  FROM alltypes
+  GROUP BY 1
+)
+SELECT t0.*
+FROM t0
+  INNER JOIN t0 t1
+    ON t0.`g` = t1.`g`"""
+
+        assert result == expected
+
+    def test_tpch_self_join_failure(self):
+        yoy = self._case_tpch_self_join_failure()
+        to_sql(yoy)
+
+    def test_extract_subquery_nested_lower(self):
+        # We may have a join between two tables requiring subqueries, and
+        # buried inside these there may be a common subquery. Let's test that
+        # we find it and pull it out to the top level to avoid repeating
+        # ourselves.
+        pass
+
+    def test_subquery_in_filter_predicate(self):
+        expr, expr2 = self._case_subquery_in_filter_predicate()
+
+        result = to_sql(expr)
+        expected = """SELECT *
+FROM star1
+WHERE `f` > (
+  SELECT avg(`f`) AS `mean`
+  FROM star1
+)"""
+        assert result == expected
+
+        result = to_sql(expr2)
+        expected = """SELECT *
+FROM star1
+WHERE `f` > (
+  SELECT avg(`f`) AS `mean`
+  FROM star1
+  WHERE `foo_id` = 'foo'
+)"""
+        assert result == expected
+
+    def test_filter_subquery_derived_reduction(self):
+        expr3, expr4 = self._case_filter_subquery_derived_reduction()
+
+        result = to_sql(expr3)
+        expected = """SELECT *
+FROM star1
+WHERE `f` > (
+  SELECT ln(avg(`f`)) AS `tmp`
+  FROM star1
+  WHERE `foo_id` = 'foo'
+)"""
+        assert result == expected
+
+        result = to_sql(expr4)
+        expected = """SELECT *
+FROM star1
+WHERE `f` > (
+  SELECT ln(avg(`f`)) + 1 AS `tmp`
+  FROM star1
+  WHERE `foo_id` = 'foo'
+)"""
+        assert result == expected
+
+    def test_topk_operation(self):
+        filtered, filtered2 = self._case_topk_operation()
+
+        query = to_sql(filtered)
+        expected = """SELECT t0.*
+FROM tbl t0
+  LEFT SEMI JOIN (
+    SELECT `city`, avg(`v2`) AS `mean`
+    FROM tbl
+    GROUP BY 1
+    ORDER BY `mean` DESC
+    LIMIT 10
+  ) t1
+    ON t0.`city` = t1.`city`"""
+        assert query == expected
+
+        query = to_sql(filtered2)
+        expected = """SELECT t0.*
+FROM tbl t0
+  LEFT SEMI JOIN (
+    SELECT `city`, count(`city`) AS `count`
+    FROM tbl
+    GROUP BY 1
+    ORDER BY `count` DESC
+    LIMIT 10
+  ) t1
+    ON t0.`city` = t1.`city`"""
+        assert query == expected
+
+    def test_topk_predicate_pushdown_bug(self):
+        # Observed on TPCH data
+        cplusgeo = (
+            customer.inner_join(nation, [customer.c_nationkey ==
+                                         nation.n_nationkey])
+                    .inner_join(region, [nation.n_regionkey ==
+                                         region.r_regionkey])
+            [customer, nation.n_name, region.r_name])
+
+        pred = cplusgeo.n_name.topk(10, by=cplusgeo.c_acctbal.sum())
+        expr = cplusgeo.filter([pred])
+
+        result = to_sql(expr)
+        expected = """\
+SELECT t0.*, t1.`n_name`, t2.`r_name`
+FROM customer t0
+  INNER JOIN nation t1
+    ON t0.`c_nationkey` = t1.`n_nationkey`
+  INNER JOIN region t2
+    ON t1.`n_regionkey` = t2.`r_regionkey`
+  LEFT SEMI JOIN (
+    SELECT t1.`n_name`, sum(t0.`c_acctbal`) AS `sum`
+    FROM customer t0
+      INNER JOIN nation t1
+        ON t0.`c_nationkey` = t1.`n_nationkey`
+      INNER JOIN region t2
+        ON t1.`n_regionkey` = t2.`r_regionkey`
+    GROUP BY 1
+    ORDER BY `sum` DESC
+    LIMIT 10
+  ) t3
+    ON t1.`n_name` = t3.`n_name`"""
+        assert result == expected
+
+    def test_topk_analysis_bug(self):
+        # GH #398
+        airlines = ibis.table([('dest', 'string'),
+                               ('origin', 'string'),
+                               ('arrdelay', 'int32')], 'airlines')
+
+        dests = ['ORD', 'JFK', 'SFO']
+        t = airlines[airlines.dest.isin(dests)]
+        delay_filter = t.dest.topk(10, by=t.arrdelay.mean())
+        expr = t[delay_filter].group_by('origin').size()
+
+        result = to_sql(expr)
+        expected = """\
+SELECT t0.`origin`, count(*) AS `count`
+FROM airlines t0
+  LEFT SEMI JOIN (
+    SELECT `dest`, avg(`arrdelay`) AS `mean`
+    FROM airlines
+    WHERE `dest` IN ('ORD', 'JFK', 'SFO')
+    GROUP BY 1
+    ORDER BY `mean` DESC
+    LIMIT 10
+  ) t1
+    ON t0.`dest` = t1.`dest`
+WHERE t0.`dest` IN ('ORD', 'JFK', 'SFO')
+GROUP BY 1"""
+
+        assert result == expected
+
+    def test_topk_to_aggregate(self):
+        t = ibis.table([('dest', 'string'),
+                        ('origin', 'string'),
+                        ('arrdelay', 'int32')], 'airlines')
+
+        top = t.dest.topk(10, by=t.arrdelay.mean())
+
+        result = to_sql(top)
+        expected = to_sql(top.to_aggregation())
+        assert result == expected
+
+    def test_bottomk(self):
+        pass
+
+    def test_topk_antijoin(self):
+        # Get the "other" category somehow
+        pass
+
+    def test_case_in_projection(self):
+        t = self.con.table('alltypes')
+
+        expr = (t.g.case()
+                .when('foo', 'bar')
+                .when('baz', 'qux')
+                .else_('default').end())
+
+        expr2 = (api.case()
+                 .when(t.g == 'foo', 'bar')
+                 .when(t.g == 'baz', t.g)
+                 .end())
+
+        proj = t[expr.name('col1'), expr2.name('col2'), t]
+
+        result = to_sql(proj)
+        expected = """SELECT
+  CASE `g`
+    WHEN 'foo' THEN 'bar'
+    WHEN 'baz' THEN 'qux'
+    ELSE 'default'
+  END AS `col1`,
+  CASE
+    WHEN `g` = 'foo' THEN 'bar'
+    WHEN `g` = 'baz' THEN `g`
+    ELSE NULL
+  END AS `col2`, *
+FROM alltypes"""
+        assert result == expected
+
+    def test_identifier_quoting(self):
+        data = api.table([
+            ('date', 'int32'),
+            ('explain', 'string')
+        ], 'table')
+
+        expr = data[data.date.name('else'), data.explain.name('join')]
+
+        result = to_sql(expr)
+        expected = """SELECT `date` AS `else`, `explain` AS `join`
+FROM `table`"""
+        assert result == expected
+
+    def test_scalar_subquery_different_table(self):
+        t1, t2 = self.foo, self.bar
+        expr = t1[t1.y > t2.x.max()]
+
+        result = to_sql(expr)
+        expected = """SELECT *
+FROM foo
+WHERE `y` > (
+  SELECT max(`x`) AS `max`
+  FROM bar
+)"""
+        assert result == expected
+
+    def test_where_uncorrelated_subquery(self):
+        expr = self._case_where_uncorrelated_subquery()
+
+        result = to_sql(expr)
+        expected = """SELECT *
+FROM foo
+WHERE `job` IN (
+  SELECT `job`
+  FROM bar
+)"""
+        assert result == expected
+
+    def test_where_correlated_subquery(self):
+        expr = self._case_where_correlated_subquery()
+        result = to_sql(expr)
+        expected = """SELECT t0.*
+FROM foo t0
+WHERE t0.`y` > (
+  SELECT avg(t1.`y`) AS `mean`
+  FROM foo t1
+  WHERE t0.`dept_id` = t1.`dept_id`
+)"""
+        assert result == expected
+
+    def test_where_array_correlated(self):
+        # Test membership in some record-dependent values, if this is supported
+        pass
+
+    def test_exists(self):
+        e1, e2 = self._case_exists()
+
+        result = to_sql(e1)
+        expected = """SELECT t0.*
+FROM foo t0
+WHERE EXISTS (
+  SELECT 1
+  FROM bar t1
+  WHERE t0.`key1` = t1.`key1`
+)"""
+        assert result == expected
+
+        result = to_sql(e2)
+        expected = """SELECT t0.*
+FROM foo t0
+WHERE EXISTS (
+  SELECT 1
+  FROM bar t1
+  WHERE t0.`key1` = t1.`key1` AND
+        t1.`key2` = 'foo'
+)"""
+        assert result == expected
+
+    def test_exists_subquery_repr(self):
+        # GH #660
+        t1, t2 = self.t1, self.t2
+
+        cond = t1.key1 == t2.key1
+        expr = t1[cond.any()]
+        stmt = build_ast(expr).queries[0]
+
+        repr(stmt.where[0])
+
+    def test_not_exists(self):
+        expr = self._case_not_exists()
+        result = to_sql(expr)
+        expected = """SELECT t0.*
+FROM foo t0
+WHERE NOT EXISTS (
+  SELECT 1
+  FROM bar t1
+  WHERE t0.`key1` = t1.`key1`
+)"""
+        assert result == expected
+
+    def test_filter_inside_exists(self):
+        events = ibis.table([('session_id', 'int64'),
+                             ('user_id', 'int64'),
+                             ('event_type', 'int32'),
+                             ('ts', 'timestamp')], 'events')
+
+        purchases = ibis.table([('item_id', 'int64'),
+                                ('user_id', 'int64'),
+                                ('price', 'double'),
+                                ('ts', 'timestamp')], 'purchases')
+        filt = purchases.ts > '2015-08-15'
+        cond = (events.user_id == purchases[filt].user_id).any()
+        expr = events[cond]
+
+        result = to_sql(expr)
+        expected = """\
+SELECT t0.*
+FROM events t0
+WHERE EXISTS (
+  SELECT 1
+  FROM purchases t1
+  WHERE t1.`ts` > '2015-08-15' AND
+        t0.`user_id` = t1.`user_id`
+)"""
+
+        assert result == expected
+
+    def test_self_reference_in_exists(self):
+        semi, anti = self._case_self_reference_in_exists()
+
+        result = to_sql(semi)
+        expected = """\
+SELECT t0.*
+FROM functional_alltypes t0
+WHERE EXISTS (
+  SELECT 1
+  FROM functional_alltypes t1
+  WHERE t0.`string_col` = t1.`string_col`
+)"""
+        assert result == expected
+
+        result = to_sql(anti)
+        expected = """\
+SELECT t0.*
+FROM functional_alltypes t0
+WHERE NOT EXISTS (
+  SELECT 1
+  FROM functional_alltypes t1
+  WHERE t0.`string_col` = t1.`string_col`
+)"""
+        assert result == expected
+
+    def test_self_reference_limit_exists(self):
+        case = self._case_self_reference_limit_exists()
+
+        expected = """\
+WITH t0 AS (
+  SELECT *
+  FROM functional_alltypes
+  LIMIT 100
+)
+SELECT *
+FROM t0
+WHERE NOT EXISTS (
+  SELECT 1
+  FROM t0 t1
+  WHERE t0.`string_col` = t1.`string_col`
+)"""
+        self._compare_sql(case, expected)
+
+    def test_limit_cte_extract(self):
+        case = self._case_limit_cte_extract()
+
+        expected = """\
+WITH t0 AS (
+  SELECT *
+  FROM functional_alltypes
+  LIMIT 100
+)
+SELECT t0.*
+FROM t0
+  CROSS JOIN t0 t1"""
+
+        self._compare_sql(case, expected)
+
+    def test_sort_by(self):
+        cases = self._case_sort_by()
+
+        expected = [
+            """SELECT *
+FROM star1
+ORDER BY `f`""",
+            """SELECT *
+FROM star1
+ORDER BY `f` DESC""",
+            """SELECT *
+FROM star1
+ORDER BY `c`, `f` DESC"""
+        ]
+
+        for case, ex in zip(cases, expected):
+            result = to_sql(case)
+            assert result == ex
+
+    def test_limit(self):
+        cases = self._case_limit()
+
+        expected = [
+            """SELECT *
+FROM star1
+LIMIT 10""",
+            """SELECT *
+FROM star1
+LIMIT 10 OFFSET 5""",
+            """SELECT *
+FROM star1
+WHERE `f` > 0
+LIMIT 10""",
+            """SELECT *
+FROM (
+  SELECT *
+  FROM star1
+  LIMIT 10
+) t0
+WHERE `f` > 0"""
+        ]
+
+        for case, ex in zip(cases, expected):
+            result = to_sql(case)
+            assert result == ex
+
+    def test_join_with_limited_table(self):
+        joined = self._case_join_with_limited_table()
+
+        result = to_sql(joined)
+        expected = """SELECT t0.*
+FROM (
+  SELECT *
+  FROM star1
+  LIMIT 100
+) t0
+  INNER JOIN star2 t1
+    ON t0.`foo_id` = t1.`foo_id`"""
+
+        assert result == expected
+
+    def test_sort_by_on_limit_yield_subquery(self):
+        # x.limit(...).sort_by(...)
+        #   is semantically different from
+        # x.sort_by(...).limit(...)
+        #   and will often yield different results
+        t = self.con.table('functional_alltypes')
+        expr = (t.group_by('string_col')
+                .aggregate([t.count().name('nrows')])
+                .limit(5)
+                .sort_by('string_col'))
+
+        result = to_sql(expr)
+        expected = """SELECT *
+FROM (
+  SELECT `string_col`, count(*) AS `nrows`
+  FROM functional_alltypes
+  GROUP BY 1
+  LIMIT 5
+) t0
+ORDER BY `string_col`"""
+        assert result == expected
+
+    def test_multiple_limits(self):
+        t = self.con.table('functional_alltypes')
+
+        expr = t.limit(20).limit(10)
+        stmt = build_ast(expr).queries[0]
+
+        assert stmt.limit['n'] == 10
+
+    def test_top_convenience(self):
+        # x.top(10, by=field)
+        # x.top(10, by=[field1, field2])
+        pass
+
+    def test_self_aggregate_in_predicate(self):
+        # Per ibis #43
+        pass
+
+    def test_self_join_filter_analysis_bug(self):
+        expr, _ = self._case_filter_self_join_analysis_bug()
+
+        expected = """\
+WITH t0 AS (
+  SELECT `region`, `kind`, sum(`amount`) AS `total`
+  FROM purchases
+  GROUP BY 1, 2
+)
+SELECT t1.`region`, t1.`total` - t2.`total` AS `diff`
+FROM (
+  SELECT *
+  FROM t0
+  WHERE `kind` = 'foo'
+) t1
+  INNER JOIN (
+    SELECT *
+    FROM t0
+    WHERE `kind` = 'bar'
+  ) t2
+    ON t1.`region` = t2.`region`"""
+        self._compare_sql(expr, expected)
+
+
+class TestUnions(unittest.TestCase, ExprTestCases):
+
+    def setUp(self):
+        self.con = MockConnection()
+
+    def test_union(self):
+        union1 = self._case_union()
+
+        result = to_sql(union1)
+        expected = """\
+SELECT `string_col` AS `key`, CAST(`float_col` AS double) AS `value`
+FROM functional_alltypes
+WHERE `int_col` > 0
+UNION ALL
+SELECT `string_col` AS `key`, `double_col` AS `value`
+FROM functional_alltypes
+WHERE `int_col` <= 0"""
+        assert result == expected
+
+    def test_union_distinct(self):
+        union = self._case_union(distinct=True)
+        result = to_sql(union)
+        expected = """\
+SELECT `string_col` AS `key`, CAST(`float_col` AS double) AS `value`
+FROM functional_alltypes
+WHERE `int_col` > 0
+UNION
+SELECT `string_col` AS `key`, `double_col` AS `value`
+FROM functional_alltypes
+WHERE `int_col` <= 0"""
+        assert result == expected
+
+    def test_union_project_column(self):
+        # select a column, get a subquery
+        union1 = self._case_union()
+        expr = union1[[union1.key]]
+        result = to_sql(expr)
+        expected = """SELECT `key`
+FROM (
+  SELECT `string_col` AS `key`, CAST(`float_col` AS double) AS `value`
+  FROM functional_alltypes
+  WHERE `int_col` > 0
+  UNION ALL
+  SELECT `string_col` AS `key`, `double_col` AS `value`
+  FROM functional_alltypes
+  WHERE `int_col` <= 0
+) t0"""
+        assert result == expected
+
+
+class TestDistinct(unittest.TestCase):
+
+    def setUp(self):
+        self.con = MockConnection()
+
+    def test_table_distinct(self):
+        t = self.con.table('functional_alltypes')
+
+        expr = t[t.string_col, t.int_col].distinct()
+
+        result = to_sql(expr)
+        expected = """SELECT DISTINCT `string_col`, `int_col`
+FROM functional_alltypes"""
+        assert result == expected
+
+    def test_array_distinct(self):
+        t = self.con.table('functional_alltypes')
+        expr = t.string_col.distinct()
+
+        result = to_sql(expr)
+        expected = """SELECT DISTINCT `string_col`
+FROM functional_alltypes"""
+        assert result == expected
+
+    def test_count_distinct(self):
+        t = self.con.table('functional_alltypes')
+
+        metric = t.int_col.nunique().name('nunique')
+        expr = t[t.bigint_col > 0].group_by('string_col').aggregate([metric])
+
+        result = to_sql(expr)
+        expected = """\
+SELECT `string_col`, COUNT(DISTINCT `int_col`) AS `nunique`
+FROM functional_alltypes
+WHERE `bigint_col` > 0
+GROUP BY 1"""
+        assert result == expected
+
+    def test_multiple_count_distinct(self):
+        # Impala and some other databases will not execute multiple
+        # count-distincts in a single aggregation query. This error reporting
+        # will be left to the database itself, for now.
+        t = self.con.table('functional_alltypes')
+        metrics = [t.int_col.nunique().name('int_card'),
+                   t.smallint_col.nunique().name('smallint_card')]
+
+        expr = t.group_by('string_col').aggregate(metrics)
+
+        result = to_sql(expr)
+        expected = """\
+SELECT `string_col`, COUNT(DISTINCT `int_col`) AS `int_card`,
+       COUNT(DISTINCT `smallint_col`) AS `smallint_card`
+FROM functional_alltypes
+GROUP BY 1"""
+        assert result == expected
```

